# é·ç§»ç­–ç•¥èˆ‡é¢¨éšªç®¡ç† (Migration Strategy)

## æ–‡æª”ç·¨è™Ÿ
`COGNITIVE-ARCH-09`

**ç‰ˆæœ¬**: 1.0.0
**æœ€å¾Œæ›´æ–°**: 2026-02-12
**ç‹€æ…‹**: å¯¦æ–½è¨ˆåŠƒ

---

## ç¸½è¦½

æœ¬æ–‡æª”æä¾›å¾ç•¶å‰æ¶æ§‹å‘é¡äººé¡èªçŸ¥æ¶æ§‹é·ç§»çš„è©³ç´°ç­–ç•¥ï¼ŒåŒ…æ‹¬é¢¨éšªè©•ä¼°ã€ç·©è§£æªæ–½ã€å›æ»¾è¨ˆåŠƒèˆ‡é©—è­‰æ¨™æº–ã€‚

### é·ç§»åŸå‰‡

1. **é›¶åœæ©Ÿé·ç§» (Zero-Downtime Migration)**ï¼šæ–°èˆŠç³»çµ±ä¸¦è¡Œï¼Œå¹³æ»‘åˆ‡æ›
2. **æ¼¸é€²å¼æ¨é€² (Incremental Rollout)**ï¼šå¾ 5% é–‹å§‹é€æ­¥æ“´å¤§
3. **ç‰¹æ€§é–‹é—œæ§åˆ¶ (Feature Flag Driven)**ï¼šæ‰€æœ‰æ–°åŠŸèƒ½éƒ½åœ¨é–‹é—œå¾Œé¢
4. **å®Œå…¨å¯å›æ»¾ (Fully Reversible)**ï¼šä»»ä½•æ™‚å€™éƒ½å¯ä»¥å›é€€
5. **æ•¸æ“šé©…å‹•æ±ºç­– (Data-Driven Decisions)**ï¼šåŸºæ–¼æŒ‡æ¨™æ±ºå®šæ¨é€²æˆ–å›é€€

---

## é·ç§»æ¶æ§‹

### é›™è»Œé‹è¡Œæ¶æ§‹ (Dual-Track Architecture)

```mermaid
graph TB
    Request[User Request] --> LB[Load Balancer<br/>Traffic Split]

    LB -->|90%| Legacy[Legacy Engine<br/>DefaultRouter]
    LB -->|10%| Cognitive[Cognitive Engine<br/>OODARouter + Metacog]

    Legacy --> LegacyProc[Legacy Processors]
    Cognitive --> CogProc[Cognitive Processors]

    LegacyProc --> Response1[Response]
    CogProc --> Response2[Response]

    Response1 --> Metrics[Metrics Collector]
    Response2 --> Metrics

    Metrics --> Comparison[A/B Comparison<br/>Dashboard]
    Comparison --> Decision{Quality<br/>Improved?}

    Decision -->|Yes| IncreaseTraffic[Increase Traffic<br/>to Cognitive]
    Decision -->|No| RollbackOrFix[Rollback or Fix]
```

### Feature Flag å±¤æ¬¡çµæ§‹

```yaml
# config/feature_flags.yaml

cognitive:
  enabled: true  # ä¸»é–‹é—œ

  global_workspace:
    enabled: true  # å…¨åŸŸå·¥ä½œç©ºé–“
    capacity: 7    # å·¥ä½œè¨˜æ†¶å®¹é‡
    decay_rate: 0.1

  metacognition:
    enabled: true  # å…ƒèªçŸ¥æ²»ç†
    quality_threshold: 0.7
    max_iterations: 3
    budget:
      max_tokens: 10000
      max_time_seconds: 300

  ooda_router:
    enabled: false  # OODA è·¯ç”±ï¼ˆåˆæœŸé—œé–‰ï¼‰
    use_embeddings: false

  memory_systems:
    enabled: false  # è¨˜æ†¶ç³»çµ±ï¼ˆåˆæœŸé—œé–‰ï¼‰
    episodic_enabled: false
    semantic_enabled: false

  neuromodulation:
    enabled: false  # ç¥ç¶“èª¿æ§ï¼ˆåˆæœŸé—œé–‰ï¼‰
    exploration_rate: 0.1

  event_driven:
    enabled: false  # äº‹ä»¶é©…å‹•ï¼ˆåˆæœŸé—œé–‰ï¼‰
    parallel_execution: false
```

---

## é·ç§»éšæ®µ

### Stage 0: æº–å‚™éšæ®µï¼ˆWeek 1-2ï¼‰

**ç›®æ¨™**ï¼šå»ºç«‹é·ç§»åŸºç¤è¨­æ–½

#### 0.1 Feature Flag ç³»çµ±éƒ¨ç½²

```python
# src/core/feature_flags.py

import yaml
from typing import Any, Dict
from pathlib import Path

class FeatureFlagManager:
    """ç‰¹æ€§é–‹é—œç®¡ç†å™¨"""

    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return

        self.flags: Dict[str, Any] = {}
        self.reload()
        self._initialized = True

    def reload(self):
        """é‡æ–°åŠ è¼‰é…ç½®ï¼ˆæ”¯æŒç†±æ›´æ–°ï¼‰"""
        config_path = Path("config/feature_flags.yaml")
        with open(config_path) as f:
            self.flags = yaml.safe_load(f)

    def is_enabled(self, flag_path: str) -> bool:
        """
        æª¢æŸ¥ç‰¹æ€§æ˜¯å¦å•Ÿç”¨

        Examples:
            is_enabled("cognitive.global_workspace")
            is_enabled("cognitive.metacognition.enabled")
        """
        keys = flag_path.split(".")
        value = self.flags

        for key in keys:
            if not isinstance(value, dict):
                return False
            value = value.get(key, False)

        return bool(value)

    def get(self, flag_path: str, default: Any = None) -> Any:
        """ç²å–ç‰¹æ€§é…ç½®å€¼"""
        keys = flag_path.split(".")
        value = self.flags

        for key in keys:
            if not isinstance(value, dict):
                return default
            value = value.get(key, default)

        return value

# å…¨åŸŸå–®ä¾‹
feature_flags = FeatureFlagManager()
```

#### 0.2 æµé‡åˆ†å‰²å™¨

```python
# src/api/traffic_splitter.py

import random
from typing import Optional
from src.core.feature_flags import feature_flags

class TrafficSplitter:
    """æµé‡åˆ†å‰²å™¨ï¼šæ§åˆ¶æ–°èˆŠç³»çµ±çš„æµé‡æ¯”ä¾‹"""

    def __init__(self):
        self.cognitive_traffic_ratio = 0.0  # åˆå§‹ç‚º 0%

    def should_use_cognitive_engine(
        self,
        user_id: Optional[str] = None,
        force: Optional[bool] = None
    ) -> bool:
        """
        æ±ºå®šæ˜¯å¦ä½¿ç”¨èªçŸ¥å¼•æ“

        Args:
            user_id: ç”¨æˆ¶IDï¼ˆç”¨æ–¼ä¸€è‡´æ€§å“ˆå¸Œï¼‰
            force: å¼·åˆ¶ä½¿ç”¨ï¼ˆç”¨æ–¼æ¸¬è©¦ï¼‰

        Returns:
            True if should use cognitive engine
        """
        # å¼·åˆ¶æ¨¡å¼
        if force is not None:
            return force

        # æª¢æŸ¥ç¸½é–‹é—œ
        if not feature_flags.is_enabled("cognitive.enabled"):
            return False

        # ä¸€è‡´æ€§å“ˆå¸Œï¼ˆç¢ºä¿åŒä¸€ç”¨æˆ¶ç¸½æ˜¯ä½¿ç”¨åŒä¸€ç³»çµ±ï¼‰
        if user_id:
            hash_value = hash(user_id) % 100
            return hash_value < (self.cognitive_traffic_ratio * 100)

        # éš¨æ©Ÿåˆ†é…
        return random.random() < self.cognitive_traffic_ratio

    def set_traffic_ratio(self, ratio: float):
        """
        è¨­ç½®æµé‡æ¯”ä¾‹

        Args:
            ratio: 0.0 to 1.0
        """
        self.cognitive_traffic_ratio = max(0.0, min(1.0, ratio))
        logger.info(f"Cognitive traffic ratio set to {self.cognitive_traffic_ratio:.1%}")

# å…¨åŸŸå–®ä¾‹
traffic_splitter = TrafficSplitter()
```

#### 0.3 A/B æ¸¬è©¦æ¡†æ¶

```python
# src/monitoring/ab_testing.py

from dataclasses import dataclass
from typing import List, Dict
from datetime import datetime, timedelta
import numpy as np

@dataclass
class VariantMetrics:
    """è®Šé«”æŒ‡æ¨™"""
    variant_name: str
    total_requests: int
    success_count: int
    avg_latency: float
    avg_confidence: float
    avg_user_rating: float
    error_rate: float

    @property
    def success_rate(self) -> float:
        return self.success_count / self.total_requests if self.total_requests > 0 else 0

class ABTestManager:
    """A/B æ¸¬è©¦ç®¡ç†å™¨"""

    def __init__(self):
        self.metrics: Dict[str, VariantMetrics] = {
            "legacy": VariantMetrics("legacy", 0, 0, 0.0, 0.0, 0.0, 0.0),
            "cognitive": VariantMetrics("cognitive", 0, 0, 0.0, 0.0, 0.0, 0.0)
        }

    def record_result(
        self,
        variant: str,
        success: bool,
        latency: float,
        confidence: float,
        user_rating: Optional[float] = None
    ):
        """è¨˜éŒ„æ¸¬è©¦çµæœ"""
        metrics = self.metrics[variant]

        metrics.total_requests += 1
        if success:
            metrics.success_count += 1

        # ç§»å‹•å¹³å‡
        alpha = 0.1  # å¹³æ»‘ä¿‚æ•¸
        metrics.avg_latency = (1 - alpha) * metrics.avg_latency + alpha * latency
        metrics.avg_confidence = (1 - alpha) * metrics.avg_confidence + alpha * confidence

        if user_rating:
            metrics.avg_user_rating = (1 - alpha) * metrics.avg_user_rating + alpha * user_rating

        metrics.error_rate = 1 - metrics.success_rate

    def compare_variants(self) -> Dict[str, Any]:
        """
        æ¯”è¼ƒå…©å€‹è®Šé«”

        Returns:
            æ¯”è¼ƒå ±å‘Š
        """
        legacy = self.metrics["legacy"]
        cognitive = self.metrics["cognitive"]

        # è¨ˆç®—æ”¹é€²ç™¾åˆ†æ¯”
        def improvement(new_val, old_val):
            if old_val == 0:
                return 0.0
            return ((new_val - old_val) / old_val) * 100

        return {
            "success_rate": {
                "legacy": legacy.success_rate,
                "cognitive": cognitive.success_rate,
                "improvement": improvement(cognitive.success_rate, legacy.success_rate)
            },
            "latency": {
                "legacy": legacy.avg_latency,
                "cognitive": cognitive.avg_latency,
                "improvement": improvement(legacy.avg_latency, cognitive.avg_latency)  # è² å€¼ç‚ºå¥½
            },
            "confidence": {
                "legacy": legacy.avg_confidence,
                "cognitive": cognitive.avg_confidence,
                "improvement": improvement(cognitive.avg_confidence, legacy.avg_confidence)
            },
            "user_rating": {
                "legacy": legacy.avg_user_rating,
                "cognitive": cognitive.avg_user_rating,
                "improvement": improvement(cognitive.avg_user_rating, legacy.avg_user_rating)
            },
            "recommendation": self._make_recommendation(legacy, cognitive)
        }

    def _make_recommendation(
        self,
        legacy: VariantMetrics,
        cognitive: VariantMetrics
    ) -> str:
        """
        åŸºæ–¼æ•¸æ“šçµ¦å‡ºæ¨é€²å»ºè­°

        æ±ºç­–æ¨¹ï¼š
        1. å¦‚æœèªçŸ¥å¼•æ“æˆåŠŸç‡ < èˆŠå¼•æ“ -5% â†’ ROLLBACK
        2. å¦‚æœéŒ¯èª¤ç‡ > 10% â†’ ROLLBACK
        3. å¦‚æœæˆåŠŸç‡æå‡ >5% ä¸”éŒ¯èª¤ç‡ <5% â†’ SCALE_UP
        4. å¦‚æœæ²’æœ‰é¡¯è‘—å·®ç•° â†’ HOLDï¼ˆéœ€è¦æ›´å¤šæ•¸æ“šï¼‰
        """
        if cognitive.total_requests < 100:
            return "HOLD - Need more data (min 100 requests)"

        # è‡´å‘½å•é¡Œï¼šéŒ¯èª¤ç‡éé«˜
        if cognitive.error_rate > 0.1:
            return "ROLLBACK - Error rate too high"

        # é¡¯è‘—é€€åŒ–
        if cognitive.success_rate < legacy.success_rate - 0.05:
            return "ROLLBACK - Success rate degraded"

        # é¡¯è‘—æå‡
        if (
            cognitive.success_rate > legacy.success_rate + 0.05 and
            cognitive.error_rate < 0.05
        ):
            return "SCALE_UP - Significant improvement"

        # è¼•å¾®æå‡
        if cognitive.success_rate > legacy.success_rate + 0.02:
            return "SCALE_UP_SLOWLY - Minor improvement"

        # æ²’æœ‰é¡¯è‘—å·®ç•°
        return "HOLD - No significant difference"

# å…¨åŸŸå–®ä¾‹
ab_test_manager = ABTestManager()
```

---

### Stage 1: ç°åº¦ç™¼å¸ƒï¼ˆWeek 3-16ï¼‰

#### 1.1 æµé‡åˆ†é…æ™‚é–“è¡¨

| Week | æµé‡æ¯”ä¾‹ | é©—è­‰æ¨™æº– | å‹•ä½œ |
|------|---------|---------|------|
| 3-4 | 0% â†’ 5% | Phase 1 å®Œæˆï¼Œå·¥ä½œç©ºé–“åŠŸèƒ½é©—è­‰ | å…§éƒ¨æ¸¬è©¦é€šéå¾Œé–‹æ”¾ |
| 5-6 | 5% â†’ 10% | Phase 2 å®Œæˆï¼Œå…ƒèªçŸ¥åŠŸèƒ½é©—è­‰ | éŒ¯èª¤ç‡ <10% |
| 7-8 | 10% â†’ 20% | Phase 3 å®Œæˆï¼ŒOODA è·¯ç”±é©—è­‰ | æˆåŠŸç‡æå‡ >5% |
| 9-10 | 20% â†’ 30% | Phase 4 å®Œæˆï¼Œè¨˜æ†¶ç³»çµ±é©—è­‰ | ç”¨æˆ¶æ»¿æ„åº¦æå‡ >10% |
| 11-12 | 30% â†’ 50% | Phase 5 å®Œæˆï¼Œç¥ç¶“èª¿æ§é©—è­‰ | ç„¡é‡å¤§äº‹æ•… |
| 13-14 | 50% â†’ 70% | Phase 6 å®Œæˆï¼Œäº‹ä»¶é©…å‹•é©—è­‰ | æ€§èƒ½é”æ¨™ |
| 15-16 | 70% â†’ 100% | Phase 7 å®Œæˆï¼Œå…¨é‡ä¸Šç·š | æ‰€æœ‰æŒ‡æ¨™é”æ¨™ |

#### 1.2 ç°åº¦æ§åˆ¶è…³æœ¬

```bash
#!/bin/bash
# scripts/set_cognitive_traffic.sh

RATIO=$1

if [ -z "$RATIO" ]; then
    echo "Usage: $0 <ratio>"
    echo "Example: $0 0.05  # Set to 5%"
    exit 1
fi

# æ›´æ–°é…ç½®
curl -X POST http://admin-api/config/traffic-ratio \
    -H "Content-Type: application/json" \
    -d "{\"ratio\": $RATIO}"

echo "Cognitive traffic ratio set to $RATIO"

# ç­‰å¾… 1 åˆ†é˜è§€å¯Ÿ
sleep 60

# æª¢æŸ¥æŒ‡æ¨™
curl http://admin-api/metrics/ab-comparison | jq .

echo "Monitor: http://grafana/dashboard/ab-testing"
```

---

### Stage 2: çµ„ä»¶é·ç§»é †åº

#### 2.1 Processor é·ç§»å„ªå…ˆç´š

**å„ªå…ˆç´š 1ï¼ˆWeek 3-4ï¼‰**ï¼š
- âœ… SearchProcessor - ä½é¢¨éšªï¼Œæ¸…æ™°çš„è¼¸å…¥è¼¸å‡º
- âœ… KnowledgeProcessor - ä½é¢¨éšªï¼Œè®€å–å‹æ“ä½œ

**å„ªå…ˆç´š 2ï¼ˆWeek 5-6ï¼‰**ï¼š
- âœ… ThinkingProcessor - ä¸­é¢¨éšªï¼Œå—ç›Šæ–¼å…ƒèªçŸ¥ç›£æ§
- âœ… CodeProcessor - ä¸­é¢¨éšªï¼Œéœ€è¦å“è³ªä¿è­‰

**å„ªå…ˆç´š 3ï¼ˆWeek 7-10ï¼‰**ï¼š
- âœ… CustomProcessor - é«˜é¢¨éšªï¼Œç”¨æˆ¶è‡ªå®šç¾©é‚è¼¯

#### 2.2 å–®å€‹ Processor é·ç§»æ¨¡æ¿

```python
# src/core/processors/search_processor.py

from src.core.feature_flags import feature_flags
from src.core.cognitive.global_workspace import GlobalWorkspace

class SearchProcessor:
    """Search Processor with cognitive workspace integration"""

    def __init__(self, workspace: Optional[GlobalWorkspace] = None):
        self.workspace = workspace
        self.tool = SearchTool()

    async def process(self, task: Task) -> ProcessingResult:
        """Process with optional workspace integration"""

        # åŸ·è¡Œæœå°‹
        results = await self.tool.run(task.query)

        # å¦‚æœå•Ÿç”¨å·¥ä½œç©ºé–“ï¼Œå»£æ’­çµæœ
        if self.workspace and feature_flags.is_enabled("cognitive.global_workspace"):
            self._broadcast_to_workspace(results, task)

        return ProcessingResult(
            content=results,
            confidence=0.8,
            metadata={"processor": "SearchProcessor"}
        )

    def _broadcast_to_workspace(self, results: Any, task: Task):
        """å»£æ’­åˆ°å·¥ä½œç©ºé–“ï¼ˆæ–°åŠŸèƒ½ï¼‰"""
        from src.core.cognitive.cognitive_item import CognitiveItem, ItemType

        item = CognitiveItem(
            id=f"search_{uuid.uuid4()}",
            type=ItemType.OBSERVATION,
            content=results,
            confidence=0.8,
            priority=0.7,
            timestamp=datetime.now(),
            source="SearchProcessor",
            tags=["search_result", "external_info"],
            metadata={"query": task.query}
        )

        self.workspace.post(item)
        logger.debug(f"Broadcasted search results to workspace: {item.id}")
```

#### 2.3 é·ç§»é©—è­‰æ¸…å–®

æ¯å€‹ Processor é·ç§»å¾Œå¿…é ˆé€šéï¼š

```python
# tests/migration/test_processor_migration.py

class TestSearchProcessorMigration:
    """é©—è­‰ SearchProcessor é·ç§»"""

    def test_backward_compatibility(self):
        """å‘å¾Œå…¼å®¹æ€§ï¼šé—œé–‰å·¥ä½œç©ºé–“æ™‚è¡Œç‚ºèˆ‡èˆŠç‰ˆä¸€è‡´"""
        feature_flags.disable("cognitive.global_workspace")

        processor = SearchProcessor()
        result = await processor.process(simple_task)

        # èˆ‡èˆŠç‰ˆ baseline æ¯”è¼ƒ
        assert result.content == baseline_result.content

    def test_workspace_integration(self):
        """å·¥ä½œç©ºé–“æ•´åˆï¼šå•Ÿç”¨æ™‚æ­£ç¢ºå»£æ’­"""
        feature_flags.enable("cognitive.global_workspace")
        workspace = GlobalWorkspace()

        processor = SearchProcessor(workspace=workspace)
        result = await processor.process(simple_task)

        # é©—è­‰å»£æ’­
        assert len(workspace.blackboard.items) > 0
        assert any(item.source == "SearchProcessor" for item in workspace.blackboard.items.values())

    def test_performance_overhead(self):
        """æ€§èƒ½é–‹éŠ·ï¼šå•Ÿç”¨å·¥ä½œç©ºé–“å¾Œé–‹éŠ· <10%"""
        baseline_time = measure_time(lambda: processor.process(task))

        feature_flags.enable("cognitive.global_workspace")
        cognitive_time = measure_time(lambda: processor.process(task))

        overhead = (cognitive_time - baseline_time) / baseline_time
        assert overhead < 0.1, f"Performance overhead too high: {overhead:.1%}"
```

---

## é¢¨éšªç®¡ç†

### é¢¨éšªçŸ©é™£

| é¢¨éšª | å¯èƒ½æ€§ | å½±éŸ¿ | ç­‰ç´š | ç·©è§£æªæ–½ |
|-----|-------|------|------|---------|
| R1: æ€§èƒ½åš´é‡é€€åŒ– | ä¸­ | é«˜ | ğŸ”´ é«˜ | æ€§èƒ½æ¸¬è©¦é–˜é–€ + è‡ªå‹•å›æ»¾ |
| R2: éŒ¯èª¤ç‡ä¸Šå‡ | ä¸­ | é«˜ | ğŸ”´ é«˜ | A/B æ¸¬è©¦ + å“è³ªé–˜é–€ |
| R3: å…§å­˜æ´©æ¼ | ä½ | é«˜ | ğŸŸ¡ ä¸­ | å…§å­˜ç›£æ§ + å®šæœŸé‡å•Ÿ |
| R4: ä¸¦ç™¼ bugï¼ˆæ­»é–/ç«¶æ…‹ï¼‰ | ä¸­ | ä¸­ | ğŸŸ¡ ä¸­ | å£“åŠ›æ¸¬è©¦ + è¶…æ™‚ä¿è­· |
| R5: é…ç½®éŒ¯èª¤å°è‡´æœå‹™ä¸­æ–· | ä½ | é«˜ | ğŸŸ¡ ä¸­ | é…ç½®é©—è­‰ + é›™ä»½éƒ¨ç½² |
| R6: å­¸ç¿’æ›²ç·šå½±éŸ¿ç”Ÿç”¢åŠ› | é«˜ | ä½ | ğŸŸ¢ ä½ | åŸ¹è¨“ + æ–‡æª” |

### å›æ»¾è§¸ç™¼æ¢ä»¶

**è‡ªå‹•å›æ»¾**ï¼ˆç„¡éœ€äººå·¥å¹²é ï¼‰ï¼š
```python
# src/monitoring/auto_rollback.py

class AutoRollbackMonitor:
    """è‡ªå‹•å›æ»¾ç›£æ§å™¨"""

    def __init__(self):
        self.thresholds = {
            "error_rate": 0.15,  # 15% éŒ¯èª¤ç‡
            "p95_latency": 30.0,  # 30s P95 å»¶é²
            "memory_usage": 0.9,  # 90% å…§å­˜ä½¿ç”¨
            "cpu_usage": 0.9,     # 90% CPU ä½¿ç”¨
        }

    def check_and_rollback(self) -> bool:
        """æª¢æŸ¥æŒ‡æ¨™ä¸¦æ±ºå®šæ˜¯å¦è‡ªå‹•å›æ»¾"""
        current_metrics = metrics_collector.get_current()

        # æª¢æŸ¥æ¯å€‹é–¾å€¼
        if current_metrics["error_rate"] > self.thresholds["error_rate"]:
            logger.critical("AUTO ROLLBACK: Error rate exceeded threshold")
            self._execute_rollback("error_rate_high")
            return True

        if current_metrics["p95_latency"] > self.thresholds["p95_latency"]:
            logger.critical("AUTO ROLLBACK: P95 latency exceeded threshold")
            self._execute_rollback("latency_high")
            return True

        # ... å…¶ä»–æª¢æŸ¥

        return False

    def _execute_rollback(self, reason: str):
        """åŸ·è¡Œå›æ»¾"""
        # 1. ç¦ç”¨æ‰€æœ‰èªçŸ¥åŠŸèƒ½
        feature_flags.disable("cognitive.enabled")

        # 2. å°‡æµé‡å…¨éƒ¨åˆ‡æ›åˆ°èˆŠç³»çµ±
        traffic_splitter.set_traffic_ratio(0.0)

        # 3. ç™¼é€å‘Šè­¦
        alert_manager.send_critical_alert(
            title="Auto Rollback Triggered",
            reason=reason,
            metrics=metrics_collector.get_current()
        )

        # 4. è¨˜éŒ„äº‹ä»¶
        logger.critical(f"Auto rollback executed: {reason}")
```

**æ‰‹å‹•å›æ»¾**ï¼ˆéœ€è¦äººå·¥åˆ¤æ–·ï¼‰ï¼š
- ç”¨æˆ¶æ»¿æ„åº¦ä¸‹é™ >10%
- å‡ºç¾æœªé æœŸçš„è¡Œç‚ºæ¨¡å¼
- åœ˜éšŠåé¥‹è² é¢

### å›æ»¾è…³æœ¬

```bash
#!/bin/bash
# scripts/emergency_rollback.sh

echo "ğŸš¨ EMERGENCY ROLLBACK INITIATED"

# 1. ç¦ç”¨æ‰€æœ‰èªçŸ¥åŠŸèƒ½
echo "Disabling cognitive features..."
kubectl set env deployment/openagent COGNITIVE_ENABLED=false

# 2. åˆ‡æ›æµé‡åˆ°èˆŠç³»çµ±
echo "Switching traffic to legacy system..."
curl -X POST http://admin-api/config/traffic-ratio -d '{"ratio": 0.0}'

# 3. é‡å•Ÿæœå‹™ï¼ˆç¢ºä¿é…ç½®ç”Ÿæ•ˆï¼‰
echo "Restarting services..."
kubectl rollout restart deployment/openagent

# 4. é©—è­‰
echo "Waiting for rollout..."
kubectl rollout status deployment/openagent

# 5. æª¢æŸ¥æŒ‡æ¨™
echo "Checking metrics..."
sleep 30
curl http://admin-api/metrics/health | jq .

echo "âœ… Rollback complete. Monitor: http://grafana/dashboard/health"
```

---

## æ•¸æ“šé·ç§»

### æƒ…ç¯€è¨˜æ†¶æ•¸æ“šé·ç§»

```python
# scripts/migrate_episodic_memory.py

"""
å°‡èˆŠçš„ cache æ•¸æ“šé·ç§»åˆ°æ–°çš„æƒ…ç¯€è¨˜æ†¶ç³»çµ±

å¾: Redis cache (key-value)
åˆ°: EpisodicMemory (çµæ§‹åŒ–çš„ Episode)
"""

import asyncio
from src.core.memory.episodic_memory import EpisodicMemory, Episode
from src.legacy.cache import LegacyCache

async def migrate_cache_to_episodic():
    """é·ç§» cache åˆ°æƒ…ç¯€è¨˜æ†¶"""
    legacy_cache = LegacyCache()
    episodic_memory = EpisodicMemory()

    # ç²å–æ‰€æœ‰ cache æ¢ç›®
    cache_entries = legacy_cache.get_all()

    migrated_count = 0
    for entry in cache_entries:
        # å°‡ cache æ¢ç›®è½‰æ›ç‚º Episode
        episode = Episode(
            item=CognitiveItem(
                id=entry["id"],
                type=ItemType.OBSERVATION,
                content=entry["content"],
                confidence=0.7,  # é»˜èªä¿¡å¿ƒåº¦
                priority=0.5,
                timestamp=entry["timestamp"],
                source="LegacyCache",
                tags=["migrated"],
                metadata={"original_key": entry["key"]}
            ),
            context={},
            timestamp=entry["timestamp"]
        )

        # å­˜å„²åˆ°æƒ…ç¯€è¨˜æ†¶
        episodic_memory.store(episode)
        migrated_count += 1

    print(f"âœ… Migrated {migrated_count} cache entries to episodic memory")

if __name__ == "__main__":
    asyncio.run(migrate_cache_to_episodic())
```

---

## é©—è­‰èˆ‡æ¸¬è©¦

### å†’ç…™æ¸¬è©¦ï¼ˆSmoke Testï¼‰

æ¯æ¬¡éƒ¨ç½²å¾Œå¿…é ˆé€šéçš„æœ€å°æ¸¬è©¦é›†ï¼š

```python
# tests/smoke/test_cognitive_smoke.py

class TestCognitiveSmoke:
    """å†’ç…™æ¸¬è©¦ï¼šç¢ºä¿åŸºæœ¬åŠŸèƒ½å¯ç”¨"""

    def test_basic_request(self):
        """åŸºæœ¬è«‹æ±‚ï¼šç³»çµ±å¯ä»¥éŸ¿æ‡‰"""
        response = client.post("/api/v1/process", json={
            "query": "What is 2+2?",
            "use_cognitive": True
        })

        assert response.status_code == 200
        assert "result" in response.json()

    def test_workspace_enabled(self):
        """å·¥ä½œç©ºé–“å•Ÿç”¨æ™‚æ­£å¸¸å·¥ä½œ"""
        feature_flags.enable("cognitive.global_workspace")

        response = client.post("/api/v1/process", json={
            "query": "Search for Python tutorials",
            "use_cognitive": True
        })

        assert response.status_code == 200
        # æª¢æŸ¥æ˜¯å¦æœ‰å·¥ä½œç©ºé–“æ´»å‹•
        workspace_metrics = metrics_collector.get_workspace_metrics()
        assert workspace_metrics["item_count"] > 0

    def test_metacog_enabled(self):
        """å…ƒèªçŸ¥å•Ÿç”¨æ™‚æ­£å¸¸å·¥ä½œ"""
        feature_flags.enable("cognitive.metacognition")

        response = client.post("/api/v1/process", json={
            "query": "Explain quantum computing",
            "use_cognitive": True
        })

        assert response.status_code == 200
        result = response.json()["result"]
        assert "confidence" in result
        assert 0 <= result["confidence"] <= 1

    def test_rollback_works(self):
        """å›æ»¾æ©Ÿåˆ¶å¯ç”¨"""
        # ç¦ç”¨èªçŸ¥åŠŸèƒ½
        feature_flags.disable("cognitive.enabled")

        response = client.post("/api/v1/process", json={
            "query": "Test query",
            "use_cognitive": True
        })

        # æ‡‰è©²è‡ªå‹• fallback åˆ°èˆŠç³»çµ±
        assert response.status_code == 200
        assert response.json()["engine"] == "legacy"
```

### æ€§èƒ½åŸºæº–æ¸¬è©¦

```python
# tests/benchmarks/test_performance_regression.py

class TestPerformanceRegression:
    """æ€§èƒ½å›æ­¸æ¸¬è©¦"""

    @pytest.mark.benchmark
    def test_latency_regression(self, baseline_metrics):
        """å»¶é²ä¸æ‡‰å›æ­¸ >10%"""
        # æ¸¬è©¦èªçŸ¥å¼•æ“
        cognitive_latency = benchmark(
            lambda: client.post("/api/v1/process", json={
                "query": "Sample query",
                "use_cognitive": True
            })
        )

        # èˆ‡åŸºç·šæ¯”è¼ƒ
        baseline_latency = baseline_metrics["p95_latency"]
        regression = (cognitive_latency - baseline_latency) / baseline_latency

        assert regression < 0.1, f"Latency regression: {regression:.1%}"

    @pytest.mark.benchmark
    def test_token_usage_regression(self, baseline_metrics):
        """Token ä½¿ç”¨é‡ä¸æ‡‰å¢åŠ  >20%"""
        # ... é¡ä¼¼æ¸¬è©¦
```

---

## ç›£æ§èˆ‡å‘Šè­¦

### é—œéµæŒ‡æ¨™å„€è¡¨æ¿

**Grafana Dashboard: Cognitive Architecture Migration**

```yaml
# config/grafana/dashboards/migration.json

{
  "dashboard": {
    "title": "Cognitive Architecture Migration",
    "panels": [
      {
        "title": "Traffic Split",
        "type": "gauge",
        "targets": [{
          "expr": "cognitive_traffic_ratio"
        }]
      },
      {
        "title": "Success Rate Comparison",
        "type": "graph",
        "targets": [
          {"expr": "rate(legacy_success_total[5m])", "legend": "Legacy"},
          {"expr": "rate(cognitive_success_total[5m])", "legend": "Cognitive"}
        ]
      },
      {
        "title": "P95 Latency Comparison",
        "type": "graph",
        "targets": [
          {"expr": "histogram_quantile(0.95, legacy_latency_bucket)", "legend": "Legacy"},
          {"expr": "histogram_quantile(0.95, cognitive_latency_bucket)", "legend": "Cognitive"}
        ]
      },
      {
        "title": "Error Rate",
        "type": "graph",
        "targets": [
          {"expr": "rate(legacy_errors_total[5m])", "legend": "Legacy"},
          {"expr": "rate(cognitive_errors_total[5m])", "legend": "Cognitive"}
        ]
      }
    ]
  }
}
```

### å‘Šè­¦è¦å‰‡

```yaml
# config/prometheus/alerts/cognitive_migration.yml

groups:
  - name: cognitive_migration
    rules:
      - alert: CognitiveErrorRateHigh
        expr: rate(cognitive_errors_total[5m]) > 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Cognitive engine error rate too high"
          description: "Error rate: {{ $value | humanizePercentage }}"

      - alert: CognitiveLatencyHigh
        expr: histogram_quantile(0.95, cognitive_latency_bucket) > 30
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Cognitive engine P95 latency too high"
          description: "P95 latency: {{ $value }}s"

      - alert: CognitiveSuccessRateLow
        expr: rate(cognitive_success_total[5m]) / rate(cognitive_requests_total[5m]) < 0.8
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Cognitive engine success rate too low"
          description: "Success rate: {{ $value | humanizePercentage }}"
```

---

## åœ˜éšŠåŸ¹è¨“

### åŸ¹è¨“è¨ˆåŠƒ

**Week 1: æ¶æ§‹æ¦‚è¦½**
- é¡äººé¡èªçŸ¥æ¶æ§‹çš„æ ¸å¿ƒæ¦‚å¿µ
- Global Workspace Theory
- Metacognition åŸç†
- å¯¦æ©Ÿæ¼”ç¤º

**Week 2: é–‹ç™¼å¯¦è¸**
- Feature Flag ä½¿ç”¨
- å¦‚ä½•é·ç§» Processor
- æ¸¬è©¦ç­–ç•¥
- å¯¦ä½œç·´ç¿’

**Week 3: é‹ç¶­å¯¦è¸**
- ç›£æ§æŒ‡æ¨™è§£è®€
- æ•…éšœæ’é™¤
- å›æ»¾æµç¨‹
- æ¼”ç·´

### åŸ¹è¨“ææ–™

```
docs/training/
â”œâ”€â”€ 01_architecture_overview.pdf
â”œâ”€â”€ 02_development_guide.md
â”œâ”€â”€ 03_operations_runbook.md
â”œâ”€â”€ 04_troubleshooting.md
â””â”€â”€ videos/
    â”œâ”€â”€ architecture_walkthrough.mp4
    â””â”€â”€ hands_on_migration.mp4
```

---

## æˆåŠŸæ¨™æº–

### æœ€çµ‚é©—æ”¶æ¨™æº–

åœ¨å®Œæˆ 100% æµé‡åˆ‡æ›å‰ï¼Œå¿…é ˆæ»¿è¶³ï¼š

âœ… **å“è³ªæŒ‡æ¨™**
- è¼¸å‡ºä¿¡å¿ƒåˆ†æ•¸ >0.8 ä½”æ¯” â‰¥ 80%
- å“è³ªé–˜é–€é€šéç‡ â‰¥ 90%
- éŒ¯èª¤ç‡ < 10%
- ç”¨æˆ¶æ»¿æ„åº¦æå‡ â‰¥ 20%

âœ… **æ€§èƒ½æŒ‡æ¨™**
- System 1 P95 å»¶é² < 3s
- System 2 P95 å»¶é² < 15s
- Token ä½¿ç”¨é‡é™ä½æˆ–æŒå¹³ï¼ˆÂ±5%ï¼‰

âœ… **ç©©å®šæ€§æŒ‡æ¨™**
- ç„¡ P0/P1 äº‹æ•…
- å¯ç”¨æ€§ â‰¥ 99.9%
- è‡ªå‹•å›æ»¾æ©Ÿåˆ¶é©—è­‰é€šé

âœ… **é‹ç¶­æŒ‡æ¨™**
- ç›£æ§è¦†è“‹ç‡ 100%
- å‘Šè­¦åŠæ™‚æ€§ < 5min
- æ•…éšœæ¢å¾©æ™‚é–“ < 15min

---

## æ™‚é–“è¡¨ç¸½è¦½

```
Week 1-2  â”â”â”â”â”â”â”â”â”â” Phase 0: æº–å‚™
Week 3-4  â”â”â”â”â”â”â”â”â”â” Phase 1: å·¥ä½œç©ºé–“ (5%)
Week 5-6  â”â”â”â”â”â”â”â”â”â” Phase 2: å…ƒèªçŸ¥ (10%)
Week 7-8  â”â”â”â”â”â”â”â”â”â” Phase 3: OODA (20%)
Week 9-10 â”â”â”â”â”â”â”â”â”â” Phase 4: è¨˜æ†¶ (30%)
Week 11-12â”â”â”â”â”â”â”â”â”â” Phase 5: ç¥ç¶“èª¿æ§ (50%)
Week 13-14â”â”â”â”â”â”â”â”â”â” Phase 6: äº‹ä»¶é©…å‹• (70%)
Week 15-16â”â”â”â”â”â”â”â”â”â” Phase 7: å…¨é‡ä¸Šç·š (100%)
```

---

## ä¸‹ä¸€æ­¥

- **[10_code_examples.md](./10_code_examples.md)**: å®Œæ•´çš„ç¨‹å¼ç¢¼ç¯„ä¾‹
- **[00_overview_and_vision.md](./00_overview_and_vision.md)**: è¿”å›ç¸½è¦½

---

**æ–‡æª”ç¶­è­·è€…**: OpenAgent Architecture Team
**å¯©æ ¸ç‹€æ…‹**: Pending Review
