"""
OpenCode Platform - æœ€çµ‚çµ±ä¸€å¼•æ“
èåˆ Deep Thinking æ¶æ§‹èˆ‡ Service æ¶æ§‹
Final Unified Engine combining both architectures
"""

import asyncio
import os
import sys
import json  # Added for parsing LLM responses
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from enum import Enum

# æ·»åŠ  utils åˆ°è·¯å¾‘


# æ·»åŠ  utils åˆ°è·¯å¾‘
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.logging_config import get_logger, LogContext, LogLevel

# å°å…¥ OpenAI LLM Client
from services.llm.openai_client import OpenAILLMClient

# ç²å–å°ˆç”¨ logger
logger = get_logger("FinalUnifiedEngine", LogLevel.INFO)


# ========================================
# çµ±ä¸€æ¨¡å¼å®šç¾© - èåˆå…©ç¨®æ¶æ§‹
# ========================================


class ProcessingMode(Enum):
    """çµ±ä¸€è™•ç†æ¨¡å¼ - èåˆ Thinking å’Œ Service æ¨¡å¼"""

    # Service æ¨¡å¼ï¼ˆåŸ·è¡Œå°å‘ï¼‰
    CHAT = "chat"  # AI å°è©±åŠŸèƒ½
    KNOWLEDGE = "knowledge"  # çŸ¥è­˜åº«æª¢ç´¢
    SANDBOX = "sandbox"  # ä»£ç¢¼åŸ·è¡Œ
    PLUGIN = "plugin"  # æ’ä»¶åŸ·è¡Œ

    QUICK = "quick"  # å¿«é€ŸéŸ¿æ‡‰ (1-2æ­¥)
    THINKING = "thinking"  # æ·±åº¦æ€è€ƒ (5-10æ­¥)
    RESEARCH = "research"  # ç ”ç©¶æ¨¡å¼ (10+æ­¥)

    # æ··åˆæ¨¡å¼
    HYBRID = "hybrid"  # æ€è€ƒ + åŸ·è¡Œ
    AUTO = "auto"  # è‡ªå‹•é¸æ“‡


class ThinkingDepth:
    """æ€è€ƒæ·±åº¦é…ç½® - ä¾†è‡ª unified_python_architecture.md"""

    SHALLOW = 1  # æ·ºå±¤ï¼š1-2æ­¥
    MEDIUM = 3  # ä¸­å±¤ï¼š3-5æ­¥
    DEEP = 5  # æ·±å±¤ï¼š5-10æ­¥
    RESEARCH = 10  # ç ”ç©¶ï¼š10+æ­¥


# ========================================
# æ•¸æ“šæ¨¡å‹
# ========================================


@dataclass
class UnifiedRequest:
    """çµ±ä¸€è«‹æ±‚æ ¼å¼ - æ”¯æ´æ‰€æœ‰æ¨¡å¼"""

    query: str
    mode: Optional[ProcessingMode] = None

    # Service åƒæ•¸
    context_id: Optional[str] = None
    model: str = "gpt-4o"
    temperature: float = 0.7
    max_tokens: int = 4000

    # Thinking åƒæ•¸
    thinking_depth: Optional[int] = None
    enable_reflection: bool = True

    # å…±ç”¨åƒæ•¸
    metadata: Optional[Dict[str, Any]] = None
    plugins: Optional[List[str]] = None


@dataclass
class UnifiedResponse:
    """çµ±ä¸€éŸ¿æ‡‰æ ¼å¼"""

    result: Any
    mode: ProcessingMode

    # Thinking ç›¸é—œ
    thinking_trace: Optional[List[str]] = None
    confidence: float = 1.0

    # Service ç›¸é—œ
    usage: Optional[Dict[str, int]] = None
    context_id: Optional[str] = None

    # å…ƒæ•¸æ“š
    metadata: Optional[Dict[str, Any]] = None


# ========================================
# æ€è€ƒå¼•æ“çµ„ä»¶ (Deep Thinking)
# ========================================


class ThinkingEngine:
    """æ·±åº¦æ€è€ƒå¼•æ“ - åŸºæ–¼ unified_python_architecture.md"""

    def __init__(self, llm_client = None):
        self.thinking_chain = None
        self.reflection_module = None
        self.critique_module = None
        # Use provided llm_client, even if it's None (for mock mode)
        self.llm_client = llm_client

    async def think_deeply(
        self, query: str, depth: int = ThinkingDepth.MEDIUM, enable_reflection: bool = True
    ) -> Dict[str, Any]:
        """åŸ·è¡Œæ·±åº¦æ€è€ƒéç¨‹"""
        logger.info(
            f"ğŸ§  Starting deep thinking: query='{query[:50]}...', depth={depth}, reflection={enable_reflection}"
        )

        thinking_trace = []

        # Phase 1: å•é¡Œç†è§£èˆ‡åˆ†è§£
        with LogContext(logger, "Problem Understanding", query=query[:100]):
            thinking_trace.append(f"ğŸ“‹ åˆ†æå•é¡Œ: {query}")
            understanding = await self._understand_problem(query)
            logger.debug(f"Understanding result: {understanding}")
            thinking_trace.append(understanding)

        # Phase 2: å¤šæ­¥æ¨ç†ï¼ˆæ€è€ƒéˆï¼‰
        for step in range(depth):
            logger.debug(f"Thinking step {step+1}/{depth}")
            thinking_trace.append(f"ğŸ” æ€è€ƒæ­¥é©Ÿ {step+1}/{depth}")

            # ç”Ÿæˆæ€è€ƒ
            thought = await self._generate_thought(query, step)
            logger.debug(f"Generated thought at step {step+1}: {thought[:100]}")

            # è‡ªæˆ‘åæ€ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            if enable_reflection and step % 2 == 0:
                logger.debug(f"Applying self-reflection at step {step+1}")
                thinking_trace.append("ğŸ’­ è‡ªæˆ‘åæ€...")
                thought = await self._reflect_on_thought(thought)

        # Phase 3: ç¶œåˆèˆ‡ç¸½çµ
        with LogContext(logger, "Conclusion Synthesis", steps=len(thinking_trace)):
            thinking_trace.append("ğŸ“ ç¶œåˆçµè«–...")
            conclusion = await self._synthesize_conclusion(thinking_trace)
            confidence = 0.85 + (depth * 0.02)  # æ·±åº¦è¶Šæ·±ï¼Œä¿¡å¿ƒè¶Šé«˜

            logger.info(
                f"âœ… Thinking completed: confidence={confidence:.2f}, trace_length={len(thinking_trace)}"
            )

            return {
                "answer": conclusion,
                "thinking_trace": thinking_trace,
                "confidence": confidence,
            }

    async def _understand_problem(self, query: str) -> str:
        """ç†è§£å•é¡Œ - é€é LLM é€²è¡Œ"""
        if self.llm_client is None:
            # Mock implementation
            if "ä½ å¥½" in query or "hello" in query.lower():
                return "Greeting: User is saying hello"
            elif "?" in query or "ä»€éº¼" in query or "what" in query.lower():
                return "Question: User asking a question"
            else:
                return "Query: User making a request"

        prompt = f"Analyze the following user query to understand its intent and categorize it:\n'{query}'\n\nProvide a concise understanding (e.g., 'Greeting: User is saying hello', 'Question: User asking for definition')."
        return await self.llm_client.generate(prompt)

    async def _generate_thought(self, query: str, step: int) -> str:
        """ç”Ÿæˆæ€è€ƒæ­¥é©Ÿ - é€é LLM é€²è¡Œ"""
        if self.llm_client is None:
            # Mock implementation with varied thoughts
            thoughts = [
                f"æ­¥é©Ÿ {step + 1}: åˆ†ææŸ¥è©¢å…§å®¹",
                f"æ­¥é©Ÿ {step + 1}: ç†è§£ç”¨æˆ¶æ„åœ–",
                f"æ­¥é©Ÿ {step + 1}: æœé›†ç›¸é—œè³‡è¨Š",
                f"æ­¥é©Ÿ {step + 1}: æ•´ç†æ€è·¯",
                f"æ­¥é©Ÿ {step + 1}: æº–å‚™å›æ‡‰"
            ]
            return thoughts[min(step, len(thoughts) - 1)]

        prompt = f"Given the query: '{query}', and that this is thinking step {step + 1}, generate a concise thought or next step in the analysis process."
        return await self.llm_client.generate(prompt)

    async def _reflect_on_thought(self, thought: str) -> str:
        """åæ€æ€è€ƒ - é€é LLM é€²è¡Œ"""
        if self.llm_client is None:
            # Mock implementation
            return f"åæ€: {thought} - ç¢ºèªé‚è¼¯æ­£ç¢º"

        prompt = f"Critically reflect on the following thought to improve its quality or identify potential flaws:\n'{thought}'"
        return await self.llm_client.generate(prompt)

    async def _synthesize_conclusion(self, thinking_trace: List[str]) -> str:
        """ç¶œåˆçµè«– - é€é LLM é€²è¡Œ"""
        # Extract original query from the thinking trace
        original_query = ""
        for item in thinking_trace:
            if "åˆ†æå•é¡Œ:" in item or "analyzing" in item.lower():
                parts = item.split(":")
                if len(parts) > 1:
                    original_query = ":".join(parts[1:]).strip()
                    break

        prompt = f"Given the thinking trace:\n{chr(10).join(thinking_trace)}\n\nAnd the original query: '{original_query}'\n\nSynthesize a comprehensive and accurate conclusion or final answer. If the original query was a greeting, respond with a friendly greeting. If it was a status inquiry, provide a status report. If it was about features, describe them. Otherwise, provide a detailed expert response or deep analysis if relevant."

        if self.llm_client is None:
            # Mock implementation based on query type
            if "ä½ å¥½" in original_query or "hello" in original_query.lower():
                return "ä½ å¥½! æ­¡è¿ä½¿ç”¨ OpenCode Platformã€‚ç³»çµ±æ­£åœ¨æ¨¡æ“¬æ¨¡å¼ä¸‹é‹è¡Œã€‚"
            elif "ç‹€æ…‹" in original_query or "status" in original_query.lower():
                return "ç³»çµ±ç‹€æ…‹: é‹è¡Œä¸­ (æ¨¡æ“¬æ¨¡å¼)ã€‚æ‰€æœ‰æ ¸å¿ƒçµ„ä»¶å·²è¼‰å…¥ã€‚"
            elif "åŠŸèƒ½" in original_query or "feature" in original_query.lower():
                return "ç³»çµ±åŠŸèƒ½: æ€è€ƒå¼•æ“ã€æœå‹™ç®¡ç†å™¨ã€æ™ºèƒ½è·¯ç”±å™¨ (æ¨¡æ“¬æ¨¡å¼)ã€‚"
            elif "æ·±åº¦åˆ†æ" in original_query:
                return f"æ·±åº¦åˆ†æçµæœ (æ¨¡æ“¬): é—œæ–¼ '{original_query}' çš„åˆ†æå·²å®Œæˆã€‚é€™æ˜¯ä¸€å€‹è¤‡é›œçš„ä¸»é¡Œéœ€è¦å¤šå±¤æ¬¡çš„ç†è§£ã€‚"
            else:
                return f"è™•ç†æŸ¥è©¢ '{original_query}' (æ¨¡æ“¬æ¨¡å¼): ç³»çµ±å·²åˆ†ææ‚¨çš„è«‹æ±‚ä¸¦æº–å‚™äº†å›æ‡‰ã€‚"

        return await self.llm_client.generate(prompt)


# ========================================
# æœå‹™ç®¡ç†å™¨ (Service Layer)
# ========================================


class ServiceManager:
    """æœå‹™ç®¡ç†å™¨ - ç®¡ç†æ‰€æœ‰æœå‹™"""

    def __init__(self):
        self.services = {}
        self.llm_providers = {}
        self.plugins = {}

    async def initialize(self):
        """åˆå§‹åŒ–æ‰€æœ‰æœå‹™"""
        logger.info("ğŸ”§ Initializing Service Manager...")

        # åˆå§‹åŒ–æœå‹™
        await self._init_knowledge_service()
        await self._init_sandbox_service()
        await self._init_search_service()
        await self._init_llm_providers()

    async def _init_knowledge_service(self):
        """åˆå§‹åŒ–çŸ¥è­˜åº«æœå‹™"""
        try:
            logger.debug("Attempting to initialize knowledge service...")
            from services.knowledge.service import KnowledgeBaseService

            self.services["knowledge"] = KnowledgeBaseService()
            logger.info("âœ“ Knowledge service initialized")
        except Exception as e:
            logger.warning(f"âš ï¸ Knowledge service not available: {e}")

    async def _init_sandbox_service(self):
        """åˆå§‹åŒ–æ²™ç®±æœå‹™"""
        try:
            from services.sandbox.service import SandboxService

            self.services["sandbox"] = SandboxService()
            logger.info("âœ“ Sandbox service initialized")
        except Exception as e:
            logger.warning(f"Sandbox service not available: {e}")

    async def _init_search_service(self):
        """åˆå§‹åŒ–æœç´¢æœå‹™"""
        try:
            from services.search.service import WebSearchService

            self.services["search"] = WebSearchService()
            logger.info("âœ“ Search service initialized")
        except Exception as e:
            logger.warning(f"Search service not available: {e}")

    async def _init_llm_providers(self):
        """åˆå§‹åŒ– LLM æä¾›è€…"""
        # ç°¡åŒ–çš„ LLM æä¾›è€…
        self.llm_providers["default"] = {
            "model": "gpt-4o",
            "api_key": os.getenv("OPENAI_API_KEY", ""),
        }

    async def execute_service(self, service_name: str, params: Dict[str, Any]) -> Any:
        """åŸ·è¡Œæœå‹™"""
        logger.debug(f"ğŸš€ Executing service: {service_name}")

        if service_name in self.services:
            with LogContext(
                logger, f"Service Execution: {service_name}", params_keys=list(params.keys())
            ):
                service = self.services[service_name]
                # èª¿ç”¨æœå‹™çš„åŸ·è¡Œæ–¹æ³•
                result = await service.execute(params)
                logger.debug(f"Service {service_name} completed successfully")
                return result
        else:
            logger.warning(f"âš ï¸ Service not found: {service_name}")
            return None


# ========================================
# æ™ºèƒ½è·¯ç”±å™¨
# ========================================


class IntelligentRouter:
    """æ™ºèƒ½è·¯ç”±å™¨ - åˆ†æè¤‡é›œåº¦ä¸¦é¸æ“‡è™•ç†æ¨¡å¼"""

    def __init__(self, llm_client = None):
        # Use provided llm_client, even if it's None (for mock mode)
        self.llm_client = llm_client

    async def analyze_complexity(self, query: str) -> Dict[str, float]:
        """åˆ†ææŸ¥è©¢è¤‡é›œåº¦ - é€é LLM é€²è¡Œ"""
        # If no LLM client, use heuristic-based complexity analysis
        if self.llm_client is None:
            query_lower = query.lower()
            # Simple heuristic complexity analysis
            complexity = {
                "reasoning_required": 0.3,
                "multi_step": 0.2,
                "domain_knowledge": 0.2,
                "creativity": 0.1,
                "research_needed": 0.1,
            }

            # Increase complexity for certain keywords
            if any(word in query_lower for word in ['æ·±åº¦åˆ†æ', 'deep', 'è©³ç´°', 'detailed']):
                complexity["reasoning_required"] = 0.8
                complexity["multi_step"] = 0.7
            if any(word in query_lower for word in ['ç ”ç©¶', 'research', 'èª¿æŸ¥']):
                complexity["research_needed"] = 0.8

            return complexity

        prompt = f"""Analyze the complexity of the following query across several dimensions:
Query: '{query}'

Provide your analysis as a JSON object with the following keys and float values between 0.0 and 1.0 (where 1.0 is highest complexity):
{{
    "reasoning_required": float,  // How much logical inference is needed?
    "multi_step": float,          // Does it require multiple distinct steps or sub-tasks?
    "domain_knowledge": float,    // How much specialized knowledge is required?
    "creativity": float,          // Does it require novel or creative solutions?
    "research_needed": float      // Does it require external information retrieval?
}}
Example: {{"reasoning_required": 0.8, "multi_step": 0.7, "domain_knowledge": 0.5, "creativity": 0.3, "research_needed": 0.9}}
"""
        # For now, the mock client will return a dummy string, parse it later
        llm_response = await self.llm_client.generate(prompt)
        # In a real scenario, you'd parse the JSON from the LLM response
        try:
            # Attempt to parse as JSON. If it fails, return a default complexity.
            # The MockLLMClient currently returns a string, so this will likely fail
            # unless the MockLLMClient is updated to return valid JSON strings for this prompt.
            complexity = json.loads(llm_response)
            if not isinstance(complexity, dict) or not all(
                isinstance(v, (int, float)) for v in complexity.values()
            ):
                raise ValueError("LLM did not return a valid complexity dictionary.")
            return complexity
        except (json.JSONDecodeError, ValueError):
            logger.warning(
                f"LLM did not return parsable JSON for complexity analysis. Returning default scores. Response: {llm_response[:100]}..."
            )
            return {
                "reasoning_required": 0.5,
                "multi_step": 0.5,
                "domain_knowledge": 0.5,
                "creativity": 0.5,
                "research_needed": 0.5,
            }

    async def select_mode(
        self, query: str, hint: Optional[ProcessingMode] = None
    ) -> ProcessingMode:
        """é¸æ“‡æœ€ä½³è™•ç†æ¨¡å¼ - é€é LLM é€²è¡Œ"""
        logger.debug(f"ğŸ¤” Selecting processing mode for query: '{query[:50]}...'")

        # å¦‚æœæœ‰æ˜ç¢ºæŒ‡å®šï¼Œä½¿ç”¨æŒ‡å®šæ¨¡å¼
        if hint and hint != ProcessingMode.AUTO:
            logger.info(f"ğŸ¯ Using specified mode: {hint.value}")
            return hint

        # Use LLM to select the mode
        mode_options = ", ".join([f"'{mode.value}'" for mode in ProcessingMode])
        # If no LLM client available, use simple heuristics
        if self.llm_client is None:
            logger.debug("ğŸ­ Using mock mode selection (no LLM client)")
            # Simple heuristic-based mode selection
            query_lower = query.lower()
            if any(word in query_lower for word in ['æ·±åº¦åˆ†æ', 'deep analysis', 'è©³ç´°', 'detailed']):
                return ProcessingMode.THINKING
            elif any(word in query_lower for word in ['ç ”ç©¶', 'research', 'èª¿æŸ¥']):
                return ProcessingMode.RESEARCH
            elif any(word in query_lower for word in ['çŸ¥è­˜', 'knowledge', 'æŸ¥è©¢', 'search']):
                return ProcessingMode.KNOWLEDGE
            else:
                return ProcessingMode.QUICK

        prompt = f"""Given the user query: '{query}', recommend the most appropriate ProcessingMode from the following options: {mode_options}.
Consider the inherent complexity of the query.

Provide only the recommended ProcessingMode value (e.g., 'thinking', 'chat', 'research')."""

        llm_recommendation = await self.llm_client.generate(prompt)
        llm_recommendation = llm_recommendation.strip().lower()  # Clean up the response

        # Try to match LLM's recommendation to a valid ProcessingMode
        for mode in ProcessingMode:
            if mode.value == llm_recommendation:
                logger.info(f"ğŸ¤– LLM-selected mode: {mode.value}")
                return mode

        # Fallback to a default mode if LLM's recommendation is invalid or unexpected
        logger.warning(
            f"LLM returned an invalid mode: '{llm_recommendation}'. Falling back to CHAT mode."
        )
        return ProcessingMode.CHAT

    def get_thinking_depth(self, mode: ProcessingMode) -> int:
        """æ ¹æ“šæ¨¡å¼ç²å–æ€è€ƒæ·±åº¦"""
        depth_mapping = {
            ProcessingMode.QUICK: ThinkingDepth.SHALLOW,
            ProcessingMode.CHAT: ThinkingDepth.SHALLOW,
            ProcessingMode.THINKING: ThinkingDepth.DEEP,
            ProcessingMode.RESEARCH: ThinkingDepth.RESEARCH,
            ProcessingMode.HYBRID: ThinkingDepth.MEDIUM,
        }
        return depth_mapping.get(mode, ThinkingDepth.SHALLOW)


# ========================================
# æœ€çµ‚çµ±ä¸€å¼•æ“
# ========================================


class FinalUnifiedEngine:
    """
    æœ€çµ‚çµ±ä¸€å¼•æ“ - èåˆ Deep Thinking èˆ‡ Service Architecture

    æ ¸å¿ƒç†å¿µï¼š
    1. æ™ºèƒ½è·¯ç”±æ±ºå®šè™•ç†æ¨¡å¼
    2. æ€è€ƒå¼•æ“æä¾›æ·±åº¦æ¨ç†
    3. æœå‹™å±¤åŸ·è¡Œå…·é«”åŠŸèƒ½
    4. çµ±ä¸€æ¥å£å°å¤–æä¾›æœå‹™
    """

    def __init__(self):
        self.initialized = False

        # æ ¸å¿ƒçµ„ä»¶ - Try to create LLM client, use mock if no API key
        try:
            self.llm_client = OpenAILLMClient()
            logger.info("âœ… OpenAI LLM Client initialized successfully")
        except ValueError as e:
            logger.warning(f"âš ï¸ ç„¡æ³•åˆå§‹åŒ– OpenAI client: {e}. ä½¿ç”¨ Mock LLM Client")
            self.llm_client = None  # Will use mock responses

        self.router = IntelligentRouter(llm_client=self.llm_client)
        self.thinking_engine = ThinkingEngine(
            llm_client=self.llm_client
        )  # Pass it to ThinkingEngine
        self.service_manager = ServiceManager()

        # ç‹€æ…‹ç®¡ç†
        self.contexts = {}
        self.memory = {}

    async def initialize(self):
        """åˆå§‹åŒ–å¼•æ“"""
        logger.info("=" * 50)
        logger.info("ğŸš€ Initializing Final Unified Engine")
        logger.info("=" * 50)

        try:
            # åˆå§‹åŒ–æœå‹™ç®¡ç†å™¨
            await self.service_manager.initialize()

            # åˆå§‹åŒ–æ€è€ƒå¼•æ“
            # self.thinking_engine å·²åœ¨ __init__ ä¸­å‰µå»º

            self.initialized = True
            logger.info("âœ… Final Unified Engine initialized successfully")
            logger.debug(f"Services loaded: {list(self.service_manager.services.keys())}")
            logger.debug(f"LLM providers: {list(self.service_manager.llm_providers.keys())}")

        except Exception as e:
            logger.error(f"âŒ Failed to initialize engine: {e}", exc_info=True)
            raise

    async def process(self, request: UnifiedRequest) -> UnifiedResponse:
        """
        çµ±ä¸€è™•ç†å…¥å£ - æ™ºèƒ½è·¯ç”±åˆ°ä¸åŒè™•ç†å™¨
        """
        logger.info(f"ğŸ“¥ Received request: mode={request.mode}, query='{request.query[:50]}...'")

        if not self.initialized:
            logger.debug("Engine not initialized, initializing now...")
            await self.initialize()

        # æ™ºèƒ½é¸æ“‡è™•ç†æ¨¡å¼
        if request.mode is None or request.mode == ProcessingMode.AUTO:
            request.mode = await self.router.select_mode(request.query)

        logger.info(f"ğŸŒ Processing with mode: {request.mode.value}")

        # æ ¹æ“šæ¨¡å¼é¡å‹è·¯ç”±
        if request.mode in [ProcessingMode.QUICK, ProcessingMode.THINKING, ProcessingMode.RESEARCH]:
            # æ€è€ƒå°å‘æ¨¡å¼
            return await self._process_thinking_mode(request)

        elif request.mode in [
            ProcessingMode.CHAT,
            ProcessingMode.KNOWLEDGE,
            ProcessingMode.SANDBOX,
            ProcessingMode.PLUGIN,
        ]:
            # æœå‹™å°å‘æ¨¡å¼
            return await self._process_service_mode(request)

        elif request.mode == ProcessingMode.HYBRID:
            # æ··åˆæ¨¡å¼
            return await self._process_hybrid_mode(request)

        else:
            # é»˜èª CHAT æ¨¡å¼
            request.mode = ProcessingMode.CHAT
            return await self._process_service_mode(request)

    async def _process_thinking_mode(self, request: UnifiedRequest) -> UnifiedResponse:
        """è™•ç†æ€è€ƒå°å‘æ¨¡å¼"""
        with LogContext(logger, "Thinking Mode Processing", mode=request.mode.value):
            # ç²å–æ€è€ƒæ·±åº¦
            depth = request.thinking_depth or self.router.get_thinking_depth(request.mode)
            logger.debug(f"Using thinking depth: {depth}")

        # åŸ·è¡Œæ·±åº¦æ€è€ƒ
        result = await self.thinking_engine.think_deeply(
            query=request.query, depth=depth, enable_reflection=request.enable_reflection
        )

        response = UnifiedResponse(
            result=result["answer"],
            mode=request.mode,
            thinking_trace=result.get("thinking_trace"),
            confidence=result.get("confidence", 0.8),
            metadata={"depth": depth},
        )

        logger.info(f"âœ… Thinking mode completed: confidence={response.confidence:.2f}")
        return response

    async def _process_service_mode(self, request: UnifiedRequest) -> UnifiedResponse:
        """è™•ç†æœå‹™å°å‘æ¨¡å¼"""
        with LogContext(logger, "Service Mode Processing", mode=request.mode.value):
            service_mapping = {
                ProcessingMode.KNOWLEDGE: "knowledge",
                ProcessingMode.SANDBOX: "sandbox",
                ProcessingMode.PLUGIN: "plugin",
                ProcessingMode.CHAT: "chat",
            }

        service_name = service_mapping.get(request.mode, "chat")

        # æº–å‚™æœå‹™åƒæ•¸
        params = {
            "query": request.query,
            "model": request.model,
            "temperature": request.temperature,
            "max_tokens": request.max_tokens,
        }

        # åŸ·è¡Œæœå‹™
        if service_name in self.service_manager.services:
            result = await self.service_manager.execute_service(service_name, params)
        else:
            # ç•¶æœå‹™ä¸å¯ç”¨æ™‚ï¼Œä½¿ç”¨ LLM client è™•ç†æ‰€æœ‰æ¨¡å¼
            if self.llm_client is not None:
                logger.info(f"ğŸ“¡ Service '{service_name}' not available, using LLM fallback")

                # æ ¹æ“šä¸åŒæ¨¡å¼æ§‹å»ºé©ç•¶çš„æç¤ºè©
                if service_name == "knowledge":
                    prompt = f"è«‹è©³ç´°è§£é‡‹é€™å€‹å•é¡Œï¼Œæä¾›æº–ç¢ºå’Œæœ‰ç”¨çš„çŸ¥è­˜ï¼š{request.query}"
                elif service_name == "sandbox":
                    prompt = f"è«‹åˆ†æä¸¦å›ç­”é€™å€‹æŠ€è¡“å•é¡Œï¼š{request.query}"
                elif service_name == "plugin":
                    prompt = f"è«‹è™•ç†é€™å€‹è«‹æ±‚ï¼š{request.query}"
                else:  # chat æˆ–å…¶ä»–
                    prompt = request.query

                result = await self.llm_client.generate(prompt)
            else:
                # å®Œå…¨æ²’æœ‰ LLM æ™‚çš„å›é€€
                result = f"Service '{service_name}' is not available and no LLM configured."

        response = UnifiedResponse(
            result=result,
            mode=request.mode,
            context_id=request.context_id,
            usage={"tokens": 0},
            metadata={"service": service_name},
        )

        logger.info(f"âœ… Service mode completed: service={service_name}")
        return response

    async def _process_hybrid_mode(self, request: UnifiedRequest) -> UnifiedResponse:
        """è™•ç†æ··åˆæ¨¡å¼ - å…ˆæ€è€ƒå¾ŒåŸ·è¡Œ"""

        # Step 1: æ€è€ƒéšæ®µ
        thinking_result = await self.thinking_engine.think_deeply(
            query=request.query, depth=ThinkingDepth.MEDIUM, enable_reflection=True
        )

        # Step 2: åŸºæ–¼æ€è€ƒçµæœé¸æ“‡æœå‹™
        # é€™è£¡ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›æ‡‰è©²æ ¹æ“šæ€è€ƒçµæœå‹•æ…‹é¸æ“‡
        service_params = {"query": request.query, "thinking_context": thinking_result}

        # Step 3: åŸ·è¡Œæœå‹™
        service_result = await self.service_manager.execute_service("chat", service_params)

        # Step 4: æ•´åˆçµæœ
        return UnifiedResponse(
            result={
                "thought": thinking_result["answer"],
                "action": service_result,
                "summary": "Completed hybrid processing",
            },
            mode=ProcessingMode.HYBRID,
            thinking_trace=thinking_result.get("thinking_trace"),
            confidence=thinking_result.get("confidence", 0.8),
            metadata={"hybrid": True},
        )

    async def get_status(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±ç‹€æ…‹"""
        return {
            "initialized": self.initialized,
            "services": list(self.service_manager.services.keys()),
            "thinking_engine": "active" if self.thinking_engine else "inactive",
            "router": "active" if self.router else "inactive",
            "contexts": len(self.contexts),
            "supported_modes": [mode.value for mode in ProcessingMode],
        }


# ========================================
# ç°¡åŒ–çš„ Engine æ¥å£ï¼ˆä¿æŒå…¼å®¹ï¼‰
# ========================================


class Engine:
    """ç°¡åŒ–çš„ Engine é¡ï¼Œä¿æŒå‘å¾Œå…¼å®¹"""

    def __init__(self):
        self.unified_engine = FinalUnifiedEngine()

    async def initialize(self):
        """åˆå§‹åŒ–å¼•æ“"""
        await self.unified_engine.initialize()

    async def process(self, query: str) -> str:
        """è™•ç†æŸ¥è©¢ - ç°¡åŒ–æ¥å£"""
        request = UnifiedRequest(query=query, mode=ProcessingMode.AUTO)
        response = await self.unified_engine.process(request)

        # è¿”å›å­—ç¬¦ä¸²çµæœ
        if isinstance(response.result, str):
            return response.result
        elif isinstance(response.result, dict):
            # Prioritize 'answer', then 'summary', then a generic string representation of the dict
            return str(
                response.result.get("answer", response.result.get("summary", response.result))
            )
        return str(response.result)


# ========================================
# å°å‡º
# ========================================

__all__ = [
    "FinalUnifiedEngine",
    "Engine",
    "UnifiedRequest",
    "UnifiedResponse",
    "ProcessingMode",
    "ThinkingDepth",
    "IntelligentRouter",
    "ThinkingEngine",
    "ServiceManager",
]
