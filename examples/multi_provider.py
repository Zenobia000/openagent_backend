"""
Multi-Provider LLM Example
===========================

Demonstrates multi-provider fallback chain with OpenCode Platform.

This example shows:
- Automatic provider fallback
- Provider priority configuration
- Error handling and retry
- Cost optimization

Requirements:
    - At least one LLM API key (OpenAI, Anthropic, or Gemini)

Usage:
    python examples/multi_provider.py
"""

import sys
import os

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from services.llm.openai_client import OpenAILLMClient
from services.llm.anthropic_client import AnthropicLLMClient
from services.llm.gemini_client import GeminiLLMClient
from services.llm.multi_provider import MultiProviderLLMClient
from services.llm.errors import ProviderError


def main():
    """Demonstrate multi-provider fallback."""

    print("üöÄ Multi-Provider LLM Example\n")

    # Check available providers
    providers = []

    if os.getenv("OPENAI_API_KEY"):
        providers.append(OpenAILLMClient(api_key=os.getenv("OPENAI_API_KEY")))
        print("‚úÖ OpenAI configured")
    else:
        print("‚ö†Ô∏è  OpenAI not configured (OPENAI_API_KEY not set)")

    if os.getenv("ANTHROPIC_API_KEY"):
        providers.append(AnthropicLLMClient(api_key=os.getenv("ANTHROPIC_API_KEY")))
        print("‚úÖ Anthropic configured")
    else:
        print("‚ö†Ô∏è  Anthropic not configured (ANTHROPIC_API_KEY not set)")

    if os.getenv("GEMINI_API_KEY"):
        providers.append(GeminiLLMClient(api_key=os.getenv("GEMINI_API_KEY")))
        print("‚úÖ Gemini configured")
    else:
        print("‚ö†Ô∏è  Gemini not configured (GEMINI_API_KEY not set)")

    if not providers:
        print("\n‚ùå Error: No LLM providers configured")
        print("   Set at least one API key:")
        print("   - OPENAI_API_KEY")
        print("   - ANTHROPIC_API_KEY")
        print("   - GEMINI_API_KEY")
        return

    print(f"\nüìä Active providers: {len(providers)}")
    print()

    # Initialize multi-provider client
    multi_llm = MultiProviderLLMClient(providers=providers)

    # Example 1: Normal operation
    print("=" * 60)
    print("Example 1: Normal Operation (Primary Provider)")
    print("=" * 60)

    try:
        response = multi_llm.generate(
            prompt="Explain quantum computing in one sentence.",
            temperature=0.7,
            max_tokens=100
        )
        print(f"Response: {response}")
        print(f"Provider Used: {providers[0].__class__.__name__}")
    except Exception as e:
        print(f"‚ùå Error: {e}")
    print()

    # Example 2: Demonstrate fallback (simulated)
    print("=" * 60)
    print("Example 2: Fallback Chain Demonstration")
    print("=" * 60)
    print("(Simulating primary provider failure...)")
    print()

    # This is a conceptual example - actual implementation would require
    # mocking or intentionally causing a failure
    print("Fallback chain:")
    for i, provider in enumerate(providers, 1):
        print(f"  {i}. {provider.__class__.__name__}")
        if i < len(providers):
            print(f"     ‚Üì (if fails)")

    print("\n‚úÖ With multi-provider setup:")
    print(f"   - Availability: ~99.5%")
    print(f"   - Automatic retry on errors")
    print(f"   - No manual intervention needed")
    print()

    # Example 3: Streaming
    print("=" * 60)
    print("Example 3: Streaming with Fallback")
    print("=" * 60)

    try:
        print("Streaming response:")
        for chunk in multi_llm.generate_stream(
            prompt="Count from 1 to 5 slowly.",
            temperature=0.7
        ):
            print(chunk, end='', flush=True)
        print("\n")
    except Exception as e:
        print(f"‚ùå Error: {e}")
    print()

    # Example 4: Cost optimization insights
    print("=" * 60)
    print("Example 4: Cost Optimization")
    print("=" * 60)

    print("üí∞ Cost Comparison (per 1M tokens):")
    print("   OpenAI GPT-4o:     $5.00 (primary - highest quality)")
    print("   Anthropic Claude:  $3.00 (fallback 1)")
    print("   Google Gemini:     $0.70 (fallback 2)")
    print()
    print("Strategy:")
    print("   1. Use OpenAI for primary (best quality)")
    print("   2. Fallback to Anthropic on rate limits")
    print("   3. Gemini as last resort (cost-effective)")
    print()
    print("With intelligent routing:")
    print("   - Simple queries ‚Üí Cache (System 1) ‚Üí $0")
    print("   - Complex queries ‚Üí OpenAI (System 2) ‚Üí $5/M")
    print("   - Rate limited ‚Üí Anthropic ‚Üí $3/M")
    print()

    # Example 5: Error handling
    print("=" * 60)
    print("Example 5: Error Handling")
    print("=" * 60)

    print("Retryable errors (will fallback):")
    print("   ‚úÖ HTTP 429 (Rate Limit)")
    print("   ‚úÖ HTTP 503 (Service Unavailable)")
    print("   ‚úÖ Timeout")
    print()
    print("Non-retryable errors (will fail immediately):")
    print("   ‚ùå HTTP 401 (Invalid API Key)")
    print("   ‚ùå HTTP 400 (Invalid Request)")
    print("   ‚ùå Content Policy Violation")
    print()

    print("=" * 60)
    print("‚úÖ Multi-provider examples completed!")
    print("=" * 60)
    print(f"\nüìä Your Setup: {len(providers)} provider(s) configured")
    print("   Recommended: Configure all 3 for maximum reliability")


if __name__ == "__main__":
    main()
