#!/usr/bin/env python3
"""
OpenCode Platform - å–®ä¸€å…¥å£é»
ä½¿ç”¨æ ¸å¿ƒ logger.py çš„å°ˆæ¥­æ—¥èªŒç³»çµ±
"""

import asyncio
import os
import sys
import time
from pathlib import Path
from datetime import datetime

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
from dotenv import load_dotenv
load_dotenv()

# æ·»åŠ  src åˆ°è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent / "src"))

from core.engine import RefactoredEngine
from core.models import Request, ProcessingMode
from core.logger import structured_logger as logger
from services.llm import create_llm_client


async def chat_mode():
    """å°è©±æ¨¡å¼ - ä½¿ç”¨æ ¸å¿ƒ logger"""

    # åˆå§‹åŒ– LLM
    logger.info("="*50, "main", "initialize")
    logger.info("ğŸš€ Initializing OpenCode Platform", "main", "initialize")
    logger.info("="*50, "main", "initialize")

    try:
        llm_client = create_llm_client()
    except ValueError as e:
        logger.error(f"âŒ {e}", "main", "chat_mode")
        print(f"è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­ç½® LLM API Key (OPENAI_API_KEY, ANTHROPIC_API_KEY, æˆ– GEMINI_API_KEY)")
        return
    logger.info(f"âœ… LLM Client initialized: {llm_client.provider_name}", "main", "initialize")

    engine = RefactoredEngine(llm_client=llm_client)
    await engine.initialize()
    logger.info("âœ… AI Engine initialized successfully", "main", "initialize")

    # æ¨¡å¼æ˜ å°„
    modes = {
        "auto": ProcessingMode.AUTO,
        "chat": ProcessingMode.CHAT,
        "think": ProcessingMode.THINKING,
        "thinking": ProcessingMode.THINKING,
        "knowledge": ProcessingMode.KNOWLEDGE,
        "search": ProcessingMode.SEARCH,
        "code": ProcessingMode.CODE,
        "research": ProcessingMode.DEEP_RESEARCH,
        "deep": ProcessingMode.DEEP_RESEARCH,
        "deep_research": ProcessingMode.DEEP_RESEARCH,
    }

    print("\n" + "="*50)
    print("OpenCode Platform - Cognitive AI Engine")
    print("="*50)
    print("å‘½ä»¤:")
    print("  /mode <æ¨¡å¼> - åˆ‡æ›æ¨¡å¼ (auto/chat/thinking/knowledge/search/code/research)")
    print("  /help       - é¡¯ç¤ºå¹«åŠ©")
    print("  /exit       - é€€å‡º")
    print("-"*50)

    current_mode = ProcessingMode.AUTO
    session_start = datetime.now()
    query_count = 0

    while True:
        try:
            # é¡¯ç¤ºæç¤ºç¬¦
            prompt = f"[{current_mode.value}]> "
            raw_input = input(prompt).strip()
            # Sanitize surrogate characters from WSL2 terminal
            user_input = raw_input.encode('utf-8', errors='replace').decode('utf-8')

            # è™•ç†å‘½ä»¤
            if user_input.lower() in ['/exit', '/quit', 'exit', 'quit']:
                session_duration = (datetime.now() - session_start).seconds
                logger.info(
                    f"ğŸ‘‹ Session ended: duration={session_duration}s, queries={query_count}",
                    "main", "session_end"
                )
                print("ğŸ‘‹ å†è¦‹ï¼")
                break

            elif user_input.lower() == '/help':
                print("\nå¯ç”¨æ¨¡å¼:")
                print("  auto     - è‡ªå‹•åˆ†é¡ (Router æ ¹æ“šæŸ¥è©¢å…§å®¹é¸æ“‡æœ€ä½³æ¨¡å¼)")
                print("  â”€â”€â”€ System 1 (å¿«é€Ÿå›æ‡‰, å¯å¿«å–) â”€â”€â”€")
                print("  chat     - ä¸€èˆ¬å°è©±")
                print("  knowledge - çŸ¥è­˜æª¢ç´¢ (RAG)")
                print("  â”€â”€â”€ System 2 (æ·±åº¦åˆ†æ, å¤šæ­¥é©Ÿ) â”€â”€â”€")
                print("  thinking - æ·±åº¦æ€è€ƒ")
                print("  search   - ç¶²è·¯æœç´¢")
                print("  code     - ä»£ç¢¼åŸ·è¡Œ (Docker æ²™ç®±)")
                print("  â”€â”€â”€ Agent (æœ‰ç‹€æ…‹å·¥ä½œæµ, è‡ªå‹•é‡è©¦) â”€â”€â”€")
                print("  research - æ·±åº¦ç ”ç©¶ (å®Œæ•´ç ”ç©¶å ±å‘Š)\n")
                continue

            elif user_input.lower().startswith('/mode'):
                parts = user_input.split()
                if len(parts) > 1 and parts[1] in modes:
                    old_mode = current_mode.value
                    current_mode = modes[parts[1]]
                    logger.info(
                        f"ğŸ”„ Mode switched: {old_mode} -> {current_mode.value}",
                        "main", "mode_switch"
                    )
                    print(f"âœ… åˆ‡æ›åˆ° {current_mode.value} æ¨¡å¼\n")
                else:
                    logger.warning(
                        f"Invalid mode: {parts[1] if len(parts) > 1 else 'none'}",
                        "main", "mode_switch"
                    )
                    print(f"âŒ ç„¡æ•ˆæ¨¡å¼ã€‚å¯ç”¨: {', '.join(modes.keys())}\n")
                continue

            elif user_input:
                query_count += 1
                start_time = time.time()

                # è¨­ç½®è¿½è¹¤ ID
                request = Request(query=user_input, mode=current_mode)
                logger.set_trace(request.trace_id)

                # è¨˜éŒ„æ¥æ”¶è«‹æ±‚
                logger.info(
                    f"ğŸ“¥ Received request: mode={current_mode.value}, query='{user_input[:50]}...'",
                    "main", "process"
                )

                # é¡¯ç¤ºè™•ç†ç‹€æ…‹
                logger.info(f"ğŸŒ Processing with mode: {current_mode.value}", "main", "process")

                # æ ¹æ“šæ¨¡å¼é¡¯ç¤ºä¸åŒçš„è™•ç†è³‡è¨Š
                if current_mode == ProcessingMode.THINKING:
                    logger.info("ğŸ§  Starting deep thinking process...", "main", "thinking")
                elif current_mode == ProcessingMode.KNOWLEDGE:
                    logger.info("ğŸ“š Retrieving from knowledge base...", "main", "knowledge")
                elif current_mode == ProcessingMode.SEARCH:
                    logger.info("ğŸ” Searching web...", "main", "search")
                elif current_mode == ProcessingMode.CODE:
                    logger.info("ğŸ’» Preparing code execution...", "main", "code")
                elif current_mode == ProcessingMode.DEEP_RESEARCH:
                    logger.info("ğŸ”¬ Starting deep research process...", "main", "deep_research")

                # ä½¿ç”¨ logger çš„æ€§èƒ½æ¸¬é‡
                with logger.measure("process_request"):
                    response = await engine.process(request)

                # è¨ˆç®—è™•ç†æ™‚é–“
                elapsed_time = (time.time() - start_time) * 1000

                # è¨˜éŒ„å®Œæˆ
                logger.info(
                    f"âœ… Processing completed: time={elapsed_time:.0f}ms",
                    "main", "process"
                )

                # é¡¯ç¤ºçµæœ
                print("\n" + "="*50)
                print("ğŸ“Š å›æ‡‰:")
                print("="*50)
                print(response.result)
                print("="*50)

                # é¡¯ç¤ºè™•ç†è³‡è¨Š
                resolved_mode = response.mode
                cognitive = resolved_mode.cognitive_level
                print(f"\nğŸ“ˆ è™•ç†è³‡è¨Š:")
                print(f"  ğŸ§  èªçŸ¥å±¤ç´š: {cognitive} | æ¨¡å¼: {resolved_mode.value}" +
                      (f" (auto -> {resolved_mode.value})" if current_mode == ProcessingMode.AUTO else ""))
                print(f"  â±ï¸  è™•ç†æ™‚é–“: {elapsed_time:.0f}ms")
                print(f"  ğŸ“Š Token ä½¿ç”¨: {response.tokens_used if response.tokens_used > 0 else 'N/A'}")
                print(f"  ğŸ”— LLM æä¾›è€…: {llm_client.provider_name}")
                print(f"  ğŸ” è¿½è¹¤ ID: {request.trace_id[:8]}...")
                print()

                # æ¸…é™¤è¿½è¹¤ ID
                logger.clear_context()

        except KeyboardInterrupt:
            logger.warning("Session interrupted by user", "main", "interrupt")
            print("\nğŸ‘‹ å†è¦‹ï¼")
            break
        except Exception as e:
            logger.error(f"Error occurred: {str(e)}", "main", "error")
            logger.log_error(e)
            print(f"âŒ éŒ¯èª¤: {e}\n")


async def test_mode():
    """æ¸¬è©¦æ¨¡å¼ - é©—è­‰ç³»çµ±åŠŸèƒ½"""

    logger.info("ğŸ§ª Starting test suite...", "main", "test_mode")

    try:
        llm_client = create_llm_client()
    except ValueError as e:
        logger.error(f"âŒ {e}", "main", "test_mode")
        return
    engine = RefactoredEngine(llm_client=llm_client)
    await engine.initialize()

    tests = [
        ("Hello", ProcessingMode.CHAT),
        ("1+1=?", ProcessingMode.THINKING),
        ("What is RAG?", ProcessingMode.KNOWLEDGE),
        ("Explain quantum computing", ProcessingMode.DEEP_RESEARCH),
    ]

    for i, (query, mode) in enumerate(tests, 1):
        logger.info(
            f"Running test {i}/{len(tests)}: query='{query}', mode={mode.value}",
            "main", "test"
        )
        request = Request(query=query, mode=mode)

        with logger.measure(f"test_{i}"):
            response = await engine.process(request)

        logger.info(f"âœ… Test {i} passed - {len(response.result)} chars", "main", "test")

    logger.info("âœ… All tests completed successfully!", "main", "test_mode")


def main():
    """ä¸»å‡½æ•¸"""

    if len(sys.argv) > 1:
        command = sys.argv[1].lower()

        if command == "test":
            asyncio.run(test_mode())
        elif command == "help":
            print_help()
        else:
            print(f"æœªçŸ¥å‘½ä»¤: {command}")
            print_help()
    else:
        # é è¨­é€²å…¥å°è©±æ¨¡å¼
        asyncio.run(chat_mode())


def print_help():
    """é¡¯ç¤ºå¹«åŠ©"""
    print("""
OpenCode Platform - Cognitive AI Engine

ä½¿ç”¨æ–¹å¼:
  python main.py         # é€²å…¥å°è©±æ¨¡å¼ï¼ˆé è¨­ auto æ¨¡å¼ï¼‰
  python main.py test    # é‹è¡Œæ¸¬è©¦
  python main.py help    # é¡¯ç¤ºæ­¤å¹«åŠ©

å°è©±æ¨¡å¼å‘½ä»¤:
  /mode <æ¨¡å¼>  - åˆ‡æ›è™•ç†æ¨¡å¼
  /help        - é¡¯ç¤ºå¯ç”¨æ¨¡å¼èˆ‡èªçŸ¥å±¤ç´š
  /exit        - é€€å‡ºç¨‹å¼

å¯ç”¨æ¨¡å¼:
  auto     - è‡ªå‹•åˆ†é¡ (Router æ™ºæ…§é¸æ“‡)
  chat     - ä¸€èˆ¬å°è©±          [System 1]
  knowledge - çŸ¥è­˜æª¢ç´¢ (RAG)   [System 1]
  thinking - æ·±åº¦æ€è€ƒ           [System 2]
  search   - ç¶²è·¯æœç´¢           [System 2]
  code     - ä»£ç¢¼åŸ·è¡Œ           [System 2]
  research - æ·±åº¦ç ”ç©¶å ±å‘Š       [Agent]

èªçŸ¥æ¶æ§‹:
  System 1 - å¿«é€Ÿå›æ‡‰, å¯å¿«å–
  System 2 - æ·±åº¦åˆ†æ, å¤šæ­¥é©Ÿ
  Agent    - æœ‰ç‹€æ…‹å·¥ä½œæµ, è‡ªå‹•é‡è©¦

LLM æä¾›è€…:
  è‡ªå‹•åµæ¸¬ .env ä¸­å¯ç”¨çš„ API Key
  Fallback: OpenAI -> Anthropic -> Gemini
    """)


if __name__ == "__main__":
    main()