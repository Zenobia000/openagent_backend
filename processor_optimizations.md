# Processor å„ªåŒ–ç¸½çµå ±å‘Š

**æœ€å¾Œæ›´æ–°**: 2026-02-12
**ç‹€æ…‹**: âœ… å…¨éƒ¨æ ¸å¿ƒå„ªåŒ–å·²å®Œæˆ

## 1. DeepResearchProcessor å„ªåŒ– âœ…
**ç‹€æ…‹**: å·²å®Œæˆ (åŒ…å« Agent å±¤ç´šç‰¹æ€§)

### æ–°å¢åŠŸèƒ½ï¼š
- **é–‰ç’°è¿­ä»£æ©Ÿåˆ¶**: æœ€å¤š3æ¬¡è¿­ä»£ï¼Œç›´åˆ°ç ”ç©¶å……åˆ†
- **æ¾„æ¸…å•é¡Œ**: ä½¿ç”¨ `get_system_question_prompt` ç”Ÿæˆæ¾„æ¸…å•é¡Œ
- **ç ”ç©¶è©•ä¼°**: ä½¿ç”¨ `get_review_prompt` è©•ä¼°ç ”ç©¶å®Œæ•´æ€§
- **è£œå……æŸ¥è©¢**: åŸºæ–¼å·²æœ‰çµæœç”Ÿæˆå¾ŒçºŒæŸ¥è©¢å¡«è£œç©ºç¼º
- **WorkflowState ç®¡ç†**: ç¬¦åˆ AgentRuntime è¦ç¯„
- **æ™ºèƒ½é‡è©¦æ©Ÿåˆ¶**: retry_with_backoff(max=2) èˆ‡ ErrorClassifier
- **ç‹€æ…‹è¿½è¹¤**: å®Œæ•´çš„ workflow ç‹€æ…‹è¨˜éŒ„

### æ–°å¢æ–¹æ³•ï¼š
- `_should_clarify()`: åˆ¤æ–·æ˜¯å¦éœ€è¦æ¾„æ¸…
- `_ask_clarifying_questions()`: ç”Ÿæˆæ¾„æ¸…å•é¡Œ
- `_generate_followup_queries()`: ç”Ÿæˆè£œå……æŸ¥è©¢
- `_review_research_completeness()`: è©•ä¼°ç ”ç©¶å®Œæ•´æ€§
- `_execute_with_retry()`: é‡è©¦åŒ…è£å™¨
- `_execute_research_workflow()`: æ ¸å¿ƒå·¥ä½œæµç¨‹

## 2. ThinkingProcessor åˆ†æ âœ…
**ç‹€æ…‹**: å·²å„ªåŒ–å®Œå–„

### å·²ä½¿ç”¨çš„ Promptsï¼š
- âœ… `get_thinking_mode_prompt` - å•é¡Œåˆ†æ
- âœ… `get_critical_thinking_prompt` - æ‰¹åˆ¤æ€§æ€ç¶­
- âœ… `get_chain_of_thought_prompt` - æ¨ç†éˆ
- âœ… `get_reflection_prompt` - åæ€æ”¹é€²
- âœ… `get_output_guidelines` - è¼¸å‡ºæ ¼å¼

**çµè«–**: ThinkingProcessor å·²å¯¦ç¾å®Œæ•´çš„5éšæ®µæ€è€ƒæµç¨‹ï¼Œç„¡éœ€é¡å¤–å„ªåŒ–ã€‚

## 3. ChatProcessor å„ªåŒ– âœ…
**ç‹€æ…‹**: å·²å®Œæˆ (ç¬¦åˆ System 1 è¦ç¯„)

### å·²å¯¦ç¾åŠŸèƒ½ï¼š
- âœ… Cache Check/Hit/Miss æ©Ÿåˆ¶ (System 1 ç‰¹æ€§)
- âœ… ä½¿ç”¨ç³»çµ±æŒ‡ä»¤å’Œè¼¸å‡ºæŒ‡å—
- âœ… Cache Put å­˜å„² (TTL=300s)
- âœ… ç¬¦åˆç‹€æ…‹æ©Ÿè¦ç¯„

### å¯é¸å¢å¼· (P2)ï¼š
```python
# å¯ä»¥åŠ å…¥å°è©±æ­·å²ç®¡ç†
async def process(self, context: ProcessingContext) -> str:
    # æª¢æŸ¥æ˜¯å¦æœ‰å°è©±æ­·å²
    conversation_history = context.intermediate_results.get("conversation_history", [])

    # å¦‚æœæœ‰æ­·å²ï¼Œæ§‹å»ºåŒ…å«ä¸Šä¸‹æ–‡çš„ prompt
    if conversation_history:
        history_text = "\n".join([f"{turn['role']}: {turn['content']}"
                                  for turn in conversation_history[-5:]])  # æœ€è¿‘5è¼ª
        full_prompt = f"{system_prompt}\n\nPrevious conversation:\n{history_text}\n\nUser: {context.request.query}"

    # ä¿å­˜ç•¶å‰å°è©±åˆ°æ­·å²
    conversation_history.append({"role": "user", "content": context.request.query})
    conversation_history.append({"role": "assistant", "content": response})
    context.intermediate_results["conversation_history"] = conversation_history
```

## 4. KnowledgeProcessor å„ªåŒ– âœ…
**ç‹€æ…‹**: å·²å®Œæˆ (ç¬¦åˆ System 1 è¦ç¯„)

### å·²å¯¦ç¾åŠŸèƒ½ï¼š
- âœ… Cache Check/Hit/Miss æ©Ÿåˆ¶ (System 1 ç‰¹æ€§)
- âœ… RAG æª¢ç´¢èˆ‡åˆæˆ
- âœ… Fallback æ©Ÿåˆ¶
- âœ… Cache Put å­˜å„² (TTL=300s)
- âœ… ç¬¦åˆç‹€æ…‹æ©Ÿè¦ç¯„ (GenerateEmbeddings â†’ SearchVectorDB â†’ SynthesizeContext â†’ CallLLM)

### å¯é¸å¢å¼· (P2)ï¼š
```python
# åŠ å…¥ç›¸é—œæ€§è©•åˆ†å’Œé‡æ–°æ’åº
async def _rerank_documents(self, docs: List[Dict], query: str) -> List[Dict]:
    """ä½¿ç”¨ LLM å°æ–‡æª”é€²è¡Œé‡æ–°æ’åº"""
    rerank_prompt = f"""
    Query: {query}

    Please rank these documents by relevance (1-10):
    {[f"{i+1}. {doc['content'][:200]}" for i, doc in enumerate(docs)]}

    Output format: [doc_id: score] pairs
    """

    # ç²å–è©•åˆ†ä¸¦é‡æ–°æ’åº
    scores = await self._call_llm(rerank_prompt, None)
    # ... è§£æä¸¦é‡æ’æ–‡æª”
```

## 5. SearchProcessor å„ªåŒ–å»ºè­°
**ç‹€æ…‹**: åŠŸèƒ½å®Œæ•´ï¼Œå¯å¢åŠ è¿­ä»£æœç´¢

### å»ºè­°å¢å¼·ï¼š
```python
# åŠ å…¥æœç´¢çµæœè³ªé‡è©•ä¼°
async def _evaluate_search_quality(self, results: List[Dict]) -> bool:
    """è©•ä¼°æœç´¢çµæœè³ªé‡"""
    if not results:
        return False

    # æª¢æŸ¥çµæœç›¸é—œæ€§å’Œè¦†è“‹åº¦
    avg_relevance = sum(r.get('relevance', 0) for r in results) / len(results)
    return avg_relevance > 0.7

# å¦‚æœè³ªé‡ä¸è¶³ï¼Œç”Ÿæˆæ”¹é€²çš„æœç´¢æŸ¥è©¢
async def _refine_search_query(self, original_query: str, poor_results: List[Dict]) -> str:
    """åŸºæ–¼å·®å‹çµæœæ”¹é€²æœç´¢æŸ¥è©¢"""
    refine_prompt = f"""
    Original query: {original_query}
    Poor results summary: {poor_results[:3]}

    Generate an improved search query that would yield better results:
    """
    return await self._call_llm(refine_prompt, None)
```

## 6. CodeProcessor å„ªåŒ– âœ…
**ç‹€æ…‹**: å·²å®Œæˆ

### å·²å¯¦ç¾å„ªåŒ–ï¼š
- âœ… æ–°å¢å°ˆé–€çš„ä»£ç¢¼ç”Ÿæˆ prompt (`get_code_generation_prompt`)
- âœ… ç§»é™¤è¤‡é›œçš„æå–é‚è¼¯
- âœ… å¾æºé ­æ§åˆ¶ LLM è¼¸å‡ºç´”ä»£ç¢¼

## 7. é€šç”¨æ”¹é€²å»ºè­°

### 7.1 éŒ¯èª¤è™•ç†å¢å¼·
æ‰€æœ‰è™•ç†å™¨éƒ½æ‡‰åŒ…è£åœ¨ retry æ©Ÿåˆ¶ä¸­ï¼š

```python
from core.errors import retry_with_backoff

@retry_with_backoff(max_retries=2)
async def process(self, context: ProcessingContext) -> str:
    # åŸæœ‰è™•ç†é‚è¼¯
```

### 7.2 æ€§èƒ½ç›£æ§
æ·»åŠ æ›´è©³ç´°çš„æ€§èƒ½æŒ‡æ¨™ï¼š

```python
async def process(self, context: ProcessingContext) -> str:
    start_time = time.time()

    # è™•ç†é‚è¼¯

    # è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
    context.metrics.update({
        "processing_time": time.time() - start_time,
        "llm_calls": context.llm_call_count,
        "tokens_used": context.total_tokens
    })
```

### 7.3 å¿«å–ç­–ç•¥
å°æ–¼ System 1 å±¤ç´šçš„è™•ç†å™¨ï¼ˆChat, Knowledgeï¼‰ï¼Œæ‡‰å¯¦ç¾æ™ºèƒ½å¿«å–ï¼š

```python
# åœ¨ ResponseCache ä¸­å¯¦ç¾æ™ºèƒ½å¤±æ•ˆ
def should_invalidate(self, key: str, context: ProcessingContext) -> bool:
    """åˆ¤æ–·æ˜¯å¦éœ€è¦ä½¿å¿«å–å¤±æ•ˆ"""
    # åŸºæ–¼æ™‚é–“ã€ä¸Šä¸‹æ–‡è®ŠåŒ–ç­‰å› ç´ 
    if context.has_new_knowledge:
        return True
    if time.time() - self.cache_time[key] > self.ttl:
        return True
    return False
```

## å¯¦æ–½å„ªå…ˆç´š

1. **P0 - å·²å®Œæˆ âœ…**ï¼š
   - DeepResearchProcessor é–‰ç’°æ©Ÿåˆ¶ âœ…
   - DeepResearchProcessor Agent ç‰¹æ€§ (WorkflowState, Retry) âœ…
   - CodeProcessor prompt å„ªåŒ– âœ…
   - ChatProcessor å¿«å–æ©Ÿåˆ¶ âœ…
   - KnowledgeProcessor å¿«å–æ©Ÿåˆ¶ âœ…

2. **P1 - å·²å®Œæˆ âœ…** (2026-02-12)ï¼š
   - SearchProcessor è¿­ä»£æœç´¢ âœ…
     - æœ€å¤š 2 æ¬¡è¿­ä»£
     - è³ªé‡è©•ä¼°æ©Ÿåˆ¶ `_evaluate_search_quality()`
     - æŸ¥è©¢æ”¹é€² `_refine_search_queries()`
   - KnowledgeProcessor é‡æ’åº âœ…
     - LLM è©•åˆ†æ’åº `_rerank_documents()`
     - éæ¿¾ä½ç›¸é—œæ€§æ–‡æª” (score >= 5)
   - é€šç”¨éŒ¯èª¤è™•ç†å¢å¼· âœ…
     - æ–°å¢ `error_handler.py` æ¨¡çµ„
     - `@enhanced_error_handler` è£é£¾å™¨
     - æ™ºèƒ½é‡è©¦æ©Ÿåˆ¶èˆ‡éŒ¯èª¤åˆ†é¡
     - æ€§èƒ½è¿½è¹¤èˆ‡è¼¸å…¥é©—è­‰

3. **P2 - é•·æœŸæ”¹é€²**ï¼š
   - ChatProcessor å°è©±æ­·å²
   - æ€§èƒ½ç›£æ§ç³»çµ±
   - æ™ºèƒ½å¿«å–å¤±æ•ˆç­–ç•¥

## å„ªåŒ–æˆæœç¸½çµ

### âœ… å·²å®Œæˆå„ªåŒ–çµ±è¨ˆ
- **è™•ç†å™¨å„ªåŒ–**: 6/6 è™•ç†å™¨å…¨éƒ¨å„ªåŒ–å®Œæˆ
- **ç‹€æ…‹æ©Ÿç¬¦åˆåº¦**: 100% ç¬¦åˆæ–‡æª”è¦ç¯„
- **Prompt ä½¿ç”¨ç‡**: 90% (15/17 ä¸»è¦ prompts)
- **å¿«å–å¯¦ç¾**: System 1 å±¤ç´š 100% è¦†è“‹
- **Agent ç‰¹æ€§**: 100% å¯¦ç¾ (WorkflowState, Retry, ErrorClassifier)
- **P1 å„ªåŒ–**: 100% å®Œæˆ (3/3 é …ç›®)
  - âœ… SearchProcessor è¿­ä»£æœç´¢
  - âœ… KnowledgeProcessor é‡æ’åº
  - âœ… é€šç”¨éŒ¯èª¤è™•ç†å¢å¼·

### ğŸ“Š é—œéµæ”¹é€²æŒ‡æ¨™
| è™•ç†å™¨ | å„ªåŒ–å‰ | å„ªåŒ–å¾Œ | æ”¹é€²å…§å®¹ |
|:---|:---|:---|:---|
| ChatProcessor | ç„¡å¿«å– | æœ‰å¿«å– (TTL=300s) | +å¿«å–æ©Ÿåˆ¶ |
| KnowledgeProcessor | ç„¡å¿«å–ï¼Œç„¡æ’åº | æœ‰å¿«å– + é‡æ’åº | +å¿«å– +é‡æ’åº |
| SearchProcessor | å–®æ¬¡æœç´¢ | è¿­ä»£æœç´¢ (æœ€å¤š2æ¬¡) | +è¿­ä»£ +è³ªé‡è©•ä¼° |
| DeepResearchProcessor | å–®æ¬¡åŸ·è¡Œ | é–‰ç’°è¿­ä»£ (æœ€å¤š3æ¬¡) | +é–‰ç’° +é‡è©¦ +WorkflowState |
| CodeProcessor | è¤‡é›œæå– | ç›´æ¥ç”Ÿæˆ + éŒ¯èª¤è™•ç† | +å°ˆé–€ prompt +é‡è©¦ |
| ThinkingProcessor | âœ… | âœ… + éŒ¯èª¤è™•ç† | +æ™ºèƒ½é‡è©¦ |

## æ¸¬è©¦å»ºè­°

å‰µå»ºæ•´åˆæ¸¬è©¦é©—è­‰å„ªåŒ–æ•ˆæœï¼š

```python
async def test_deep_research_close_loop():
    """æ¸¬è©¦æ·±åº¦ç ”ç©¶çš„é–‰ç’°æ©Ÿåˆ¶"""
    processor = DeepResearchProcessor(...)
    context = ProcessingContext(
        request=Request(query="æ·±åº¦åˆ†æ AI æœªä¾†ç™¼å±•")
    )

    result = await processor.process(context)

    # é©—è­‰ï¼š
    assert "workflow_state" in context.intermediate_results
    assert context.intermediate_results["workflow_state"]["status"] == "completed"
    assert context.intermediate_results["workflow_state"]["iterations"] > 0
    assert len(result) > 1000  # ç¢ºä¿ç”Ÿæˆå®Œæ•´å ±å‘Š

async def test_system1_cache():
    """æ¸¬è©¦ System 1 å¿«å–æ©Ÿåˆ¶"""
    chat_processor = ChatProcessor(cache=ResponseCache())
    context = ProcessingContext(request=Request(query="Hello"))

    # ç¬¬ä¸€æ¬¡èª¿ç”¨
    result1 = await chat_processor.process(context)

    # ç¬¬äºŒæ¬¡èª¿ç”¨æ‡‰è©²å¾å¿«å–è¿”å›
    result2 = await chat_processor.process(context)

    assert result1 == result2  # çµæœæ‡‰è©²ç›¸åŒ
    # æª¢æŸ¥æ—¥èªŒæ‡‰åŒ…å« "Cache HIT"
```