# Processor State Machine é©—è­‰èˆ‡ä¿®æ­£å ±å‘Š

## 1. ChatProcessor é©—è­‰

### æ–‡æª”è¦æ±‚ (System 1, ModelRuntime, Cacheable):
- âœ… CacheCheck â†’ Cache HIT/MISS åˆ¤æ–·
- âœ… BuildPrompt â†’ çµ„åˆç³»çµ±æç¤ºè© + ç”¨æˆ¶æŸ¥è©¢
- âœ… CallLLM â†’ MultiProviderLLMClient.generate()
- âŒ CachePut â†’ ç¼ºå°‘å¿«å–å­˜å„²æ­¥é©Ÿ

### ç•¶å‰å¯¦ç¾å•é¡Œ:
```python
# ç•¶å‰æ²’æœ‰å¯¦ç¾å¿«å–æ©Ÿåˆ¶
async def process(self, context: ProcessingContext) -> str:
    # ç›´æ¥èª¿ç”¨ LLMï¼Œæ²’æœ‰å¿«å–æª¢æŸ¥
```

### ä¿®æ­£å»ºè­°:
```python
async def process(self, context: ProcessingContext) -> str:
    self.logger.progress("chat", "start")
    context.set_current_step("chat")

    # Step 1: Cache Check (System 1 ç‰¹æ€§)
    cache_key = f"chat:{context.request.query}"
    if self.cache and self.cache.get(cache_key):
        cached_response = self.cache.get(cache_key)
        self.logger.info("ğŸ’¾ Cache HIT", "chat", "cache_hit")
        return cached_response

    # Step 2: Build Prompt
    system_prompt = PromptTemplates.get_system_instruction()
    output_guidelines = PromptTemplates.get_output_guidelines()
    full_prompt = f"{system_prompt}\n\n{output_guidelines}\n\nUser: {context.request.query}"

    # Step 3: Call LLM
    response = await self._call_llm(full_prompt, context)

    # Step 4: Cache Put (System 1 ç‰¹æ€§)
    if self.cache:
        self.cache.put(cache_key, response, ttl=300)
        self.logger.info("ğŸ’¾ Cache PUT", "chat", "cache_put")

    self.logger.message(response)
    context.mark_step_complete("chat")
    self.logger.progress("chat", "end")
    return response
```

---

## 2. KnowledgeProcessor é©—è­‰

### æ–‡æª”è¦æ±‚ (System 1, ModelRuntime, Cacheable):
- âŒ CacheCheck â†’ ç¼ºå°‘å¿«å–æª¢æŸ¥
- âœ… GenerateEmbeddings â†’ ç”ŸæˆåµŒå…¥å‘é‡
- âœ… SearchVectorDB â†’ Qdrant æœç´¢
- âœ… SynthesizeContext â†’ çµ„åˆæª¢ç´¢çµæœ
- âœ… CallLLM â†’ ç”Ÿæˆç­”æ¡ˆ
- âŒ CachePut â†’ ç¼ºå°‘å¿«å–å­˜å„²

### ç•¶å‰ä½¿ç”¨çš„ Prompts:
- âœ… `get_search_knowledge_result_prompt` - çŸ¥è­˜åˆæˆ
- âœ… `get_citation_rules` - å¼•ç”¨è¦å‰‡
- âœ… `get_system_instruction` - ç³»çµ±æŒ‡ä»¤ (fallback)

### ä¿®æ­£å»ºè­°:
```python
async def process(self, context: ProcessingContext) -> str:
    self.logger.progress("knowledge-retrieval", "start")

    # Step 1: Cache Check (System 1)
    cache_key = f"knowledge:{context.request.query}"
    if self.cache and self.cache.get(cache_key):
        return self.cache.get(cache_key)

    # Step 2-4: ç¾æœ‰çš„ RAG æµç¨‹...
    # [ä¿æŒç¾æœ‰å¯¦ç¾]

    # Step 5: Cache Put
    if self.cache and response:
        self.cache.put(cache_key, response, ttl=300)

    return response
```

---

## 3. SearchProcessor é©—è­‰

### æ–‡æª”è¦æ±‚ (System 2, ModelRuntime, No Cache):
- âœ… GenerateSearchQueries â†’ ç”Ÿæˆå„ªåŒ–æŸ¥è©¢
- âœ… ExecuteSearches â†’ åŸ·è¡Œå¤šå¼•æ“æœç´¢
- âœ… SynthesizeResults â†’ æ•´åˆçµæœ
- âœ… CallLLM â†’ ç”Ÿæˆå ±å‘Š

### ç•¶å‰ä½¿ç”¨çš„ Prompts:
- âœ… `get_serp_queries_prompt` - SERP æŸ¥è©¢ç”Ÿæˆ
- âœ… `get_search_result_prompt` - æœç´¢çµæœè™•ç†
- âœ… `get_citation_rules` - å¼•ç”¨è¦å‰‡

### ç‹€æ…‹: âœ… ç¬¦åˆè¦ç¯„

---

## 4. CodeProcessor é©—è­‰

### æ–‡æª”è¦æ±‚ (System 2, ModelRuntime, No Cache):
- âœ… GenerateCode â†’ LLM ç”Ÿæˆä»£ç¢¼
- âœ… ExecuteInSandbox â†’ Docker åŸ·è¡Œ
- âœ… FormatSuccess/Error â†’ æ ¼å¼åŒ–çµæœ

### ç•¶å‰ä½¿ç”¨çš„ Prompts:
- âœ… `get_code_generation_prompt` - ä»£ç¢¼ç”Ÿæˆ

### ç‹€æ…‹: âœ… ç¬¦åˆè¦ç¯„

---

## 5. ThinkingProcessor é©—è­‰

### æ–‡æª”è¦æ±‚ (System 2, ModelRuntime, No Cache):
- âœ… ProblemAnalysis â†’ å•é¡Œåˆ†è§£
- âœ… MultiPerspective â†’ å¤šè§’åº¦åˆ†æ
- âœ… DeepReasoning â†’ æ¨ç†éˆ
- âœ… SynthesisAndReflection â†’ ç¶œåˆåæ€
- âœ… FinalAnswer â†’ æœ€çµ‚ç­”æ¡ˆ

### ç•¶å‰ä½¿ç”¨çš„ Prompts:
- âœ… `get_thinking_mode_prompt` - å•é¡Œåˆ†æ
- âœ… `get_critical_thinking_prompt` - æ‰¹åˆ¤æ€ç¶­
- âœ… `get_chain_of_thought_prompt` - æ¨ç†éˆ
- âœ… `get_reflection_prompt` - åæ€æ”¹é€²
- âœ… `get_output_guidelines` - è¼¸å‡ºæŒ‡å—

### ç‹€æ…‹: âœ… å®Œç¾ç¬¦åˆè¦ç¯„

---

## 6. DeepResearchProcessor é©—è­‰

### æ–‡æª”è¦æ±‚ (Agent, AgentRuntime, Stateful):
- âŒ InitWorkflow â†’ ç¼ºå°‘ WorkflowState åˆå§‹åŒ–
- âŒ RetryBoundary â†’ ç¼ºå°‘ retry_with_backoff åŒ…è£
- âœ… WriteReportPlan â†’ ç”Ÿæˆç ”ç©¶è¨ˆåŠƒ
- âœ… GenerateSearchQueries â†’ ç”ŸæˆæŸ¥è©¢
- âœ… ExecuteSearchTasks â†’ åŸ·è¡Œæœç´¢
- âœ… WriteFinalReport â†’ ç”Ÿæˆå ±å‘Š
- âŒ ErrorHandling â†’ ç¼ºå°‘ ErrorClassifier ä½¿ç”¨
- âŒ WorkflowComplete/Failed â†’ ç¼ºå°‘ç‹€æ…‹è¨˜éŒ„

### ç•¶å‰å¯¦ç¾å•é¡Œ:
1. æ²’æœ‰ä½¿ç”¨ AgentRuntime çš„ WorkflowState
2. æ²’æœ‰ retry_with_backoff åŒ…è£
3. æ²’æœ‰ä½¿ç”¨ ErrorClassifier

### ä¿®æ­£å»ºè­°:
```python
from core.errors import retry_with_backoff, ErrorClassifier

class DeepResearchProcessor(BaseProcessor):

    @retry_with_backoff(max_retries=2, base_delay=1.0)
    async def process(self, context: ProcessingContext) -> str:
        # Step 1: Init Workflow
        workflow_state = {
            "status": "running",
            "steps": ["plan", "search", "synthesize"],
            "current_step": None,
            "errors": []
        }
        context.intermediate_results["workflow_state"] = workflow_state

        try:
            # Step 2: åŸ·è¡Œç ”ç©¶æµç¨‹
            workflow_state["current_step"] = "plan"
            report_plan = await self._write_report_plan(context)

            workflow_state["current_step"] = "search"
            # [è¿­ä»£æœç´¢é‚è¼¯...]

            workflow_state["current_step"] = "synthesize"
            final_report = await self._write_final_report(...)

            # Step 3: Workflow Complete
            workflow_state["status"] = "completed"
            return final_report

        except Exception as e:
            # Step 4: Error Classification
            error_category = ErrorClassifier.classify(e)
            workflow_state["errors"].append({
                "error": str(e),
                "category": error_category,
                "step": workflow_state["current_step"]
            })

            if error_category in ["NETWORK", "LLM"]:
                raise  # Will be retried by decorator
            else:
                workflow_state["status"] = "failed"
                raise
```

---

## 7. ç¼ºå¤±çš„ Prompt æ•´åˆ

### æœªä½¿ç”¨ä½†æ‡‰è©²ä½¿ç”¨çš„ Prompts:

1. **DeepResearchProcessor**:
   - âœ… `get_system_question_prompt` - å·²åœ¨å„ªåŒ–ä¸­åŠ å…¥
   - âœ… `get_review_prompt` - å·²åœ¨å„ªåŒ–ä¸­åŠ å…¥

2. **SearchProcessor å¯é¸å¢å¼·**:
   - `get_query_result_prompt` - ç”¨æ–¼è™•ç†å–®å€‹æŸ¥è©¢çµæœ

3. **æ‰€æœ‰è™•ç†å™¨**:
   - `get_guidelines_prompt` - å¯åµŒå…¥å…¶ä»– prompts ä¸­

---

## ç¸½çµèˆ‡è¡Œå‹•è¨ˆåŠƒ

### å¿…é ˆä¿®æ­£ (P0):
1. **ChatProcessor** - åŠ å…¥å¿«å–æ©Ÿåˆ¶
2. **KnowledgeProcessor** - åŠ å…¥å¿«å–æ©Ÿåˆ¶
3. **DeepResearchProcessor** - åŠ å…¥ WorkflowState å’Œ retry æ©Ÿåˆ¶

### å»ºè­°å¢å¼· (P1):
1. çµ±ä¸€éŒ¯èª¤è™•ç†æ¨¡å¼
2. åŠ å¼·æ€§èƒ½ç›£æ§
3. å®Œå–„æ—¥èªŒè¨˜éŒ„

### é©—è­‰ç‹€æ…‹:
- âœ… å®Œå…¨ç¬¦åˆ: ThinkingProcessor, SearchProcessor, CodeProcessor
- âš ï¸ éœ€è¦ä¿®æ­£: ChatProcessor, KnowledgeProcessor
- âš ï¸ éœ€è¦é‡æ§‹: DeepResearchProcessor (Agent å±¤ç´šç‰¹æ€§)

## å¯¦æ–½å„ªå…ˆç´š

1. **ç«‹å³åŸ·è¡Œ**: ä¿®æ­£ System 1 è™•ç†å™¨çš„å¿«å–æ©Ÿåˆ¶
2. **çŸ­æœŸè¨ˆåŠƒ**: å®Œå–„ DeepResearchProcessor çš„ Agent ç‰¹æ€§
3. **é•·æœŸå„ªåŒ–**: çµ±ä¸€æ‰€æœ‰è™•ç†å™¨çš„éŒ¯èª¤è™•ç†å’Œç›£æ§