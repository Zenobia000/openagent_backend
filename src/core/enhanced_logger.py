"""
Enhanced Logging System with Long Content Support and Markdown Export
æ”¯æ´é•·å…§å®¹åˆ†å‰²è¨˜éŒ„å’Œ Markdown è¼¸å‡ºçš„å¢å¼·æ—¥èªŒç³»çµ±

DEPRECATION WARNING:
--------------------
This module is DEPRECATED and will be removed in a future version.

Please use src/core/logger.py (StructuredLogger) instead, which provides:
- Unified logging interface
- Better SSE event integration
- Simplified API
- Active maintenance and support

Migration Guide:
----------------
Old: enhanced_logger.log_long_content(...)
New: structured_logger.info(...) with automatic truncation

Old: enhanced_logger.save_response_as_markdown(...)
New: Markdown export is no longer part of core logging (use a separate service)

This logger remains available for backward compatibility but will not receive
new features or bug fixes.
"""

import warnings
import json
import logging
import hashlib
from typing import Dict, Any, Optional, List
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass, asdict
import textwrap

# Issue deprecation warning when this module is imported
warnings.warn(
    "enhanced_logger.py is deprecated. Use logger.py (StructuredLogger) instead.",
    DeprecationWarning,
    stacklevel=2
)


@dataclass
class ContentSegment:
    """å…§å®¹åˆ†æ®µ"""
    segment_id: str
    segment_index: int
    total_segments: int
    content: str
    checksum: str


class EnhancedLogger:
    """å¢å¼·å‹æ—¥èªŒç³»çµ± - æ”¯æ´é•·å…§å®¹è™•ç†"""

    # å–®å€‹æ—¥èªŒæ¢ç›®çš„æœ€å¤§å­—ç¬¦æ•¸
    MAX_LOG_SIZE = 10000  # 10KB per log entry

    # Markdown å ±å‘Šçš„æœ€å¤§ token ä¼°ç®—
    MAX_MARKDOWN_TOKENS = 100000  # ç´„ 100K tokens

    def __init__(self, base_path: Path = None):
        self.base_path = base_path or Path.cwd() / "logs"
        self.base_path.mkdir(parents=True, exist_ok=True)

        # å‰µå»ºå­ç›®éŒ„
        self.segments_dir = self.base_path / "segments"
        self.segments_dir.mkdir(exist_ok=True)

        self.reports_dir = self.base_path / "reports"
        self.reports_dir.mkdir(exist_ok=True)

        # è¨­ç½®åŸºç¤æ—¥èªŒ
        self.setup_logging()

    def setup_logging(self):
        """è¨­ç½®æ—¥èªŒç³»çµ±"""
        # ä¸»æ—¥èªŒæ–‡ä»¶
        log_file = self.base_path / f"opencode_{datetime.now().strftime('%Y%m%d')}.log"

        # é…ç½®æ ¼å¼
        formatter = logging.Formatter(
            '%(asctime)s %(levelname)-8s %(message)s',
            datefmt='%H:%M:%S'
        )

        # æ–‡ä»¶è™•ç†å™¨
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setFormatter(formatter)

        # File-only logger â€” console output is handled by StructuredLogger
        self.logger = logging.getLogger('opencode.enhanced')
        self.logger.setLevel(logging.INFO)
        self.logger.propagate = False
        self.logger.addHandler(file_handler)

    def log_long_content(self,
                        level: str,
                        message: str,
                        content: str,
                        trace_id: str,
                        category: str = "application") -> List[str]:
        """
        è¨˜éŒ„é•·å…§å®¹ï¼Œè‡ªå‹•åˆ†å‰²æˆå¤šå€‹æ®µè½

        Args:
            level: æ—¥èªŒç´šåˆ¥
            message: ç°¡çŸ­æè¿°
            content: é•·å…§å®¹
            trace_id: è¿½è¹¤ID
            category: æ—¥èªŒé¡åˆ¥

        Returns:
            åˆ†æ®µIDåˆ—è¡¨
        """
        # å¦‚æœå…§å®¹ä¸é•·ï¼Œç›´æ¥è¨˜éŒ„
        if len(content) <= self.MAX_LOG_SIZE:
            self._log_single(level, f"{message}: {content[:200]}...", trace_id, category)
            return []

        # è¨ˆç®—éœ€è¦åˆ†å‰²çš„æ®µæ•¸
        segments = self._split_content(content, self.MAX_LOG_SIZE)
        segment_ids = []

        # ç”Ÿæˆå…§å®¹å“ˆå¸Œä½œç‚ºä¸»ID
        content_hash = hashlib.sha256(content.encode()).hexdigest()[:8]

        # è¨˜éŒ„ä¸»æ¢ç›®
        self._log_single(
            level,
            f"{message} [Long content: {len(content)} chars, {len(segments)} segments, ID: {content_hash}]",
            trace_id,
            category
        )

        # è¨˜éŒ„æ¯å€‹åˆ†æ®µ
        for i, segment in enumerate(segments):
            segment_id = f"{content_hash}_{i+1}of{len(segments)}"
            segment_ids.append(segment_id)

            # ä¿å­˜åˆ†æ®µåˆ°æ–‡ä»¶
            segment_file = self.segments_dir / f"{trace_id}_{segment_id}.json"
            segment_data = ContentSegment(
                segment_id=segment_id,
                segment_index=i + 1,
                total_segments=len(segments),
                content=segment,
                checksum=hashlib.md5(segment.encode()).hexdigest()
            )

            with open(segment_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(segment_data), f, ensure_ascii=False, indent=2)

            # è¨˜éŒ„åˆ†æ®µä¿¡æ¯
            self._log_single(
                "DEBUG",
                f"Content segment {i+1}/{len(segments)} saved: {segment_id} [{len(segment)} chars]",
                trace_id,
                category
            )

        return segment_ids

    def _split_content(self, content: str, max_size: int) -> List[str]:
        """æ™ºèƒ½åˆ†å‰²å…§å®¹"""
        if len(content) <= max_size:
            return [content]

        segments = []

        # å˜—è©¦æŒ‰æ®µè½åˆ†å‰²
        paragraphs = content.split('\n\n')
        current_segment = ""

        for para in paragraphs:
            if len(current_segment) + len(para) + 2 <= max_size:
                if current_segment:
                    current_segment += "\n\n"
                current_segment += para
            else:
                if current_segment:
                    segments.append(current_segment)

                # å¦‚æœå–®å€‹æ®µè½å¤ªé•·ï¼Œå¼·åˆ¶åˆ†å‰²
                if len(para) > max_size:
                    para_segments = textwrap.wrap(para, width=max_size)
                    segments.extend(para_segments[:-1])
                    current_segment = para_segments[-1]
                else:
                    current_segment = para

        if current_segment:
            segments.append(current_segment)

        return segments

    def _log_single(self, level: str, message: str, trace_id: str, category: str):
        """è¨˜éŒ„å–®æ¢æ—¥èªŒ"""
        log_method = getattr(self.logger, level.lower(), self.logger.info)
        log_method(f"ğŸš€ {message} [{trace_id}]")

    def save_response_as_markdown(self,
                                 response: str,
                                 metadata: Dict[str, Any],
                                 trace_id: str) -> Path:
        """
        å°‡å›æ‡‰ä¿å­˜ç‚º Markdown æ–‡ä»¶

        Args:
            response: å›æ‡‰å…§å®¹
            metadata: å…ƒæ•¸æ“šï¼ˆåŒ…å«æŸ¥è©¢ã€æ¨¡å¼ã€æ™‚é–“ç­‰ï¼‰
            trace_id: è¿½è¹¤ID

        Returns:
            Markdown æ–‡ä»¶è·¯å¾‘
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"response_{trace_id}_{timestamp}.md"
        filepath = self.reports_dir / filename

        # æ§‹å»º Markdown å…§å®¹
        markdown_content = self._build_markdown_report(response, metadata, trace_id)

        # ä¿å­˜æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(markdown_content)

        # å¦‚æœå…§å®¹å¤ªé•·ï¼ŒåŒæ™‚å‰µå»ºåˆ†æ®µç‰ˆæœ¬
        if len(response) > self.MAX_LOG_SIZE:
            self._save_segmented_markdown(response, metadata, trace_id, timestamp)

        self.logger.info(f"ğŸ“ Response saved to markdown: {filepath} [{trace_id}]")

        return filepath

    def _build_markdown_report(self,
                              response: str,
                              metadata: Dict[str, Any],
                              trace_id: str) -> str:
        """æ§‹å»º Markdown å ±å‘Š"""

        # æå–å…ƒæ•¸æ“š
        query = metadata.get('query', 'N/A')
        mode = metadata.get('mode', 'N/A')
        model = metadata.get('model', 'N/A')
        tokens = metadata.get('tokens', {})
        duration = metadata.get('duration_ms', 0)
        timestamp = metadata.get('timestamp', datetime.now().isoformat())

        # æ§‹å»º Markdown
        markdown = f"""# OpenCode AI Response Report

## Metadata

| Field | Value |
|-------|-------|
| **Trace ID** | `{trace_id}` |
| **Timestamp** | {timestamp} |
| **Mode** | {mode} |
| **Model** | {model} |
| **Duration** | {duration}ms |
| **Input Tokens** | {tokens.get('prompt_tokens', 'N/A')} |
| **Output Tokens** | {tokens.get('completion_tokens', 'N/A')} |
| **Total Tokens** | {tokens.get('total_tokens', 'N/A')} |

## Query

```
{query}
```

## Response

{response}

---

### Processing Details

"""

        # æ·»åŠ è™•ç†ç´°ç¯€
        if 'stages' in metadata:
            markdown += "#### Stage Execution\n\n"
            for stage in metadata.get('stages', []):
                markdown += f"- **{stage['name']}**: {stage['duration']}ms - {stage['status']}\n"

        # æ·»åŠ å¼•ç”¨çµ±è¨ˆ
        if 'citations' in metadata:
            citations = metadata['citations']
            markdown += f"\n#### Citation Statistics\n\n"
            markdown += f"- Cited References: {citations.get('cited_count', 0)}\n"
            markdown += f"- Uncited References: {citations.get('uncited_count', 0)}\n"
            markdown += f"- Total References: {citations.get('total_count', 0)}\n"
            markdown += f"- Citation Rate: {citations.get('citation_rate', 0):.1f}%\n"

        # æ·»åŠ éŒ¯èª¤ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if 'errors' in metadata and metadata['errors']:
            markdown += f"\n#### Errors\n\n"
            for error in metadata['errors']:
                markdown += f"- {error}\n"

        markdown += f"""

---

*Generated by OpenCode Platform v2.0*
*Report ID: {trace_id}*
*Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""

        return markdown

    def _save_segmented_markdown(self,
                                response: str,
                                metadata: Dict[str, Any],
                                trace_id: str,
                                timestamp: str):
        """ä¿å­˜åˆ†æ®µçš„ Markdown æ–‡ä»¶"""
        segments = self._split_content(response, self.MAX_LOG_SIZE)

        for i, segment in enumerate(segments):
            filename = f"response_{trace_id}_{timestamp}_part{i+1}of{len(segments)}.md"
            filepath = self.reports_dir / filename

            # æ§‹å»ºåˆ†æ®µ Markdown
            part_metadata = metadata.copy()
            part_metadata['part'] = f"{i+1}/{len(segments)}"

            markdown_content = f"""# OpenCode AI Response Report - Part {i+1}/{len(segments)}

## Metadata

| Field | Value |
|-------|-------|
| **Trace ID** | `{trace_id}` |
| **Part** | {i+1} of {len(segments)} |
| **Segment Size** | {len(segment)} characters |

## Response (Part {i+1})

{segment}

---

*This is part {i+1} of {len(segments)} segments*
*Full response saved in segments due to length*
"""

            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(markdown_content)

    def load_segments(self, trace_id: str, content_hash: str) -> Optional[str]:
        """è¼‰å…¥åˆ†æ®µå…§å®¹ä¸¦é‡çµ„"""
        segments = []
        segment_files = sorted(self.segments_dir.glob(f"{trace_id}_{content_hash}_*.json"))

        for segment_file in segment_files:
            with open(segment_file, 'r', encoding='utf-8') as f:
                segment_data = json.load(f)
                segments.append((segment_data['segment_index'], segment_data['content']))

        if not segments:
            return None

        # æŒ‰ç´¢å¼•æ’åºä¸¦åˆä½µ
        segments.sort(key=lambda x: x[0])
        full_content = ''.join(seg[1] for seg in segments)

        return full_content

    def get_metrics(self) -> Dict[str, Any]:
        """ç²å–æ—¥èªŒæŒ‡æ¨™"""
        log_files = list(self.base_path.glob("*.log"))
        segment_files = list(self.segments_dir.glob("*.json"))
        report_files = list(self.reports_dir.glob("*.md"))

        total_size = sum(f.stat().st_size for f in log_files)
        total_size += sum(f.stat().st_size for f in segment_files)
        total_size += sum(f.stat().st_size for f in report_files)

        return {
            "log_files": len(log_files),
            "segment_files": len(segment_files),
            "report_files": len(report_files),
            "total_size_mb": total_size / (1024 * 1024),
            "reports_dir": str(self.reports_dir)
        }


# å–®ä¾‹å¯¦ä¾‹
_enhanced_logger = None

def get_enhanced_logger() -> EnhancedLogger:
    """ç²å–å¢å¼·æ—¥èªŒå™¨å–®ä¾‹"""
    global _enhanced_logger
    if _enhanced_logger is None:
        _enhanced_logger = EnhancedLogger()
    return _enhanced_logger