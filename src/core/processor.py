"""
è™•ç†å™¨ç³»çµ± - ç­–ç•¥æ¨¡å¼å¯¦ç¾
æ¯å€‹è™•ç†å™¨è² è²¬ä¸€ç¨®è™•ç†æ¨¡å¼
"""

from abc import ABC, abstractmethod
from typing import Dict, Type, Optional, Any, List
import asyncio
from datetime import datetime

from .models import ProcessingContext, ProcessingMode, EventType
from .logger import structured_logger, LogCategory
from .prompts import PromptTemplates
import json
import time


class BaseProcessor(ABC):
    """è™•ç†å™¨åŸºé¡"""

    def __init__(self, llm_client=None):
        self.llm_client = llm_client
        self.logger = structured_logger

    @abstractmethod
    async def process(self, context: ProcessingContext) -> str:
        """è™•ç†è«‹æ±‚ - å­é¡å¿…é ˆå¯¦ç¾"""
        pass

    async def _call_llm(self, prompt: str, streaming: bool = False) -> str:
        """èª¿ç”¨ LLM - å…¬å…±æ–¹æ³•"""
        if not self.llm_client:
            return f"[Mock Response] {prompt[:50]}..."

        start_time = time.time()
        with self.logger.measure("llm_call"):
            # ä½¿ç”¨ return_token_info åƒæ•¸ç²å– token è³‡è¨Š
            result = await self.llm_client.generate(prompt, return_token_info=True)

            # è™•ç†è¿”å›å€¼
            if isinstance(result, tuple):
                response, token_info = result
                tokens_in = token_info.get("prompt_tokens", 0)
                tokens_out = token_info.get("completion_tokens", 0)
                total_tokens = token_info.get("total_tokens", 0)
            else:
                # å‘å¾Œå…¼å®¹ï¼šå¦‚æœè¿”å›çš„æ˜¯å­—ç¬¦ä¸²
                response = result
                tokens_in = len(prompt.split())  # ç²—ç•¥ä¼°ç®—
                tokens_out = len(response.split())  # ç²—ç•¥ä¼°ç®—
                total_tokens = tokens_in + tokens_out

            duration_ms = (time.time() - start_time) * 1000

            # è¨˜éŒ„ LLM èª¿ç”¨
            self.logger.log_llm_call(
                model="gpt-4o",
                tokens_in=tokens_in,
                tokens_out=tokens_out,
                duration_ms=duration_ms
            )

            # æ›´æ–°ä¸Šä¸‹æ–‡çš„ token çµ±è¨ˆ
            if hasattr(self, 'context') and self.context:
                self.context.total_tokens += total_tokens

            return response

    async def _log_tool_decision(self, tool_name: str, reason: str, confidence: float = 0.9):
        """è¨˜éŒ„å·¥å…·æ±ºç­–"""
        self.logger.log_tool_decision(tool_name, confidence, reason)
        self.logger.info(
            f"ğŸ”§ Tool Decision: {tool_name}",
            "processor",
            "tool_decision",
            tool=tool_name,
            confidence=confidence,
            reason=reason
        )


class ChatProcessor(BaseProcessor):
    """å°è©±è™•ç†å™¨"""

    async def process(self, context: ProcessingContext) -> str:
        self.logger.progress("chat", "start")
        context.set_current_step("chat")

        # ä½¿ç”¨ç³»çµ±æŒ‡ä»¤æç¤ºè©
        system_prompt = PromptTemplates.get_system_instruction()
        output_guidelines = PromptTemplates.get_output_guidelines()

        # çµ„åˆå®Œæ•´æç¤º
        full_prompt = f"{system_prompt}\n\n{output_guidelines}\n\nUser: {context.request.query}"
        response = await self._call_llm(full_prompt)

        # ç™¼é€æ¶ˆæ¯
        self.logger.message(response)

        context.mark_step_complete("chat")
        self.logger.progress("chat", "end")

        return response


class KnowledgeProcessor(BaseProcessor):
    """çŸ¥è­˜æª¢ç´¢è™•ç†å™¨"""

    async def process(self, context: ProcessingContext) -> str:
        self.logger.progress("knowledge-retrieval", "start")
        context.set_current_step("knowledge-retrieval")

        # è¨˜éŒ„ RAG æ±ºç­–
        await self._log_tool_decision(
            "rag_retrieval",
            "ä½¿ç”¨çŸ¥è­˜åº«æª¢ç´¢ç›¸é—œæ–‡æª”",
            0.9
        )

        # Step 1: æª¢ç´¢ç›¸é—œçŸ¥è­˜
        self.logger.progress("embedding", "start")
        await asyncio.sleep(0.1)  # æ¨¡æ“¬ embedding
        self.logger.progress("embedding", "end")

        # Step 2: æœç´¢
        self.logger.progress("search", "start")

        # è¨˜éŒ„ RAG æ“ä½œ
        self.logger.info(
            f"ğŸ“š RAG Search: {context.request.query[:50]}...",
            "rag",
            "search",
            query=context.request.query,
            vector_db="chromadb",
            embedding_model="text-embedding-ada-002"
        )

        # é€™è£¡æ‡‰è©²èª¿ç”¨å¯¦éš›çš„ RAG ç³»çµ±
        relevant_docs = ["Doc1: ç›¸é—œå…§å®¹...", "Doc2: æ›´å¤šå…§å®¹..."]

        # è¨˜éŒ„æª¢ç´¢çµæœ
        self.logger.info(
            f"ğŸ“– RAG Results: Found {len(relevant_docs)} relevant documents",
            "rag",
            "results",
            docs_count=len(relevant_docs),
            top_score=0.92
        )

        self.logger.progress("search", "end", {"docs_found": len(relevant_docs)})

        # Step 3: ç”Ÿæˆç­”æ¡ˆ
        # ä½¿ç”¨çŸ¥è­˜æª¢ç´¢æç¤ºè©æ¨¡æ¿
        prompt = PromptTemplates.get_search_knowledge_result_prompt(
            query=context.request.query,
            research_goal="æä¾›æº–ç¢ºã€è©³ç´°çš„å›ç­”",
            context=' '.join(relevant_docs)
        )

        # åŠ ä¸Šå¼•ç”¨è¦å‰‡
        citation_rules = PromptTemplates.get_citation_rules()
        full_prompt = f"{prompt}\n\n{citation_rules}"

        response = await self._call_llm(full_prompt)

        self.logger.message(response)
        context.mark_step_complete("knowledge-retrieval")
        self.logger.progress("knowledge-retrieval", "end")

        return response


class SearchProcessor(BaseProcessor):
    """ç¶²è·¯æœç´¢è™•ç†å™¨"""

    async def process(self, context: ProcessingContext) -> str:
        self.logger.progress("web-search", "start")
        context.set_current_step("web-search")

        # è¨˜éŒ„å·¥å…·æ±ºç­–
        await self._log_tool_decision(
            "web_search",
            "ç”¨æˆ¶æŸ¥è©¢éœ€è¦ç¶²è·¯æœç´¢ä¾†ç²å–æœ€æ–°è³‡è¨Š",
            0.95
        )

        # Step 1: ç”Ÿæˆ SERP æŸ¥è©¢
        self.logger.progress("query-generation", "start")
        search_queries = await self._generate_serp_queries(context.request.query)
        self.logger.progress("query-generation", "end", {"queries": len(search_queries)})

        # Step 2: åŸ·è¡Œå¤šå€‹æœç´¢
        self.logger.progress("searching", "start")
        all_results = []
        for query_obj in search_queries:
            results = await self._perform_search(query_obj.get('query', ''))
            all_results.append({
                'query': query_obj.get('query'),
                'goal': query_obj.get('researchGoal'),
                'results': results
            })
        self.logger.progress("searching", "end", {"total_results": len(all_results)})

        # Step 3: ä½¿ç”¨å°ˆæ¥­ prompt è™•ç†çµæœ
        combined_context = "\n\n".join([
            f"Query: {r['query']}\nGoal: {r['goal']}\nResults: {r['results']}"
            for r in all_results
        ])

        prompt = PromptTemplates.get_search_result_prompt(
            query=context.request.query,
            research_goal="æä¾›å…¨é¢ã€æº–ç¢ºçš„ç­”æ¡ˆ",
            context=combined_context
        )

        # åŠ ä¸Šå¼•ç”¨è¦å‰‡
        citation_rules = PromptTemplates.get_citation_rules()
        full_prompt = f"{prompt}\n\n{citation_rules}"

        response = await self._call_llm(full_prompt)

        self.logger.message(response)
        context.mark_step_complete("web-search")
        self.logger.progress("web-search", "end")

        return response

    async def _generate_serp_queries(self, user_query: str) -> List[Dict[str, str]]:
        """ç”Ÿæˆå„ªåŒ–çš„ SERP æŸ¥è©¢ - ä½¿ç”¨å°ˆæ¥­ prompt"""
        import json

        # å…ˆç”Ÿæˆç°¡å–®çš„ç ”ç©¶è¨ˆåŠƒ
        plan = f"ç ”ç©¶ä¸»é¡Œ: {user_query}"

        # å®šç¾© schema
        output_schema = {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "query": {"type": "string"},
                    "researchGoal": {"type": "string"}
                }
            }
        }

        # ä½¿ç”¨å°ˆæ¥­çš„ SERP æŸ¥è©¢æç¤ºè©
        prompt = PromptTemplates.get_serp_queries_prompt(plan, output_schema)

        response = await self._call_llm(prompt)

        # è§£æ JSON å›æ‡‰
        try:
            # å¾å›æ‡‰ä¸­æå– JSON
            import re
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response)
            if json_match:
                queries = json.loads(json_match.group(1))
            else:
                # å¦‚æœæ²’æœ‰ markdown åŒ…è£ï¼Œç›´æ¥è§£æ
                queries = json.loads(response)
            return queries[:3]  # é™åˆ¶æœ€å¤š 3 å€‹æŸ¥è©¢
        except:
            # å¦‚æœè§£æå¤±æ•—ï¼Œè¿”å›é è¨­æŸ¥è©¢
            return [{"query": user_query, "researchGoal": "ç²å–ç›¸é—œè³‡è¨Š"}]

    async def _perform_search(self, query: str) -> str:
        """åŸ·è¡Œç¶²è·¯æœç´¢ - ä½¿ç”¨ get_query_result_prompt è™•ç†çµæœ"""
        # è¨˜éŒ„æœç´¢æŸ¥è©¢
        self.logger.info(
            f"ğŸ” Web Query: {query}",
            "search",
            "query",
            query=query,
            provider="tavily"  # æˆ–å…¶ä»–æœç´¢æä¾›è€…
        )

        # é€™è£¡æ‡‰è©²èª¿ç”¨å¯¦éš›çš„æœç´¢ API
        await asyncio.sleep(0.2)  # æ¨¡æ“¬æœç´¢å»¶é²

        # å¦‚æœæœ‰ LLMï¼Œä½¿ç”¨ get_query_result_prompt ä¾†å„ªåŒ–æœç´¢çµæœ
        raw_results = f"æœç´¢çµæœï¼šé—œæ–¼ {query} çš„ç›¸é—œè³‡è¨Š..."

        if self.llm_client:
            # ä½¿ç”¨å°ˆæ¥­çš„æŸ¥è©¢çµæœ prompt
            result_prompt = PromptTemplates.get_query_result_prompt(
                query=query,
                research_goal="æä¾›æº–ç¢ºã€æœ€æ–°çš„è³‡è¨Š"
            )
            full_prompt = f"{result_prompt}\n\næœç´¢çµæœï¼š{raw_results}"
            processed_results = await self._call_llm(full_prompt)
            return processed_results

        return raw_results


class ThinkingProcessor(BaseProcessor):
    """æ·±åº¦æ€è€ƒè™•ç†å™¨"""

    async def process(self, context: ProcessingContext) -> str:
        self.logger.progress("deep-thinking", "start")
        context.set_current_step("deep-thinking")

        # è¨˜éŒ„æ€è€ƒæ±ºç­–
        await self._log_tool_decision(
            "deep_thinking",
            "è¤‡é›œå•é¡Œéœ€è¦æ·±åº¦åˆ†æå’Œæ¨ç†",
            0.95
        )

        # è¨˜éŒ„æ€è€ƒè¨ˆåŠƒ
        self.logger.info(
            f"ğŸ§  Deep Thinking: Analyzing '{context.request.query[:50]}...'",
            "thinking",
            "start",
            category=LogCategory.TOOL,
            approach="multi-perspective-reasoning"
        )

        # Step 1: Problem decomposition and understanding
        self.logger.progress("problem-analysis", "start")
        self.logger.reasoning("Decomposing and understanding core elements...", streaming=True)

        # ä½¿ç”¨æ€è€ƒæ¨¡å¼çš„å°ˆæ¥­æç¤ºè©
        thinking_prompt = PromptTemplates.get_thinking_mode_prompt(context.request.query)

        # åŸ·è¡Œæ·±åº¦æ€è€ƒ
        thinking_response = await self._call_llm(thinking_prompt)

        self.logger.progress("problem-analysis", "end", {"analyzed": True})

        # Step 2: Multi-perspective analysis
        self.logger.progress("multi-perspective", "start")
        self.logger.reasoning("Analyzing from multiple perspectives...", streaming=True)

        # ä½¿ç”¨æ‰¹åˆ¤æ€§æ€ç¶­æç¤ºè©
        critical_prompt = PromptTemplates.get_critical_thinking_prompt(
            question=context.request.query,
            context=thinking_response
        )

        critical_analysis = await self._call_llm(critical_prompt)

        self.logger.progress("multi-perspective", "end", {"perspectives": 5})

        # Step 3: Deep reasoning
        self.logger.progress("deep-reasoning", "start")
        self.logger.reasoning("Conducting deep reasoning and logical analysis...", streaming=True)

        # ä½¿ç”¨æ¨ç†éˆæç¤ºè©
        reasoning_prompt = PromptTemplates.get_chain_of_thought_prompt(context.request.query)

        chain_reasoning = await self._call_llm(reasoning_prompt)

        self.logger.progress("deep-reasoning", "end")

        # Step 4: Synthesis and reflection
        self.logger.progress("synthesis-reflection", "start")
        self.logger.reasoning("Synthesizing all analysis and reflecting...", streaming=True)

        # ä½¿ç”¨åæ€æç¤ºè©
        reflection_prompt = PromptTemplates.get_reflection_prompt(
            original_response=f"{thinking_response}\n\n{critical_analysis}\n\n{chain_reasoning}",
            question=context.request.query
        )

        reflection = await self._call_llm(reflection_prompt)

        self.logger.progress("synthesis-reflection", "end")

        # Step 5: Final answer generation
        self.logger.progress("final-synthesis", "start")

        # Combine all thinking processes
        complete_thinking = f"""
Deep Thinking Process:

ã€Problem Understanding & Decompositionã€‘
{thinking_response}

ã€Critical Analysisã€‘
{critical_analysis}

ã€Chain of Reasoningã€‘
{chain_reasoning}

ã€Reflection & Improvementã€‘
{reflection}

ã€Final Comprehensive Answerã€‘
Based on the above deep thinking process, here is the complete answer to "{context.request.query}":
"""

        # ä½¿ç”¨è¼¸å‡ºæŒ‡å—ç¢ºä¿ç­”æ¡ˆå“è³ª
        output_guidelines = PromptTemplates.get_output_guidelines()
        final_prompt = f"{complete_thinking}\n\n{output_guidelines}"

        final_response = await self._call_llm(final_prompt)

        self.logger.progress("final-synthesis", "end")

        # Send complete thinking process and final answer
        full_response = f"{complete_thinking}\n{final_response}"
        self.logger.message(full_response)

        context.mark_step_complete("deep-thinking")
        self.logger.progress("deep-thinking", "end")

        return full_response


class KnowledgeGraphProcessor(BaseProcessor):
    """çŸ¥è­˜åœ–è­œè™•ç†å™¨ - ç”Ÿæˆ Mermaid åœ–è¡¨"""

    async def process(self, context: ProcessingContext) -> str:
        self.logger.progress("knowledge-graph", "start")
        context.set_current_step("knowledge-graph")

        # Step 1: ç²å–æˆ–ç”Ÿæˆæ–‡ç« å…§å®¹
        self.logger.progress("content-preparation", "start")

        # å¦‚æœæ˜¯å•é¡Œï¼Œå…ˆç”Ÿæˆç›¸é—œå…§å®¹
        if "?" in context.request.query or len(context.request.query) < 100:
            # å…ˆç”Ÿæˆè©³ç´°å…§å®¹
            system_prompt = PromptTemplates.get_system_instruction()
            content_prompt = f"{system_prompt}\n\nè«‹é‡å°ä»¥ä¸‹ä¸»é¡Œç”Ÿæˆè©³ç´°çš„èªªæ˜æ–‡ç« ï¼š{context.request.query}"
            article = await self._call_llm(content_prompt)
        else:
            # ç›´æ¥ä½¿ç”¨æä¾›çš„å…§å®¹
            article = context.request.query

        self.logger.progress("content-preparation", "end")

        # Step 2: ç”ŸæˆçŸ¥è­˜åœ–è­œ
        self.logger.progress("graph-generation", "start")

        # ä½¿ç”¨å°ˆæ¥­çš„çŸ¥è­˜åœ–è­œ prompt
        graph_prompt = PromptTemplates.get_knowledge_graph_prompt()
        full_prompt = f"{graph_prompt}\n\næ–‡ç« å…§å®¹ï¼š\n{article}"

        mermaid_graph = await self._call_llm(full_prompt)

        self.logger.progress("graph-generation", "end")

        # Step 3: çµ„åˆæœ€çµ‚è¼¸å‡º
        response = f"""## çŸ¥è­˜åœ–è­œåˆ†æ

### åŸå§‹å…§å®¹æ‘˜è¦
{article[:500]}...

### çŸ¥è­˜åœ–è­œè¦–è¦ºåŒ–
{mermaid_graph}

### ä½¿ç”¨èªªæ˜
å°‡ä¸Šè¿° Mermaid ä»£ç¢¼è¤‡è£½åˆ°æ”¯æ´ Mermaid çš„ Markdown ç·¨è¼¯å™¨ä¸­å³å¯æŸ¥çœ‹åœ–è¡¨ã€‚
"""

        self.logger.message(response)
        context.mark_step_complete("knowledge-graph")
        self.logger.progress("knowledge-graph", "end")

        return response


class CodeProcessor(BaseProcessor):
    """ä»£ç¢¼åŸ·è¡Œè™•ç†å™¨"""

    async def process(self, context: ProcessingContext) -> str:
        self.logger.progress("code-execution", "start")
        context.set_current_step("code-execution")

        # Step 1: è§£æä»£ç¢¼è«‹æ±‚
        self.logger.progress("code-analysis", "start")
        code_request = context.request.query
        self.logger.progress("code-analysis", "end")

        # Step 2: ç”Ÿæˆä»£ç¢¼
        self.logger.progress("code-generation", "start")
        prompt = f"ç”Ÿæˆä»£ç¢¼ä¾†å®Œæˆï¼š{code_request}"
        generated_code = await self._call_llm(prompt)
        self.logger.message(f"```python\n{generated_code}\n```")
        self.logger.progress("code-generation", "end")

        # Step 3: åŸ·è¡Œä»£ç¢¼ï¼ˆæ²™ç®±ç’°å¢ƒï¼‰
        self.logger.progress("code-execution", "start")
        result = await self._execute_code(generated_code)
        self.logger.progress("code-execution", "end", {"success": result.get("success")})

        response = f"ä»£ç¢¼åŸ·è¡Œçµæœï¼š\n{result.get('output', 'No output')}"
        self.logger.message(response)

        context.mark_step_complete("code-execution")
        self.logger.progress("code-execution", "end")

        return response

    async def _execute_code(self, code: str) -> Dict[str, Any]:
        """åœ¨æ²™ç®±ä¸­åŸ·è¡Œä»£ç¢¼"""
        # é€™è£¡æ‡‰è©²èª¿ç”¨å¯¦éš›çš„æ²™ç®±æœå‹™
        await asyncio.sleep(0.1)
        return {
            "success": True,
            "output": "Hello World!"
        }


class RewritingProcessor(BaseProcessor):
    """æ–‡å­—é‡å¯«è™•ç†å™¨ - è½‰æ›ç‚º Markdown æ ¼å¼"""

    async def process(self, context: ProcessingContext) -> str:
        self.logger.progress("rewriting", "start")
        context.set_current_step("rewriting")

        # ç²å–è¦é‡å¯«çš„å…§å®¹
        content_to_rewrite = context.request.query

        # ä½¿ç”¨å°ˆæ¥­çš„é‡å¯« prompt
        rewriting_prompt = PromptTemplates.get_rewriting_prompt()
        full_prompt = f"{rewriting_prompt}\n\nText to rewrite:\n{content_to_rewrite}"

        # åŸ·è¡Œé‡å¯«
        self.logger.progress("markdown-conversion", "start")
        rewritten_content = await self._call_llm(full_prompt)
        self.logger.progress("markdown-conversion", "end")

        # è¼¸å‡ºçµæœ
        self.logger.message(rewritten_content)
        context.mark_step_complete("rewriting")
        self.logger.progress("rewriting", "end")

        return rewritten_content


class DeepResearchProcessor(BaseProcessor):
    """æ·±åº¦ç ”ç©¶è™•ç†å™¨ - å®Œæ•´ SSE äº‹ä»¶ç®¡é“å¯¦ç¾"""

    async def process(self, context: ProcessingContext) -> str:
        """åŸ·è¡Œå®Œæ•´çš„æ·±åº¦ç ”ç©¶æµç¨‹"""

        # è¨˜éŒ„æ·±åº¦ç ”ç©¶æ±ºç­–
        await self._log_tool_decision(
            "deep_research",
            "åŸ·è¡Œå…¨é¢çš„æ·±åº¦ç ”ç©¶ä»¥å›ç­”è¤‡é›œå•é¡Œ",
            0.95
        )

        # 1. å ±å‘Šè¨ˆåŠƒéšæ®µ
        report_plan = await self._write_report_plan(context)

        # 2. SERP æŸ¥è©¢ç”Ÿæˆ
        search_tasks = await self._generate_serp_queries(context, report_plan)

        # 3. åŸ·è¡Œæœç´¢ä»»å‹™
        search_results = await self._execute_search_tasks(context, search_tasks)

        # 4. ç”Ÿæˆæœ€çµ‚å ±å‘Š
        final_report = await self._write_final_report(context, search_results, report_plan)

        return final_report

    async def _write_report_plan(self, context: ProcessingContext) -> str:
        """Phase 1: ç”Ÿæˆç ”ç©¶å ±å‘Šè¨ˆç•«"""
        self.logger.progress("report-plan", "start")

        # è¨˜éŒ„è¨ˆåŠƒéšæ®µ
        self.logger.info(
            f"ğŸ“ Planning: Creating research plan for '{context.request.query[:50]}...'",
            "deep_research",
            "planning",
            phase="report-plan",
            query_length=len(context.request.query)
        )

        # ä½¿ç”¨å ±å‘Šè¨ˆåŠƒ prompt
        plan_prompt = PromptTemplates.get_report_plan_prompt(context.request.query)

        # ä¸²æµæ¨ç†éç¨‹
        self.logger.reasoning("é–‹å§‹åˆ†æç ”ç©¶éœ€æ±‚...", streaming=True)
        plan = await self._call_llm(plan_prompt, streaming=True)
        self.logger.reasoning(f"ç ”ç©¶è¨ˆåŠƒåˆ¶å®šå®Œæˆï¼š{plan[:100]}...", streaming=False)

        # ç™¼é€è¨ˆåŠƒæ¶ˆæ¯
        self.logger.message(f"ç ”ç©¶è¨ˆåŠƒï¼š\n{plan}", streaming=False)

        self.logger.progress("report-plan", "end", {"plan": plan[:200]})

        return plan

    async def _generate_serp_queries(self, context: ProcessingContext, plan: str) -> List[Dict]:
        """Phase 2: ç”Ÿæˆ SERP æŸ¥è©¢"""
        self.logger.progress("serp-query", "start")

        # è¨˜éŒ„æŸ¥è©¢ç”Ÿæˆ
        self.logger.info(
            f"ğŸ” SERP Generation: Extracting search queries from plan",
            "deep_research",
            "serp",
            phase="serp-query"
        )

        # å®šç¾©è¼¸å‡º schema
        output_schema = {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "query": {"type": "string"},
                    "researchGoal": {"type": "string"},
                    "priority": {"type": "number"}
                }
            }
        }

        # ä½¿ç”¨ SERP æŸ¥è©¢ prompt
        serp_prompt = PromptTemplates.get_serp_queries_prompt(plan, output_schema)
        response = await self._call_llm(serp_prompt)

        # è§£ææŸ¥è©¢
        try:
            import re
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response)
            if json_match:
                queries = json.loads(json_match.group(1))
            else:
                queries = json.loads(response)
        except:
            queries = [{"query": context.request.query, "researchGoal": "General research", "priority": 1}]

        # è¨˜éŒ„ç”Ÿæˆçš„æŸ¥è©¢
        self.logger.info(
            f"ğŸ“‹ SERP Queries: Generated {len(queries)} search queries",
            "deep_research",
            "serp",
            queries_count=len(queries),
            queries=queries[:3]  # åªè¨˜éŒ„å‰3å€‹
        )

        self.logger.progress("serp-query", "end", {"queries": queries})

        return queries

    async def _execute_search_tasks(self, context: ProcessingContext, search_tasks: List[Dict]) -> List[Dict]:
        """Phase 3: åŸ·è¡Œæœç´¢ä»»å‹™"""
        self.logger.progress("task-list", "start")

        # è¨˜éŒ„ä»»å‹™åˆ—è¡¨
        self.logger.info(
            f"ğŸ“‹ Task List: Executing {len(search_tasks)} search tasks",
            "deep_research",
            "tasks",
            phase="task-list",
            total_tasks=len(search_tasks)
        )

        results = []

        for i, task in enumerate(search_tasks, 1):
            query = task.get('query', '')
            goal = task.get('researchGoal', '')
            priority = task.get('priority', 1)

            # é–‹å§‹å–®å€‹æœç´¢ä»»å‹™
            self.logger.progress("search-task", "start", {"name": query})

            # è¨˜éŒ„æœç´¢ä»»å‹™
            self.logger.info(
                f"ğŸ” Search Task {i}/{len(search_tasks)}: {query}",
                "deep_research",
                "search_task",
                task_index=i,
                query=query,
                goal=goal,
                priority=priority,
                provider="tavily"
            )

            # æ¨ç†éç¨‹
            self.logger.reasoning(f"æ­£åœ¨æœç´¢ï¼š{query}...", streaming=True)

            # åŸ·è¡Œæœç´¢
            search_result = await self._perform_deep_search(query, goal)

            # è¨˜éŒ„æœç´¢çµæœ
            self.logger.info(
                f"âœ… Search Result {i}: Found {len(search_result.get('sources', []))} sources",
                "deep_research",
                "search_result",
                task_index=i,
                sources_count=len(search_result.get('sources', [])),
                relevance_score=search_result.get('relevance', 0)
            )

            # æ¶ˆæ¯è¼¸å‡º
            self.logger.message(f"æœç´¢ {i}: {query}\nçµæœ: {search_result.get('summary', '')[:200]}...")

            results.append({
                'query': query,
                'goal': goal,
                'priority': priority,
                'result': search_result
            })

            # çµæŸå–®å€‹æœç´¢ä»»å‹™
            self.logger.progress("search-task", "end", {
                "name": query,
                "data": search_result
            })

        self.logger.progress("task-list", "end")

        return results

    async def _perform_deep_search(self, query: str, goal: str) -> Dict:
        """åŸ·è¡Œæ·±åº¦æœç´¢"""

        # è¨˜éŒ„ Web Query
        self.logger.info(
            f"ğŸŒ Web Query: {query}",
            "web",
            "query",
            query=query,
            goal=goal,
            search_engine="google",
            max_results=10
        )

        # æ¨¡æ“¬æœç´¢å»¶é²
        await asyncio.sleep(0.3)

        # æ¨¡æ“¬æœç´¢çµæœ
        search_result = {
            'summary': f"é—œæ–¼ '{query}' çš„ç¶œåˆç ”ç©¶çµæœ...",
            'sources': [
                {'url': 'https://example.com/1', 'title': 'Source 1', 'relevance': 0.95},
                {'url': 'https://example.com/2', 'title': 'Source 2', 'relevance': 0.88}
            ],
            'relevance': 0.92,
            'timestamp': datetime.now().isoformat()
        }

        # è¨˜éŒ„æœç´¢çµæœè©³æƒ…
        self.logger.info(
            f"ğŸ”— Web Results: Retrieved {len(search_result['sources'])} sources",
            "web",
            "results",
            sources=search_result['sources'],
            avg_relevance=0.915
        )

        # å¦‚æœæœ‰ LLMï¼Œè™•ç†æœç´¢çµæœ
        if self.llm_client:
            result_prompt = PromptTemplates.get_search_result_prompt(
                query=query,
                research_goal=goal,
                context=json.dumps(search_result, ensure_ascii=False)
            )
            processed = await self._call_llm(result_prompt)
            search_result['processed'] = processed

        return search_result

    async def _write_final_report(self, context: ProcessingContext,
                                  search_results: List[Dict],
                                  report_plan: str) -> str:
        """Phase 4: ç”Ÿæˆæœ€çµ‚å ±å‘Š"""
        self.logger.progress("final-report", "start")

        # è¨˜éŒ„æœ€çµ‚å ±å‘Šç”Ÿæˆ
        self.logger.info(
            f"ğŸ“‘ Final Report: Synthesizing {len(search_results)} search results",
            "deep_research",
            "final_report",
            phase="final-report",
            results_count=len(search_results),
            plan_length=len(report_plan)
        )

        # æº–å‚™ä¸Šä¸‹æ–‡
        combined_context = self._prepare_report_context(search_results)

        # è¨˜éŒ„è¨˜æ†¶é«”æ“ä½œ
        self.logger.info(
            f"ğŸ’¾ Memory: Storing research context",
            "memory",
            "store",
            context_size=len(combined_context),
            chunks=len(search_results),
            type="research_report"
        )

        # ä½¿ç”¨æœ€çµ‚å ±å‘Š prompt
        # æº–å‚™ä¾†æºå’Œåœ–ç‰‡ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        sources = "\n".join([f"- {r['result'].get('summary', '')[:100]}..." for r in search_results[:5]])
        images = ""  # æš«æ™‚æ²’æœ‰åœ–ç‰‡

        report_prompt = PromptTemplates.get_final_report_prompt(
            plan=report_plan,
            learnings=combined_context,
            sources=sources,
            images=images,
            requirement=context.request.query
        )

        # åŠ ä¸Šå¼•ç”¨è¦å‰‡å’Œè¼¸å‡ºæŒ‡å—
        citation_rules = PromptTemplates.get_citation_rules()
        output_guidelines = PromptTemplates.get_output_guidelines()
        full_prompt = f"{report_prompt}\n\n{citation_rules}\n\n{output_guidelines}"

        # æ¨ç†æœ€çµ‚å ±å‘Š
        self.logger.reasoning("ç¶œåˆæ‰€æœ‰ç ”ç©¶çµæœï¼Œç”Ÿæˆæœ€çµ‚å ±å‘Š...", streaming=True)

        # ç”Ÿæˆå ±å‘Š
        final_report = await self._call_llm(full_prompt, streaming=True)

        # è¨˜éŒ„è¨˜æ†¶é«”å›æ”¶
        self.logger.info(
            f"ğŸ’¾ Memory: Retrieved research synthesis",
            "memory",
            "retrieve",
            report_length=len(final_report),
            citations_included=True
        )

        # ç™¼é€æœ€çµ‚å ±å‘Š
        self.logger.message(final_report, streaming=False)

        # å ±å‘Šå…ƒæ•¸æ“š
        report_metadata = {
            "title": f"Research Report: {context.request.query[:50]}",
            "sections": self._extract_report_sections(final_report),
            "sources_used": sum(len(r['result'].get('sources', [])) for r in search_results),
            "word_count": len(final_report.split()),
            "timestamp": datetime.now().isoformat()
        }

        # è¨˜éŒ„å ±å‘Šå®Œæˆ
        self.logger.info(
            f"âœ… Report Completed: {report_metadata['word_count']} words, {report_metadata['sources_used']} sources",
            "deep_research",
            "complete",
            metadata=report_metadata
        )

        self.logger.progress("final-report", "end", {"data": report_metadata})

        return final_report

    def _prepare_report_context(self, search_results: List[Dict]) -> str:
        """æº–å‚™å ±å‘Šä¸Šä¸‹æ–‡"""
        context_parts = []
        for i, result in enumerate(search_results, 1):
            context_parts.append(f"""
            æœç´¢ {i}: {result['query']}
            ç›®æ¨™: {result['goal']}
            å„ªå…ˆç´š: {result.get('priority', 1)}
            çµæœæ‘˜è¦: {result['result'].get('summary', '')}
            è™•ç†çµæœ: {result['result'].get('processed', '')}
            ä¾†æºæ•¸é‡: {len(result['result'].get('sources', []))}
            """)
        return "\n\n".join(context_parts)

    def _extract_report_sections(self, report: str) -> List[str]:
        """æå–å ±å‘Šç« ç¯€"""
        import re
        # åŒ¹é… Markdown æ¨™é¡Œ
        headers = re.findall(r'^#{1,3}\s+(.+)$', report, re.MULTILINE)
        return headers[:10]  # è¿”å›å‰10å€‹ç« ç¯€æ¨™é¡Œ


class ProcessorFactory:
    """è™•ç†å™¨å·¥å»  - å‰µå»ºå’Œç®¡ç†è™•ç†å™¨"""

    _processors: Dict[ProcessingMode, Type[BaseProcessor]] = {
        ProcessingMode.CHAT: ChatProcessor,
        ProcessingMode.KNOWLEDGE: KnowledgeProcessor,
        ProcessingMode.SEARCH: SearchProcessor,
        ProcessingMode.THINKING: ThinkingProcessor,
        ProcessingMode.CODE: CodeProcessor,
        ProcessingMode.DEEP_RESEARCH: DeepResearchProcessor,
    }

    def __init__(self, llm_client=None):
        self.llm_client = llm_client
        self._instances: Dict[ProcessingMode, BaseProcessor] = {}

    def get_processor(self, mode: ProcessingMode) -> BaseProcessor:
        """ç²å–è™•ç†å™¨å¯¦ä¾‹"""
        if mode not in self._instances:
            processor_class = self._processors.get(mode, ChatProcessor)
            self._instances[mode] = processor_class(self.llm_client)

        return self._instances[mode]

    def register_processor(self, mode: ProcessingMode, processor_class: Type[BaseProcessor]):
        """è¨»å†Šè‡ªå®šç¾©è™•ç†å™¨"""
        self._processors[mode] = processor_class
        # æ¸…é™¤å·²æœ‰å¯¦ä¾‹ï¼Œä¸‹æ¬¡ç²å–æ™‚æœƒå‰µå»ºæ–°çš„
        if mode in self._instances:
            del self._instances[mode]