"""
è™•ç†å™¨ç³»çµ± - ç­–ç•¥æ¨¡å¼å¯¦ç¾
æ¯å€‹è™•ç†å™¨è² è²¬ä¸€ç¨®è™•ç†æ¨¡å¼
"""

from abc import ABC, abstractmethod
from typing import Dict, Type, Optional, Any, List, Callable, AsyncGenerator
from dataclasses import dataclass
from enum import Enum
import asyncio
from datetime import datetime
import json
import time

from .models import ProcessingContext, ProcessingMode, EventType
from .logger import structured_logger, LogCategory
from .prompts import PromptTemplates
from .error_handler import robust_processor, enhanced_error_handler


class BaseProcessor(ABC):
    """è™•ç†å™¨åŸºé¡"""

    def __init__(self, llm_client=None, services: Optional[Dict[str, Any]] = None):
        self.llm_client = llm_client
        self.services = services or {}
        self.logger = structured_logger
        self._cognitive_level: Optional[str] = None

    @abstractmethod
    async def process(self, context: ProcessingContext) -> str:
        """è™•ç†è«‹æ±‚ - å­é¡å¿…é ˆå¯¦ç¾"""
        pass

    async def _call_llm(self, prompt: str, context: ProcessingContext = None) -> str:
        """èª¿ç”¨ LLM - å…¬å…±æ–¹æ³•"""
        if not self.llm_client:
            raise RuntimeError("LLM client not configured â€” cannot process request")

        # # è¨˜éŒ„ prompt (æˆªå–å‰500å­—ç¬¦ç”¨æ–¼æ—¥èªŒ)
        # self.logger.info(
        #     f"ğŸ“ LLM Prompt: {prompt[:500]}...",
        #     "llm",
        #     "prompt",
        #     prompt_length=len(prompt),
        #     prompt_preview=prompt[:200]
        # )

        start_time = time.time()
        with self.logger.measure("llm_call"):
            # ä½¿ç”¨ return_token_info åƒæ•¸ç²å– token è³‡è¨Š
            result = await self.llm_client.generate(prompt, return_token_info=True)

            # è™•ç†è¿”å›å€¼
            if isinstance(result, tuple):
                response, token_info = result
                tokens_in = token_info.get("prompt_tokens", 0)
                tokens_out = token_info.get("completion_tokens", 0)
                total_tokens = token_info.get("total_tokens", 0)
            else:
                # å‘å¾Œå…¼å®¹ï¼šå¦‚æœè¿”å›çš„æ˜¯å­—ç¬¦ä¸²
                response = result
                tokens_in = len(prompt.split())  # ç²—ç•¥ä¼°ç®—
                tokens_out = len(response.split())  # ç²—ç•¥ä¼°ç®—
                total_tokens = tokens_in + tokens_out

            duration_ms = (time.time() - start_time) * 1000

            # è¨˜éŒ„ LLM èª¿ç”¨ (åŒ…å« token å’Œæ™‚é–“è³‡è¨Š)
            self.logger.log_llm_call(
                model=getattr(self.llm_client, 'model_name', getattr(self.llm_client, 'provider_name', 'unknown')),
                tokens_in=tokens_in,
                tokens_out=tokens_out,
                duration_ms=duration_ms
            )

            # è¨˜éŒ„ LLM Response (ç”¨æ–¼ debuggingï¼Œé¡¯ç¤ºå¯¦éš›è¼¸å‡º)
            # æª¢æŸ¥æ˜¯å¦éœ€è¦åˆ†å‰²é•·å…§å®¹
            try:
                from core.enhanced_logger import get_enhanced_logger
                enhanced_logger = get_enhanced_logger()

                if len(response) > 10000:  # è¶…é 10KB
                    # ä½¿ç”¨å¢å¼·æ—¥èªŒå™¨è™•ç†é•·å…§å®¹
                    trace_id = context.trace_id if context and hasattr(context, 'trace_id') else "unknown"
                    enhanced_logger.log_long_content(
                        "INFO",
                        f"LLM Response (Long: {len(response)} chars, {total_tokens} tokens)",
                        response,
                        trace_id,
                        "llm_response"
                    )
                    # ä¸»æ—¥èªŒåªè¨˜éŒ„æ‘˜è¦
                    self.logger.info(
                        f"ğŸ’¬ LLM Response [Long content: {len(response)} chars, see segments]",
                        "llm",
                        "response",
                        response_length=len(response),
                        total_tokens=total_tokens
                    )
                else:
                    # æ­£å¸¸è¨˜éŒ„
                    self.logger.info(
                        f"ğŸ’¬ LLM Response: {response[:5000]}...",
                        "llm",
                        "response",
                        response_length=len(response),
                        response_preview=response[:200]
                    )
            except ImportError:
                # å¦‚æœå¢å¼·æ—¥èªŒå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸå§‹æ–¹å¼
                self.logger.info(
                    f"ğŸ’¬ LLM Response: {response[:5000]}...",
                    "llm",
                    "response",
                    response_length=len(response),
                    response_preview=response[:200]
                )

            # æª¢æŸ¥éŸ¿æ‡‰æ˜¯å¦ç‚ºç©º
            if not response or response.strip() == "":
                self.logger.warning(
                    "LLM returned empty response",
                    "llm",
                    "empty_response",
                    model=getattr(self.llm_client, 'model_name', 'unknown')
                )
                response = "[LLM returned empty response - please check API configuration]"

            # æ›´æ–°ä¸Šä¸‹æ–‡çš„ token çµ±è¨ˆ
            if context:
                context.total_tokens += total_tokens

            return response

    async def _log_tool_decision(self, tool_name: str, reason: str, confidence: float = 0.9):
        """è¨˜éŒ„å·¥å…·æ±ºç­–"""
        self.logger.log_tool_decision(tool_name, confidence, reason)
        self.logger.info(
            f"ğŸ”§ Tool Decision: {tool_name}",
            "processor",
            "tool_decision",
            tool=tool_name,
            confidence=confidence,
            reason=reason
        )


class ChatProcessor(BaseProcessor):
    """å°è©±è™•ç†å™¨ - System 1 with Cache Support"""

    async def process(self, context: ProcessingContext) -> str:
        self.logger.progress("chat", "start")
        context.set_current_step("chat")

        # Step 1: Cache Check (System 1 ç‰¹æ€§)
        cache_key = f"chat:{context.request.query}"
        cache = getattr(self, 'cache', None)  # å¾è™•ç†å™¨ç²å–å¿«å–å¯¦ä¾‹

        if cache:
            cached_response = cache.get(cache_key)
            if cached_response:
                self.logger.info("ğŸ’¾ Cache HIT for chat query", "chat", "cache_hit")
                self.logger.message(cached_response)
                context.mark_step_complete("chat")
                self.logger.progress("chat", "end")
                return cached_response

        # Step 2: Build Prompt (ç¬¦åˆç‹€æ…‹æ©Ÿ BuildPrompt)
        system_prompt = PromptTemplates.get_system_instruction()
        output_guidelines = PromptTemplates.get_output_guidelines()
        full_prompt = f"{system_prompt}\n\n{output_guidelines}\n\nUser: {context.request.query}"

        # Step 3: Call LLM (ç¬¦åˆç‹€æ…‹æ©Ÿ CallLLM)
        response = await self._call_llm(full_prompt, context)

        # Step 4: Cache Put (System 1 ç‰¹æ€§)
        if cache:
            cache.put(cache_key, response, ttl=300)
            self.logger.info("ğŸ’¾ Cache PUT for chat response", "chat", "cache_put")

        # ç™¼é€æ¶ˆæ¯
        self.logger.message(response)

        context.mark_step_complete("chat")
        self.logger.progress("chat", "end")

        return response


class KnowledgeProcessor(BaseProcessor):
    """çŸ¥è­˜æª¢ç´¢è™•ç†å™¨ - System 1 with Cache Support"""

    async def process(self, context: ProcessingContext) -> str:
        self.logger.progress("knowledge-retrieval", "start")
        context.set_current_step("knowledge-retrieval")

        # Step 1: Cache Check (System 1 ç‰¹æ€§ - ç¬¦åˆç‹€æ…‹æ©Ÿ)
        cache_key = f"knowledge:{context.request.query}"
        cache = getattr(self, 'cache', None)

        if cache:
            cached_response = cache.get(cache_key)
            if cached_response:
                self.logger.info("ğŸ’¾ Cache HIT for knowledge query", "knowledge", "cache_hit")
                self.logger.message(cached_response)
                context.mark_step_complete("knowledge-retrieval")
                self.logger.progress("knowledge-retrieval", "end")
                return cached_response

        # è¨˜éŒ„ RAG æ±ºç­–
        await self._log_tool_decision(
            "rag_retrieval",
            "ä½¿ç”¨çŸ¥è­˜åº«æª¢ç´¢ç›¸é—œæ–‡æª”",
            0.9
        )

        # Step 2: Generate Embeddings (ç¬¦åˆç‹€æ…‹æ©Ÿ)
        self.logger.progress("embedding", "start")
        self.logger.info(
            f"ğŸ”¢ Generating embeddings for query: {context.request.query[:100]}",
            "knowledge",
            "embedding"
        )
        self.logger.progress("embedding", "end")

        # Step 2: æœç´¢
        self.logger.progress("search", "start")

        knowledge_service = self.services.get("knowledge")
        relevant_docs = []

        if knowledge_service:
            try:
                self.logger.info(
                    f"ğŸ“š RAG Search: {context.request.query[:50]}...",
                    "rag", "search",
                    query=context.request.query,
                    vector_db="qdrant"
                )
                docs = await knowledge_service.retrieve(context.request.query, top_k=5)
                if docs:
                    relevant_docs = [
                        doc.get("content", str(doc)) for doc in docs
                    ]
            except Exception as e:
                self.logger.warning(f"Knowledge service error, using fallback: {e}", "knowledge", "fallback")

        # Fallback: no knowledge base â†’ LLM direct answer
        if not relevant_docs:
            self.logger.warning(
                "Knowledge base unavailable â€” falling back to LLM direct answer",
                "knowledge", "no_rag"
            )
            system_prompt = PromptTemplates.get_system_instruction()
            fallback_prompt = (
                f"{system_prompt}\n\n"
                f"[NOTE: Knowledge base is currently unavailable. "
                f"Answer based on your training data and clearly state that "
                f"this answer is NOT grounded in the local knowledge base.]\n\n"
                f"User: {context.request.query}"
            )
            response = await self._call_llm(fallback_prompt, context)
            self.logger.message(response)
            context.mark_step_complete("knowledge-retrieval")
            self.logger.progress("knowledge-retrieval", "end")
            return response

        # è¨˜éŒ„æª¢ç´¢çµæœåˆ°æ—¥èªŒ
        self.logger.info(
            f"ğŸ“– RAG Results: Found {len(relevant_docs)} relevant documents",
            "rag",
            "results",
            docs_count=len(relevant_docs)
        )

        self.logger.progress("search", "end", {"docs_found": len(relevant_docs)})

        # Step 3: æ–‡æª”é‡æ’åº (P1 å„ªåŒ–)
        if len(relevant_docs) > 1:
            self.logger.progress("rerank", "start")
            self.logger.info(
                f"ğŸ¯ Reranking {len(relevant_docs)} documents for relevance...",
                "knowledge",
                "reranking"
            )
            relevant_docs = await self._rerank_documents(relevant_docs, context.request.query)
            self.logger.progress("rerank", "end", {"reranked": len(relevant_docs)})

        # Step 4: ç”Ÿæˆç­”æ¡ˆ
        self.logger.info(
            f"ğŸ”„ Synthesizing answer from {len(relevant_docs)} retrieved documents...",
            "knowledge",
            "synthesis"
        )

        # ä½¿ç”¨çŸ¥è­˜æª¢ç´¢æç¤ºè©æ¨¡æ¿
        prompt = PromptTemplates.get_search_knowledge_result_prompt(
            query=context.request.query,
            research_goal="æä¾›æº–ç¢ºã€è©³ç´°çš„å›ç­”",
            context=' '.join(relevant_docs)
        )

        # åŠ ä¸Šå¼•ç”¨è¦å‰‡
        citation_rules = PromptTemplates.get_citation_rules()
        full_prompt = f"{prompt}\n\n{citation_rules}"

        response = await self._call_llm(full_prompt, context)

        # Step 5: Cache Put (System 1 ç‰¹æ€§ - ç¬¦åˆç‹€æ…‹æ©Ÿ)
        if cache:
            cache.put(cache_key, response, ttl=300)
            self.logger.info("ğŸ’¾ Cache PUT for knowledge response", "knowledge", "cache_put")

        # åªè¼¸å‡ºæœ€çµ‚ç­”æ¡ˆ
        self.logger.message(response)
        context.mark_step_complete("knowledge-retrieval")
        self.logger.progress("knowledge-retrieval", "end")

        return response

    async def _rerank_documents(self, docs: List[str], query: str) -> List[str]:
        """ä½¿ç”¨ LLM å°æ–‡æª”é€²è¡Œç›¸é—œæ€§é‡æ’åº"""
        import json

        # å¦‚æœæ–‡æª”å¤ªå¤šï¼Œåªé‡æ’å‰ 10 å€‹
        docs_to_rerank = docs[:10]

        # æº–å‚™é‡æ’åº prompt
        rerank_prompt = f"""Rank these documents by relevance to the query. Score each from 1-10.

Query: {query}

Documents:
{chr(10).join([f"{i+1}. {doc[:300]}..." for i, doc in enumerate(docs_to_rerank)])}

Output JSON format:
[{{"doc_id": 1, "score": 8}}, {{"doc_id": 2, "score": 6}}, ...]

Only include documents with score >= 5.
Output the ranking:"""

        try:
            response = await self._call_llm(rerank_prompt, None)

            # è§£ææ’å
            import re
            json_match = re.search(r'\[.*?\]', response, re.DOTALL)
            if json_match:
                rankings = json.loads(json_match.group(0))

                # æ ¹æ“šåˆ†æ•¸æ’åº
                rankings.sort(key=lambda x: x.get('score', 0), reverse=True)

                # é‡æ–°æ’åºæ–‡æª”
                reranked_docs = []
                for rank in rankings:
                    doc_id = rank.get('doc_id', 0) - 1  # è½‰ç‚º 0-based index
                    if 0 <= doc_id < len(docs_to_rerank) and rank.get('score', 0) >= 5:
                        reranked_docs.append(docs_to_rerank[doc_id])

                # å¦‚æœé‡æ’å¤±æ•—æˆ–çµæœå¤ªå°‘ï¼Œä¿ç•™åŸå§‹é †åºçš„å‰å¹¾å€‹
                if len(reranked_docs) < 2:
                    return docs[:5]

                self.logger.info(
                    f"âœ… Reranked {len(reranked_docs)} documents (filtered by relevance)",
                    "knowledge",
                    "rerank_complete"
                )
                return reranked_docs

        except Exception as e:
            self.logger.warning(f"Reranking failed, using original order: {e}", "knowledge", "rerank_error")

        # å¤±æ•—æ™‚è¿”å›åŸå§‹é †åº
        return docs[:5]


class SearchProcessor(BaseProcessor):
    """ç¶²è·¯æœç´¢è™•ç†å™¨ - æ”¯æ´è¿­ä»£æœç´¢èˆ‡è³ªé‡è©•ä¼°"""

    @enhanced_error_handler(max_retries=2, retryable_categories=["NETWORK", "LLM"])
    async def process(self, context: ProcessingContext) -> str:
        self.logger.progress("web-search", "start")
        context.set_current_step("web-search")

        # è¨˜éŒ„å·¥å…·æ±ºç­–
        await self._log_tool_decision(
            "web_search",
            "ç”¨æˆ¶æŸ¥è©¢éœ€è¦ç¶²è·¯æœç´¢ä¾†ç²å–æœ€æ–°è³‡è¨Š",
            0.95
        )

        # è¿­ä»£æœç´¢æ©Ÿåˆ¶
        MAX_ITERATIONS = 2
        all_search_results = []
        iteration = 0

        while iteration < MAX_ITERATIONS:
            iteration += 1
            self.logger.info(f"ğŸ”„ Search Iteration {iteration}/{MAX_ITERATIONS}", "search", "iteration")

            # Step 1: ç”Ÿæˆ SERP æŸ¥è©¢
            self.logger.progress("query-generation", "start")

            if iteration == 1:
                # ç¬¬ä¸€æ¬¡ï¼šåŸºæ–¼åŸå§‹æŸ¥è©¢ç”Ÿæˆ
                search_queries = await self._generate_serp_queries(context.request.query)
            else:
                # å¾ŒçºŒè¿­ä»£ï¼šåŸºæ–¼è³ªé‡è©•ä¼°æ”¹é€²æŸ¥è©¢
                search_queries = await self._refine_search_queries(
                    context.request.query,
                    all_search_results
                )

            if not search_queries:
                break

            self.logger.info(
                f"ğŸ“ Generated {len(search_queries)} search queries",
                "search",
                "queries_generated",
                queries=search_queries
            )
            self.logger.progress("query-generation", "end", {"queries": len(search_queries)})

            # Step 2: åŸ·è¡Œæœç´¢
            self.logger.progress("searching", "start")
            iteration_results = []
            for i, query_obj in enumerate(search_queries, 1):
                self.logger.info(
                    f"ğŸŒ Searching {i}/{len(search_queries)}: {query_obj.get('query', '')[:100]}",
                    "search",
                    "performing_search"
                )
                results = await self._perform_search(query_obj.get('query', ''))
                iteration_results.append({
                    'query': query_obj.get('query'),
                    'goal': query_obj.get('researchGoal'),
                    'results': results,
                    'iteration': iteration
                })
            self.logger.progress("searching", "end", {"total_results": len(iteration_results)})

            all_search_results.extend(iteration_results)

            # Step 3: è©•ä¼°æœç´¢è³ªé‡
            is_sufficient = await self._evaluate_search_quality(all_search_results, context.request.query)

            if is_sufficient:
                self.logger.info("âœ… Search quality is sufficient", "search", "quality_ok")
                break

            self.logger.info("ğŸ“Š Search needs refinement, continuing...", "search", "refine")

        # Step 4: åˆæˆæœ€çµ‚çµæœ
        combined_context = "\n\n".join([
            f"Query: {r['query']}\nGoal: {r['goal']}\nResults: {r['results']}"
            for r in all_search_results
        ])

        self.logger.info(
            f"ğŸ”„ Synthesizing {len(all_search_results)} search results...",
            "search",
            "synthesis"
        )

        prompt = PromptTemplates.get_search_result_prompt(
            query=context.request.query,
            research_goal="æä¾›å…¨é¢ã€æº–ç¢ºçš„ç­”æ¡ˆ",
            context=combined_context
        )

        citation_rules = PromptTemplates.get_citation_rules()
        full_prompt = f"{prompt}\n\n{citation_rules}"

        response = await self._call_llm(full_prompt, context)

        self.logger.message(response)
        context.mark_step_complete("web-search")
        self.logger.progress("web-search", "end")

        return response

    async def _generate_serp_queries(self, user_query: str) -> List[Dict[str, str]]:
        """ç”Ÿæˆå„ªåŒ–çš„ SERP æŸ¥è©¢ - ä½¿ç”¨å°ˆæ¥­ prompt"""
        import json

        # å…ˆç”Ÿæˆç°¡å–®çš„ç ”ç©¶è¨ˆåŠƒ
        plan = f"ç ”ç©¶ä¸»é¡Œ: {user_query}"

        # å®šç¾© schema
        output_schema = {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "query": {"type": "string"},
                    "researchGoal": {"type": "string"}
                }
            }
        }

        # ä½¿ç”¨å°ˆæ¥­çš„ SERP æŸ¥è©¢æç¤ºè©
        prompt = PromptTemplates.get_serp_queries_prompt(plan, output_schema)

        response = await self._call_llm(prompt, None)

        # è§£æ JSON å›æ‡‰
        try:
            # å¾å›æ‡‰ä¸­æå– JSON
            import re
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response)
            if json_match:
                queries = json.loads(json_match.group(1))
            else:
                # å¦‚æœæ²’æœ‰ markdown åŒ…è£ï¼Œç›´æ¥è§£æ
                queries = json.loads(response)
            return queries[:3]  # é™åˆ¶æœ€å¤š 3 å€‹æŸ¥è©¢
        except:
            # å¦‚æœè§£æå¤±æ•—ï¼Œè¿”å›é è¨­æŸ¥è©¢
            return [{"query": user_query, "researchGoal": "ç²å–ç›¸é—œè³‡è¨Š"}]

    async def _perform_search(self, query: str) -> str:
        """åŸ·è¡Œç¶²è·¯æœç´¢ - ä½¿ç”¨çœŸå¯¦æœç´¢æœå‹™æˆ– LLM fallback"""
        # è¨˜éŒ„æœç´¢æŸ¥è©¢
        search_service = self.services.get("search")
        provider = getattr(search_service, 'primary_provider', 'none') if search_service else 'none'
        self.logger.info(
            f"ğŸ” Web Query: {query}",
            "search",
            "query",
            query=query,
            provider=provider
        )

        # Use real search service if available
        raw_results = ""
        if search_service:
            try:
                results = await search_service.search(query, max_results=5)
                if results:
                    raw_results = "\n\n".join(
                        f"[{r.title}]\n{r.snippet}\nURL: {r.url}"
                        for r in results
                    )
                    self.logger.info(
                        f"ğŸ” Search returned {len(results)} results",
                        "search", "results"
                    )
            except Exception as e:
                self.logger.warning(f"Search service error, falling back to LLM: {e}", "search", "fallback")

        # Fallback: no search results â†’ LLM answers with disclaimer
        if not raw_results:
            self.logger.warning("No search results available â€” LLM will answer from training data", "search", "no_results")
            raw_results = (
                f"[Web search unavailable â€” no real-time results for '{query}'. "
                f"The following answer is based on the AI model's training data only.]"
            )

        if self.llm_client:
            result_prompt = PromptTemplates.get_query_result_prompt(
                query=query,
                research_goal="æä¾›æº–ç¢ºã€æœ€æ–°çš„è³‡è¨Š"
            )
            full_prompt = f"{result_prompt}\n\næœç´¢çµæœï¼š{raw_results}"
            processed_results = await self._call_llm(full_prompt, None)
            return processed_results

        return raw_results

    async def _evaluate_search_quality(self, results: List[Dict], original_query: str) -> bool:
        """è©•ä¼°æœç´¢çµæœè³ªé‡æ˜¯å¦å……åˆ†"""
        if not results:
            return False

        # ç°¡å–®çš„è³ªé‡æª¢æŸ¥
        total_content = sum(len(r.get('results', '')) for r in results)
        unique_queries = len(set(r['query'] for r in results))

        # åŸºæ–¼å…§å®¹é‡å’ŒæŸ¥è©¢å¤šæ¨£æ€§è©•ä¼°
        if total_content < 500 or unique_queries < 2:
            return False

        # ä½¿ç”¨ LLM è©•ä¼°ç›¸é—œæ€§
        evaluation_prompt = f"""Evaluate if the search results are sufficient for answering the query.

Original Query: {original_query}

Search Results Summary:
- Total results: {len(results)}
- Total content: {total_content} characters
- Unique queries: {unique_queries}

First few results:
{results[0].get('results', '')[:500] if results else 'No results'}

Answer with YES if sufficient, NO if more search is needed.
Consider: coverage, relevance, quality.

Answer (YES/NO):"""

        response = await self._call_llm(evaluation_prompt, None)
        return "YES" in response.upper()[:10]

    async def _refine_search_queries(self, original_query: str, previous_results: List[Dict]) -> List[Dict[str, str]]:
        """åŸºæ–¼å‰æ¬¡çµæœæ”¹é€²æœç´¢æŸ¥è©¢"""
        import json

        # æº–å‚™å·²æœ‰çµæœæ‘˜è¦
        results_summary = "\n".join([
            f"Query: {r['query']}\nFound: {r['results'][:200]}..."
            for r in previous_results[:3]
        ])

        refine_prompt = f"""Based on the original query and previous search results, generate improved search queries to fill knowledge gaps.

Original Query: {original_query}

Previous Search Results:
{results_summary}

Identify what's missing and generate 1-2 new search queries that would provide additional valuable information.

Output JSON array format:
[{{"query": "specific search query", "researchGoal": "what to find"}}]

Generate queries:"""

        response = await self._call_llm(refine_prompt, None)

        try:
            # å˜—è©¦è§£æ JSON
            import re
            json_match = re.search(r'\[.*?\]', response, re.DOTALL)
            if json_match:
                queries = json.loads(json_match.group(0))
                return queries[:2]  # é™åˆ¶æœ€å¤š2å€‹æ–°æŸ¥è©¢
        except:
            pass

        return []


class ThinkingProcessor(BaseProcessor):
    """æ·±åº¦æ€è€ƒè™•ç†å™¨"""

    @enhanced_error_handler(max_retries=1, retryable_categories=["LLM"])
    async def process(self, context: ProcessingContext) -> str:
        self.logger.progress("deep-thinking", "start")
        context.set_current_step("deep-thinking")

        # è¨˜éŒ„æ€è€ƒæ±ºç­–
        await self._log_tool_decision(
            "deep_thinking",
            "è¤‡é›œå•é¡Œéœ€è¦æ·±åº¦åˆ†æå’Œæ¨ç†",
            0.95
        )

        # è¨˜éŒ„æ€è€ƒè¨ˆåŠƒ
        self.logger.info(
            f"ğŸ§  Deep Thinking: Analyzing '{context.request.query[:50]}...'",
            "thinking",
            "start",
            category=LogCategory.TOOL,
            approach="multi-perspective-reasoning"
        )

        # Step 1: Problem decomposition and understanding
        self.logger.progress("problem-analysis", "start")
        self.logger.reasoning("Decomposing and understanding core elements...", streaming=True)

        # è¨˜éŒ„éšæ®µé–‹å§‹ (åªåœ¨æ—¥èªŒä¸­é¡¯ç¤º)
        self.logger.info(
            f"ğŸ” Stage 1: Problem Understanding & Decomposition",
            "thinking",
            "stage1",
            query=context.request.query[:100]
        )

        # ä½¿ç”¨æ€è€ƒæ¨¡å¼çš„å°ˆæ¥­æç¤ºè©
        thinking_prompt = PromptTemplates.get_thinking_mode_prompt(context.request.query)

        # åŸ·è¡Œæ·±åº¦æ€è€ƒ
        thinking_response = await self._call_llm(thinking_prompt, context)

        # å°‡çµæœè¼¸å‡ºåˆ°æ—¥èªŒ (ä¸æ˜¯ message)
        self.logger.info(
            f"ğŸ’­ Stage 1 Result: {thinking_response[:500]}...",
            "thinking",
            "stage1_result",
            full_length=len(thinking_response)
        )

        # è¨˜éŒ„éšæ®µå®Œæˆ
        self.logger.info(
            f"âœ… Stage 1: Problem Analysis Complete",
            "thinking",
            "stage1_complete",
            response_length=len(thinking_response)
        )

        self.logger.progress("problem-analysis", "end", {"analyzed": True})

        # Step 2: Multi-perspective analysis
        self.logger.progress("multi-perspective", "start")
        self.logger.reasoning("Analyzing from multiple perspectives...", streaming=True)

        # è¨˜éŒ„ç¬¬äºŒéšæ®µé–‹å§‹ (åªåœ¨æ—¥èªŒä¸­é¡¯ç¤º)
        self.logger.info(
            f"ğŸ” Stage 2: Critical Multi-Perspective Analysis",
            "thinking",
            "stage2"
        )

        # ä½¿ç”¨æ‰¹åˆ¤æ€§æ€ç¶­æç¤ºè©
        critical_prompt = PromptTemplates.get_critical_thinking_prompt(
            question=context.request.query,
            context=thinking_response
        )

        critical_analysis = await self._call_llm(critical_prompt, context)

        # å°‡çµæœè¼¸å‡ºåˆ°æ—¥èªŒ (ä¸æ˜¯ message)
        self.logger.info(
            f"ğŸ’­ Stage 2 Result: {critical_analysis[:500]}...",
            "thinking",
            "stage2_result",
            full_length=len(critical_analysis)
        )

        self.logger.progress("multi-perspective", "end", {"perspectives": 5})

        # Step 3: Deep reasoning
        self.logger.progress("deep-reasoning", "start")
        self.logger.reasoning("Conducting deep reasoning and logical analysis...", streaming=True)

        # è¨˜éŒ„ç¬¬ä¸‰éšæ®µé–‹å§‹ (åªåœ¨æ—¥èªŒä¸­é¡¯ç¤º)
        self.logger.info(
            f"ğŸ” Stage 3: Chain of Deep Reasoning",
            "thinking",
            "stage3"
        )

        # ä½¿ç”¨æ¨ç†éˆæç¤ºè©
        reasoning_prompt = PromptTemplates.get_chain_of_thought_prompt(context.request.query)

        chain_reasoning = await self._call_llm(reasoning_prompt, context)

        # å°‡çµæœè¼¸å‡ºåˆ°æ—¥èªŒ (ä¸æ˜¯ message)
        self.logger.info(
            f"ğŸ’­ Stage 3 Result: {chain_reasoning[:500]}...",
            "thinking",
            "stage3_result",
            full_length=len(chain_reasoning)
        )

        self.logger.progress("deep-reasoning", "end")

        # Step 4: Synthesis and reflection
        self.logger.progress("synthesis-reflection", "start")
        self.logger.reasoning("Synthesizing all analysis and reflecting...", streaming=True)

        # è¨˜éŒ„ç¬¬å››éšæ®µé–‹å§‹ (åªåœ¨æ—¥èªŒä¸­é¡¯ç¤º)
        self.logger.info(
            f"ğŸ” Stage 4: Synthesis & Reflection",
            "thinking",
            "stage4"
        )

        # ä½¿ç”¨åæ€æç¤ºè©
        reflection_prompt = PromptTemplates.get_reflection_prompt(
            original_response=f"{thinking_response}\n\n{critical_analysis}\n\n{chain_reasoning}",
            question=context.request.query
        )

        reflection = await self._call_llm(reflection_prompt, context)

        # å°‡çµæœè¼¸å‡ºåˆ°æ—¥èªŒ (ä¸æ˜¯ message)
        self.logger.info(
            f"ğŸ’­ Stage 4 Result: {reflection[:500]}...",
            "thinking",
            "stage4_result",
            full_length=len(reflection)
        )

        self.logger.progress("synthesis-reflection", "end")

        # Step 5: Final answer generation
        self.logger.progress("final-synthesis", "start")

        # è¨˜éŒ„æœ€çµ‚éšæ®µé–‹å§‹ (åªåœ¨æ—¥èªŒä¸­é¡¯ç¤º)
        self.logger.info(
            f"ğŸ¯ Stage 5: Final Comprehensive Answer",
            "thinking",
            "stage5"
        )

        # æº–å‚™æœ€çµ‚ç­”æ¡ˆæç¤ºè©
        final_synthesis_prompt = f"""
Based on the following deep thinking process, provide a comprehensive final answer to the question: "{context.request.query}"

Thinking Process Summary:
1. Problem Understanding: {thinking_response[:200]}...
2. Critical Analysis: {critical_analysis[:200]}...
3. Chain of Reasoning: {chain_reasoning[:200]}...
4. Reflection: {reflection[:200]}...

Please provide a complete, well-structured answer that synthesizes all insights from the above analysis.
"""

        # ä½¿ç”¨è¼¸å‡ºæŒ‡å—ç¢ºä¿ç­”æ¡ˆå“è³ª
        output_guidelines = PromptTemplates.get_output_guidelines()
        final_prompt = f"{final_synthesis_prompt}\n\n{output_guidelines}"

        final_response = await self._call_llm(final_prompt, context)

        # åªè¼¸å‡ºæœ€çµ‚ç­”æ¡ˆä½œç‚ºå›æ‡‰
        self.logger.message(final_response)

        self.logger.progress("final-synthesis", "end")

        context.mark_step_complete("deep-thinking")
        self.logger.progress("deep-thinking", "end")

        # åªè¿”å›æœ€çµ‚ç­”æ¡ˆ
        return final_response


class KnowledgeGraphProcessor(BaseProcessor):
    """çŸ¥è­˜åœ–è­œè™•ç†å™¨ - ç”Ÿæˆ Mermaid åœ–è¡¨"""

    async def process(self, context: ProcessingContext) -> str:
        self.logger.progress("knowledge-graph", "start")
        context.set_current_step("knowledge-graph")

        # Step 1: ç²å–æˆ–ç”Ÿæˆæ–‡ç« å…§å®¹
        self.logger.progress("content-preparation", "start")

        # å¦‚æœæ˜¯å•é¡Œï¼Œå…ˆç”Ÿæˆç›¸é—œå…§å®¹
        if "?" in context.request.query or len(context.request.query) < 100:
            # å…ˆç”Ÿæˆè©³ç´°å…§å®¹
            system_prompt = PromptTemplates.get_system_instruction()
            content_prompt = f"{system_prompt}\n\nè«‹é‡å°ä»¥ä¸‹ä¸»é¡Œç”Ÿæˆè©³ç´°çš„èªªæ˜æ–‡ç« ï¼š{context.request.query}"
            article = await self._call_llm(content_prompt, context)
        else:
            # ç›´æ¥ä½¿ç”¨æä¾›çš„å…§å®¹
            article = context.request.query

        self.logger.progress("content-preparation", "end")

        # Step 2: ç”ŸæˆçŸ¥è­˜åœ–è­œ
        self.logger.progress("graph-generation", "start")

        # ä½¿ç”¨å°ˆæ¥­çš„çŸ¥è­˜åœ–è­œ prompt
        graph_prompt = PromptTemplates.get_knowledge_graph_prompt()
        full_prompt = f"{graph_prompt}\n\næ–‡ç« å…§å®¹ï¼š\n{article}"

        mermaid_graph = await self._call_llm(full_prompt, context)

        self.logger.progress("graph-generation", "end")

        # Step 3: çµ„åˆæœ€çµ‚è¼¸å‡º
        response = f"""## çŸ¥è­˜åœ–è­œåˆ†æ

### åŸå§‹å…§å®¹æ‘˜è¦
{article[:500]}...

### çŸ¥è­˜åœ–è­œè¦–è¦ºåŒ–
{mermaid_graph}

### ä½¿ç”¨èªªæ˜
å°‡ä¸Šè¿° Mermaid ä»£ç¢¼è¤‡è£½åˆ°æ”¯æ´ Mermaid çš„ Markdown ç·¨è¼¯å™¨ä¸­å³å¯æŸ¥çœ‹åœ–è¡¨ã€‚
"""

        self.logger.message(response)
        context.mark_step_complete("knowledge-graph")
        self.logger.progress("knowledge-graph", "end")

        return response


class CodeProcessor(BaseProcessor):
    """ä»£ç¢¼åŸ·è¡Œè™•ç†å™¨"""

    @enhanced_error_handler(max_retries=1, retryable_categories=["LLM", "SANDBOX"])
    async def process(self, context: ProcessingContext) -> str:
        self.logger.progress("code-execution", "start")
        context.set_current_step("code-execution")

        # Step 1: è§£æä»£ç¢¼è«‹æ±‚
        self.logger.progress("code-analysis", "start")
        code_request = context.request.query
        self.logger.progress("code-analysis", "end")

        # Step 2: ç”Ÿæˆä»£ç¢¼ï¼ˆä½¿ç”¨å°ˆé–€çš„ promptï¼‰
        self.logger.progress("code-generation", "start")
        prompt = PromptTemplates.get_code_generation_prompt(code_request)
        generated_code = await self._call_llm(prompt, context)

        # æ¸…ç†å¯èƒ½çš„ç©ºç™½è¡Œ
        generated_code = generated_code.strip()

        self.logger.message(f"```python\n{generated_code}\n```")
        self.logger.progress("code-generation", "end")

        # Step 3: åŸ·è¡Œä»£ç¢¼ï¼ˆæ²™ç®±ç’°å¢ƒï¼‰
        self.logger.progress("code-execution", "start")
        result = await self._execute_code(generated_code)
        self.logger.progress("code-execution", "end", {"success": result.get("success")})

        response = f"ä»£ç¢¼åŸ·è¡Œçµæœï¼š\n{result.get('output', 'No output')}"
        self.logger.message(response)

        context.mark_step_complete("code-execution")
        self.logger.progress("code-execution", "end")

        return response

    async def _execute_code(self, code: str) -> Dict[str, Any]:
        """åœ¨æ²™ç®±ä¸­åŸ·è¡Œä»£ç¢¼ â€” ä½¿ç”¨çœŸå¯¦æ²™ç®±æœå‹™ï¼Œç„¡å‰‡å‘ŠçŸ¥ä½¿ç”¨è€…"""
        sandbox_service = self.services.get("sandbox")

        if sandbox_service:
            try:
                result = await sandbox_service.execute("execute_python", {
                    "code": code,
                    "timeout": 30
                })
                return {
                    "success": result.get("success", False),
                    "output": result.get("stdout", "") or result.get("error", "No output")
                }
            except Exception as e:
                self.logger.warning(f"Sandbox service error, using fallback: {e}", "code", "fallback")

        # Sandbox unavailable â€” return code only, do not fake execution
        self.logger.warning("Sandbox unavailable â€” code generated but not executed", "code", "no_sandbox")
        return {
            "success": False,
            "output": "[Sandbox unavailable] Code was generated but could not be executed. "
                      "Please set up the Docker sandbox to enable code execution."
        }


class RewritingProcessor(BaseProcessor):
    """æ–‡å­—é‡å¯«è™•ç†å™¨ - è½‰æ›ç‚º Markdown æ ¼å¼"""

    async def process(self, context: ProcessingContext) -> str:
        self.logger.progress("rewriting", "start")
        context.set_current_step("rewriting")

        # ç²å–è¦é‡å¯«çš„å…§å®¹
        content_to_rewrite = context.request.query

        # ä½¿ç”¨å°ˆæ¥­çš„é‡å¯« prompt
        rewriting_prompt = PromptTemplates.get_rewriting_prompt()
        full_prompt = f"{rewriting_prompt}\n\nText to rewrite:\n{content_to_rewrite}"

        # åŸ·è¡Œé‡å¯«
        self.logger.progress("markdown-conversion", "start")
        rewritten_content = await self._call_llm(full_prompt, context)
        self.logger.progress("markdown-conversion", "end")

        # è¼¸å‡ºçµæœ
        self.logger.message(rewritten_content)
        context.mark_step_complete("rewriting")
        self.logger.progress("rewriting", "end")

        return rewritten_content


# ============================================================
# Enhanced Deep Research Components
# ============================================================

class SearchProviderType(Enum):
    """æœç´¢å¼•æ“æä¾›å•†é¡å‹"""
    TAVILY = "tavily"
    EXA = "exa"  # Neural search with semantic understanding
    SERPER = "serper"
    BRAVE = "brave"
    DUCKDUCKGO = "duckduckgo"
    SEARXNG = "searxng"
    MODEL = "model"  # AIå…§å»ºæœç´¢


@dataclass
class SearchEngineConfig:
    """æœç´¢å¼•æ“é…ç½®"""
    primary: SearchProviderType = SearchProviderType.TAVILY
    fallback_chain: List[SearchProviderType] = None
    max_results: int = 10
    timeout: float = 30.0
    parallel_searches: int = 3
    # å¹³è¡Œç­–ç•¥é…ç½®
    enable_race_mode: bool = False  # ç«¶é€Ÿæ¨¡å¼ï¼šæ‰€æœ‰å¼•æ“åŒæ™‚æœç´¢
    enable_batch_parallel: bool = True  # æ‰¹æ¬¡å¹³è¡Œï¼šå¤šå€‹æŸ¥è©¢åŒæ™‚åŸ·è¡Œ
    batch_size: int = 3  # æ‰¹æ¬¡å¤§å°
    parallel_strategy: str = "batch"  # batch | race | hybrid

    def __post_init__(self):
        if self.fallback_chain is None:
            self.fallback_chain = [
                SearchProviderType.EXA,
                SearchProviderType.SERPER,
                SearchProviderType.DUCKDUCKGO,
                SearchProviderType.MODEL
            ]

        # æ ¹æ“šç­–ç•¥è¨­ç½®å°æ‡‰çš„æ¨™èªŒ
        if self.parallel_strategy == "race":
            self.enable_race_mode = True
            self.enable_batch_parallel = False
        elif self.parallel_strategy == "batch":
            self.enable_race_mode = False
            self.enable_batch_parallel = True
        elif self.parallel_strategy == "hybrid":
            # æ··åˆæ¨¡å¼ï¼šæ‰¹æ¬¡åŸ·è¡Œ + æ¯å€‹æŸ¥è©¢ä½¿ç”¨ç«¶é€Ÿ
            self.enable_race_mode = True
            self.enable_batch_parallel = True


@dataclass
class ResearchEvent:
    """ç ”ç©¶äº‹ä»¶"""
    type: str  # progress, message, reasoning, error, search_result
    step: str  # plan, search, synthesize
    data: Any
    timestamp: datetime = None

    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()

    def to_sse(self) -> str:
        """è½‰æ›ç‚º SSE æ ¼å¼"""
        event_data = {
            "type": self.type,
            "step": self.step,
            "data": self.data,
            "timestamp": self.timestamp.isoformat()
        }
        return f"data: {json.dumps(event_data, ensure_ascii=False)}\n\n"


class DeepResearchProcessor(BaseProcessor):
    """
    æ·±åº¦ç ”ç©¶è™•ç†å™¨ - Agent Level with Enhanced Features

    Features:
    - WorkflowState tracking and retry mechanism
    - SSE Streaming support
    - Multi-search engine configuration
    - Event-driven architecture
    - Closed-loop iteration (max 3 iterations)
    - Academic-style reference formatting
    """

    def __init__(self,
                 llm_client=None,
                 services: Optional[Dict[str, Any]] = None,
                 search_config: Optional[SearchEngineConfig] = None,
                 event_callback: Optional[Callable[[ResearchEvent], None]] = None):
        """
        åˆå§‹åŒ–å¢å¼·ç‰ˆè™•ç†å™¨

        Args:
            llm_client: LLMå®¢æˆ¶ç«¯
            services: æœå‹™å­—å…¸
            search_config: æœç´¢å¼•æ“é…ç½®
            event_callback: äº‹ä»¶å›èª¿å‡½æ•¸
        """
        super().__init__(llm_client, services)
        self.search_config = search_config or SearchEngineConfig()
        self.event_callback = event_callback
        self.event_queue: asyncio.Queue = asyncio.Queue()

        # åˆå§‹åŒ–å¢å¼·æ—¥èªŒç³»çµ±
        try:
            from core.enhanced_logger import get_enhanced_logger
            self.enhanced_logger = get_enhanced_logger()
        except ImportError:
            self.enhanced_logger = None
        self._streaming_enabled = False

    async def process(self, context: ProcessingContext) -> str:
        """åŸ·è¡Œå®Œæ•´çš„æ·±åº¦ç ”ç©¶æµç¨‹ - ç¬¦åˆ AgentRuntime è¦ç¯„"""

        # Step 1: Init Workflow (ç¬¦åˆç‹€æ…‹æ©Ÿ InitWorkflow)
        workflow_state = {
            "status": "running",
            "steps": ["plan", "search", "synthesize"],
            "current_step": None,
            "iterations": 0,
            "errors": []
        }
        context.intermediate_results["workflow_state"] = workflow_state

        # è¨˜éŒ„æ·±åº¦ç ”ç©¶æ±ºç­–
        await self._log_tool_decision(
            "deep_research",
            "åŸ·è¡Œå…¨é¢çš„æ·±åº¦ç ”ç©¶ä»¥å›ç­”è¤‡é›œå•é¡Œ",
            0.95
        )

        try:
            # åŸ·è¡Œç ”ç©¶æµç¨‹ (åŒ…è£¹åœ¨ retry é‚è¼¯ä¸­)
            return await self._execute_with_retry(context, workflow_state)
        except Exception as e:
            # WorkflowFailed: è¨˜éŒ„å¤±æ•—ç‹€æ…‹
            workflow_state["status"] = "failed"
            workflow_state["errors"].append({
                "error": str(e),
                "step": workflow_state["current_step"],
                "timestamp": datetime.now().isoformat()
            })
            self.logger.error(f"Research workflow failed: {e}", "deep_research", "workflow_failed")
            raise

    async def _execute_with_retry(self, context: ProcessingContext, workflow_state: dict) -> str:
        """åŸ·è¡Œç ”ç©¶æµç¨‹ï¼Œæ”¯æ´é‡è©¦æ©Ÿåˆ¶ (ç¬¦åˆç‹€æ…‹æ©Ÿ RetryBoundary)"""
        from core.errors import ErrorClassifier

        MAX_RETRIES = 2
        retry_count = 0
        last_error = None

        while retry_count <= MAX_RETRIES:
            try:
                # åŸ·è¡Œæ ¸å¿ƒç ”ç©¶æµç¨‹
                return await self._execute_research_workflow(context, workflow_state)

            except Exception as e:
                # Error Classification (ç¬¦åˆç‹€æ…‹æ©Ÿ ErrorHandling)
                error_category = ErrorClassifier.classify(e)

                workflow_state["errors"].append({
                    "error": str(e),
                    "category": error_category,
                    "retry_count": retry_count,
                    "step": workflow_state["current_step"]
                })

                if error_category in ["NETWORK", "LLM"] and retry_count < MAX_RETRIES:
                    # Retryable error - æŒ‡æ•¸é€€é¿é‡è©¦
                    retry_count += 1
                    delay = 2 ** retry_count  # Exponential backoff
                    self.logger.warning(
                        f"Retryable error ({error_category}), retrying {retry_count}/{MAX_RETRIES} after {delay}s",
                        "deep_research", "retry"
                    )
                    await asyncio.sleep(delay)
                    last_error = e
                else:
                    # Non-retryable or max retries exceeded
                    raise e

        # å¦‚æœæ‰€æœ‰é‡è©¦éƒ½å¤±æ•—
        if last_error:
            raise last_error

    async def _execute_research_workflow(self, context: ProcessingContext, workflow_state: dict) -> str:
        """åŸ·è¡Œæ ¸å¿ƒç ”ç©¶å·¥ä½œæµç¨‹"""

        # 0. å¦‚æœæŸ¥è©¢è¤‡é›œï¼Œå…ˆæ¾„æ¸…ç ”ç©¶æ–¹å‘
        workflow_state["current_step"] = "clarification"
        if await self._should_clarify(context):
            await self._ask_clarifying_questions(context)

        # 1. å ±å‘Šè¨ˆåŠƒéšæ®µ (WriteReportPlan)
        workflow_state["current_step"] = "plan"
        report_plan = await self._write_report_plan(context)

        # åˆå§‹åŒ–ç ”ç©¶è¿­ä»£
        MAX_ITERATIONS = 3
        all_search_results = []
        iteration = 0

        while iteration < MAX_ITERATIONS:
            iteration += 1
            workflow_state["iterations"] = iteration
            self.logger.info(f"ğŸ”„ Research Iteration {iteration}/{MAX_ITERATIONS}", "deep_research", "iteration")

            # 2. SERP æŸ¥è©¢ç”Ÿæˆ (GenerateSearchQueries)
            workflow_state["current_step"] = "search"
            if iteration == 1:
                search_tasks = await self._generate_serp_queries(context, report_plan)
            else:
                # å¾ŒçºŒè¿­ä»£ï¼šåŸºæ–¼å·²æœ‰çµæœç”Ÿæˆè£œå……æŸ¥è©¢
                search_tasks = await self._generate_followup_queries(
                    context, report_plan, all_search_results
                )

            if not search_tasks:  # æ²’æœ‰æ›´å¤šæŸ¥è©¢éœ€æ±‚
                break

            # 3. åŸ·è¡Œæœç´¢ä»»å‹™ (ExecuteSearchTasks)
            search_results = await self._execute_search_tasks(context, search_tasks)
            all_search_results.extend(search_results)

            # 4. è©•ä¼°ç ”ç©¶æ˜¯å¦å……åˆ†
            is_sufficient = await self._review_research_completeness(
                context, report_plan, all_search_results, iteration
            )

            if is_sufficient:
                self.logger.info("âœ… Research is sufficient, proceeding to final report", "deep_research", "complete")
                break

            self.logger.info(f"ğŸ“Š Research needs more depth, continuing...", "deep_research", "continue")

        # 5. ç”Ÿæˆæœ€çµ‚å ±å‘Š (WriteFinalReport)
        workflow_state["current_step"] = "synthesize"
        final_report = await self._write_final_report(context, all_search_results, report_plan)

        # WorkflowComplete: æ¨™è¨˜æˆåŠŸå®Œæˆ
        workflow_state["status"] = "completed"
        self.logger.info("âœ… Research workflow completed successfully", "deep_research", "workflow_complete")

        return final_report

    async def _should_clarify(self, context: ProcessingContext) -> bool:
        """åˆ¤æ–·æ˜¯å¦éœ€è¦æ¾„æ¸…ç ”ç©¶æ–¹å‘"""
        # åŸºæ–¼æŸ¥è©¢è¤‡é›œåº¦åˆ¤æ–·
        complexity_indicators = ['æ¯”è¼ƒ', 'åˆ†æ', 'è©•ä¼°', 'æ·±åº¦', 'å…¨é¢', 'è©³ç´°', 'å°æ¯”']
        query_lower = context.request.query.lower()
        return any(indicator in query_lower for indicator in complexity_indicators)

    async def _ask_clarifying_questions(self, context: ProcessingContext):
        """è©¢å•æ¾„æ¸…å•é¡Œä»¥æ›´å¥½ç†è§£ç ”ç©¶éœ€æ±‚"""
        self.logger.progress("clarification", "start")

        question_prompt = PromptTemplates.get_system_question_prompt(context.request.query)
        questions = await self._call_llm(question_prompt, context)

        self.logger.info(
            f"â“ Clarifying Questions Generated:\n{questions}",
            "deep_research",
            "clarification"
        )

        # é€™è£¡å¯ä»¥å¯¦éš›ç™¼é€çµ¦ç”¨æˆ¶ä¸¦ç²å–å›æ‡‰
        # ç›®å‰å…ˆè¨˜éŒ„ä¾›åƒè€ƒ
        context.intermediate_results["clarifying_questions"] = questions

        self.logger.progress("clarification", "end")

    async def _generate_followup_queries(self, context: ProcessingContext,
                                        report_plan: str,
                                        existing_results: List[Dict]) -> List[Dict]:
        """ç”Ÿæˆå¾ŒçºŒæŸ¥è©¢ä»¥å¡«è£œç ”ç©¶ç©ºç¼º"""
        self.logger.progress("followup-query", "start")

        # æº–å‚™å·²æœ‰å­¸ç¿’æˆæœ
        learnings = self._prepare_report_context(existing_results)

        # ä½¿ç”¨ review prompt ä¾†ç”Ÿæˆè£œå……æŸ¥è©¢
        output_schema = {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "query": {"type": "string"},
                    "researchGoal": {"type": "string"},
                    "priority": {"type": "number"}
                }
            }
        }

        review_prompt = PromptTemplates.get_review_prompt(
            plan=report_plan,
            learnings=learnings,
            suggestion="Focus on filling knowledge gaps and getting more specific details",
            output_schema=output_schema
        )

        response = await self._call_llm(review_prompt, context)

        # è§£ææ–°æŸ¥è©¢
        try:
            import re
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response)
            if json_match:
                queries = json.loads(json_match.group(1))
            else:
                queries = json.loads(response)
        except:
            queries = []

        self.logger.info(
            f"ğŸ“‹ Follow-up Queries: Generated {len(queries)} additional queries",
            "deep_research",
            "followup"
        )

        self.logger.progress("followup-query", "end")
        return queries

    async def _review_research_completeness(self, context: ProcessingContext,
                                           report_plan: str,
                                           search_results: List[Dict],
                                           iteration: int) -> bool:
        """è©•ä¼°ç ”ç©¶æ˜¯å¦å……åˆ†å®Œæ•´"""
        self.logger.progress("review", "start")

        # æº–å‚™è©•ä¼°ä¸Šä¸‹æ–‡
        learnings = self._prepare_report_context(search_results)

        # ç°¡å–®çš„å®Œæ•´æ€§æª¢æŸ¥
        review_prompt = f"""Based on the research plan and collected information, evaluate if the research is sufficient.

Research Plan:
{report_plan[:500]}

Collected Information Summary:
- Number of sources: {sum(len(r['result'].get('sources', [])) for r in search_results)}
- Topics covered: {len(search_results)}
- Current iteration: {iteration}

Learnings:
{learnings[:1000]}

Answer with YES if research is sufficient, NO if more research is needed.
Consider: coverage of all plan sections, depth of information, quality of sources.

Answer (YES/NO):"""

        response = await self._call_llm(review_prompt, context)

        is_sufficient = "YES" in response.upper()[:10]

        self.logger.info(
            f"ğŸ“Š Research Completeness: {'Sufficient' if is_sufficient else 'Needs more'}",
            "deep_research",
            "review",
            iteration=iteration,
            is_sufficient=is_sufficient
        )

        self.logger.progress("review", "end", {"is_sufficient": is_sufficient})

        return is_sufficient

    async def _write_report_plan(self, context: ProcessingContext) -> str:
        """Phase 1: ç”Ÿæˆç ”ç©¶å ±å‘Šè¨ˆç•«"""
        self.logger.progress("report-plan", "start")

        # è¨˜éŒ„è¨ˆåŠƒéšæ®µ
        self.logger.info(
            f"ğŸ“ Planning: Creating research plan for '{context.request.query[:50]}...'",
            "deep_research",
            "planning",
            phase="report-plan",
            query_length=len(context.request.query)
        )

        # ä½¿ç”¨å ±å‘Šè¨ˆåŠƒ prompt
        plan_prompt = PromptTemplates.get_report_plan_prompt(context.request.query)

        # æ¨ç†éç¨‹
        self.logger.reasoning("é–‹å§‹åˆ†æç ”ç©¶éœ€æ±‚...", streaming=True)
        plan = await self._call_llm(plan_prompt, context)

        # è¨˜éŒ„è¨ˆåŠƒåˆ°æ—¥èªŒ
        self.logger.info(
            f"ğŸ“‹ Research plan created: {plan[:300]}...",
            "deep_research",
            "plan_result",
            plan_length=len(plan)
        )

        self.logger.progress("report-plan", "end", {"plan": plan[:200]})

        return plan

    async def _generate_serp_queries(self, context: ProcessingContext, plan: str) -> List[Dict]:
        """Phase 2: ç”Ÿæˆ SERP æŸ¥è©¢"""
        self.logger.progress("serp-query", "start")

        # è¨˜éŒ„æŸ¥è©¢ç”Ÿæˆ
        self.logger.info(
            f"ğŸ” SERP Generation: Extracting search queries from plan",
            "deep_research",
            "serp",
            phase="serp-query"
        )

        # å®šç¾©è¼¸å‡º schema
        output_schema = {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "query": {"type": "string"},
                    "researchGoal": {"type": "string"},
                    "priority": {"type": "number"}
                }
            }
        }

        # ä½¿ç”¨ SERP æŸ¥è©¢ prompt
        serp_prompt = PromptTemplates.get_serp_queries_prompt(plan, output_schema)
        response = await self._call_llm(serp_prompt, context)

        # è§£ææŸ¥è©¢
        try:
            import re
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response)
            if json_match:
                queries = json.loads(json_match.group(1))
            else:
                queries = json.loads(response)
        except:
            queries = [{"query": context.request.query, "researchGoal": "General research", "priority": 1}]

        # è¨˜éŒ„ç”Ÿæˆçš„æŸ¥è©¢
        self.logger.info(
            f"ğŸ“‹ SERP Queries: Generated {len(queries)} search queries",
            "deep_research",
            "serp",
            queries_count=len(queries),
            queries=queries[:3]  # åªè¨˜éŒ„å‰3å€‹
        )

        self.logger.progress("serp-query", "end", {"queries": queries})

        return queries

    async def _execute_search_tasks(self, context: ProcessingContext, search_tasks: List[Dict]) -> List[Dict]:
        """Phase 3: åŸ·è¡Œæœç´¢ä»»å‹™ - æ”¯æ´æ‰¹æ¬¡å¹³è¡Œæœç´¢"""
        self.logger.progress("task-list", "start")

        # è¨˜éŒ„ä»»å‹™åˆ—è¡¨
        self.logger.info(
            f"ğŸ“‹ Task List: Executing {len(search_tasks)} search tasks (parallel batch size: {self.search_config.parallel_searches})",
            "deep_research",
            "tasks",
            phase="task-list",
            total_tasks=len(search_tasks),
            parallel_batch_size=self.search_config.parallel_searches
        )

        results = []

        # å°‡ä»»å‹™åˆ†æ‰¹åŸ·è¡Œ
        batch_size = self.search_config.parallel_searches

        for batch_start in range(0, len(search_tasks), batch_size):
            batch_end = min(batch_start + batch_size, len(search_tasks))
            batch_tasks = search_tasks[batch_start:batch_end]

            self.logger.info(
                f"ğŸš€ Executing batch {batch_start//batch_size + 1}: Tasks {batch_start+1}-{batch_end}",
                "deep_research",
                "batch_execution",
                batch_index=batch_start//batch_size + 1,
                batch_size=len(batch_tasks)
            )

            # æº–å‚™æ‰¹æ¬¡æœç´¢ä»»å‹™
            async_tasks = []
            for i, task in enumerate(batch_tasks, batch_start + 1):
                query = task.get('query', '')
                goal = task.get('researchGoal', '')
                priority = task.get('priority', 1)

                # è¨˜éŒ„æœç´¢ä»»å‹™
                self.logger.info(
                    f"ğŸ” Search Task {i}/{len(search_tasks)}: {query}",
                    "deep_research",
                    "search_task",
                    task_index=i,
                    query=query,
                    goal=goal,
                    priority=priority,
                    provider=self.search_config.primary.value
                )

                # æ·»åŠ åˆ°ç•°æ­¥ä»»å‹™åˆ—è¡¨
                async_tasks.append(
                    self._execute_single_search_task(i, task, query, goal, priority)
                )

            # å¹³è¡ŒåŸ·è¡Œæ‰¹æ¬¡æœç´¢
            batch_results = await asyncio.gather(*async_tasks, return_exceptions=True)

            # è™•ç†æ‰¹æ¬¡çµæœ
            for task, result in zip(batch_tasks, batch_results):
                if isinstance(result, Exception):
                    self.logger.error(
                        f"âŒ Search task failed: {str(result)}",
                        "deep_research",
                        "search_error",
                        error=str(result)
                    )
                    # å‰µå»ºéŒ¯èª¤çµæœ
                    result = {
                        'query': task.get('query', ''),
                        'goal': task.get('researchGoal', ''),
                        'priority': task.get('priority', 1),
                        'result': {
                            'error': str(result),
                            'sources': [],
                            'summary': f"Search failed: {str(result)}"
                        }
                    }

                results.append(result)

        self.logger.progress("task-list", "end")

        # è¨˜éŒ„ç¸½çµ
        total_sources = sum(len(r.get('result', {}).get('sources', [])) for r in results)
        self.logger.info(
            f"ğŸ“Š Search Summary: {total_sources} total sources from {len(search_tasks)} tasks",
            "deep_research",
            "summary",
            phase="search-complete",
            total_sources=total_sources,
            total_tasks=len(search_tasks)
        )

        return results

    async def _execute_single_search_task(self, index: int, task: Dict, query: str, goal: str, priority: int) -> Dict:
        """åŸ·è¡Œå–®å€‹æœç´¢ä»»å‹™"""
        try:
            # é–‹å§‹å–®å€‹æœç´¢ä»»å‹™
            self.logger.progress("search-task", "start", {"name": query})

            # æ¨ç†éç¨‹
            self.logger.reasoning(f"æ­£åœ¨æœç´¢ï¼š{query}...", streaming=True)

            # åŸ·è¡Œæœç´¢ï¼ˆæ”¯æ´å¤šå¼•æ“å¹³è¡Œæœç´¢ï¼‰
            search_result = await self._perform_parallel_deep_search(query, goal)

            # è¨˜éŒ„æœç´¢çµæœ
            self.logger.info(
                f"âœ… Search Result {index}: Found {len(search_result.get('sources', []))} sources",
                "deep_research",
                "search_result",
                task_index=index,
                sources_count=len(search_result.get('sources', [])),
                relevance_score=search_result.get('relevance', 0)
            )

            # æ¶ˆæ¯è¼¸å‡º
            self.logger.message(f"æœç´¢ {index}: {query}\nçµæœ: {search_result.get('summary', '')[:200]}...")

            # çµæŸå–®å€‹æœç´¢ä»»å‹™
            self.logger.progress("search-task", "end", {
                "name": query,
                "data": search_result
            })

            return {
                'query': query,
                'goal': goal,
                'priority': priority,
                'result': search_result
            }
        except Exception as e:
            self.logger.error(
                f"Error in search task: {str(e)}",
                "deep_research",
                "task_error"
            )
            raise

    async def _perform_parallel_deep_search(self, query: str, goal: str) -> Dict:
        """åŸ·è¡Œå¹³è¡Œæ·±åº¦æœç´¢ - åŒæ™‚ä½¿ç”¨å¤šå€‹æœç´¢å¼•æ“"""

        # å¦‚æœå•Ÿç”¨äº† race æ¨¡å¼ï¼ŒåŒæ™‚å•Ÿå‹•æ‰€æœ‰æœç´¢å¼•æ“
        if hasattr(self.search_config, 'enable_race_mode') and self.search_config.enable_race_mode:
            return await self._perform_race_search(query, goal)

        # å¦å‰‡ä½¿ç”¨å¢å¼·ç‰ˆæœç´¢ï¼ˆå¸¶é™ç´šï¼‰
        return await self._perform_deep_search_enhanced(query, goal)

    async def _perform_race_search(self, query: str, goal: str) -> Dict:
        """ç«¶é€Ÿæ¨¡å¼ï¼šåŒæ™‚åŸ·è¡Œå¤šå€‹æœç´¢å¼•æ“ï¼Œè¿”å›ç¬¬ä¸€å€‹æˆåŠŸçš„çµæœ"""

        # æº–å‚™æ‰€æœ‰æœç´¢æä¾›å•†
        providers = [self.search_config.primary] + (self.search_config.fallback_chain or [])

        self.logger.info(
            f"ğŸ Race mode: Starting {len(providers)} search engines in parallel",
            "deep_research",
            "race_mode",
            providers=[p.value for p in providers]
        )

        # å‰µå»ºæ‰€æœ‰æœç´¢ä»»å‹™
        search_tasks = [
            self._try_search_provider_with_timeout(provider, query, goal)
            for provider in providers
        ]

        # ä½¿ç”¨ asyncio.as_completed ç²å–ç¬¬ä¸€å€‹æˆåŠŸçš„çµæœ
        for future in asyncio.as_completed(search_tasks):
            try:
                result = await future
                if result and result.get('sources'):
                    provider_name = result.get('provider', 'unknown')
                    self.logger.info(
                        f"ğŸ† Race winner: {provider_name} returned first with {len(result.get('sources', []))} sources",
                        "deep_research",
                        "race_winner",
                        provider=provider_name
                    )
                    return result
            except Exception as e:
                # å¿½ç•¥å–®å€‹å¼•æ“çš„éŒ¯èª¤ï¼Œç¹¼çºŒç­‰å¾…å…¶ä»–å¼•æ“
                continue

        # å¦‚æœæ‰€æœ‰å¼•æ“éƒ½å¤±æ•—ï¼Œè¿”å›ç©ºçµæœ
        return {
            'summary': 'No search results available',
            'sources': [],
            'relevance': 0
        }

    async def _try_search_provider_with_timeout(self, provider: SearchProviderType, query: str, goal: str) -> Optional[Dict]:
        """å¸¶è¶…æ™‚çš„æœç´¢æä¾›å•†å˜—è©¦"""
        try:
            # ä½¿ç”¨é…ç½®çš„è¶…æ™‚æ™‚é–“
            timeout = getattr(self.search_config, 'timeout', 30.0)

            result = await asyncio.wait_for(
                self._try_search_provider(provider, query, goal),
                timeout=timeout
            )

            if result:
                result['provider'] = provider.value

            return result

        except asyncio.TimeoutError:
            self.logger.warning(
                f"â±ï¸ Search timeout for {provider.value} after {timeout}s",
                "deep_research",
                "timeout"
            )
            return None
        except Exception as e:
            self.logger.error(
                f"Search error with {provider.value}: {str(e)}",
                "deep_research",
                "provider_error"
            )
            return None

    async def _perform_deep_search(self, query: str, goal: str) -> Dict:
        """åŸ·è¡Œæ·±åº¦æœç´¢ â€” ä½¿ç”¨çœŸå¯¦æœç´¢æœå‹™ï¼Œç„¡å‰‡è¿”å›ç©ºçµæœ"""

        search_service = self.services.get("search")

        # è¨˜éŒ„ Web Query
        self.logger.info(
            f"ğŸŒ Web Query: {query}",
            "web",
            "query",
            query=query,
            goal=goal,
            search_engine="web" if search_service else "none",
            max_results=10
        )

        # Use real search service if available
        search_result = None
        if search_service:
            try:
                results = await search_service.search(query, max_results=10)
                if results:
                    sources = [
                        {'url': r.url, 'title': r.title, 'relevance': 0.9}
                        for r in results
                    ]
                    summary = "\n".join(
                        f"- {r.title}: {r.snippet}" for r in results[:5]
                    )
                    search_result = {
                        'summary': summary,
                        'sources': sources,
                        'relevance': 0.92,
                        'timestamp': datetime.now().isoformat()
                    }
            except Exception as e:
                self.logger.warning(f"Search service error in deep research: {e}", "web", "fallback")

        # Fallback: search unavailable â€” return empty result with disclaimer
        if not search_result:
            self.logger.warning(
                f"Web search unavailable for deep research query: {query}",
                "web", "no_results"
            )
            search_result = {
                'summary': f"[Web search unavailable] Unable to retrieve real-time results for '{query}'. "
                           f"The final report will be based on the AI model's training data only.",
                'sources': [],
                'relevance': 0.0,
                'timestamp': datetime.now().isoformat()
            }

        # è¨˜éŒ„æœç´¢çµæœè©³æƒ…
        self.logger.info(
            f"ğŸ”— Web Results: Retrieved {len(search_result['sources'])} sources",
            "web",
            "results",
            sources=search_result['sources'][:5],
            avg_relevance=search_result['relevance']
        )

        # å¦‚æœæœ‰ LLMï¼Œè™•ç†æœç´¢çµæœ
        if self.llm_client:
            result_prompt = PromptTemplates.get_search_result_prompt(
                query=query,
                research_goal=goal,
                context=json.dumps(search_result, ensure_ascii=False)
            )
            processed = await self._call_llm(result_prompt, None)
            search_result['processed'] = processed

        return search_result

    async def _write_final_report(self, context: ProcessingContext,
                                  search_results: List[Dict],
                                  report_plan: str) -> str:
        """Phase 4: ç”Ÿæˆæœ€çµ‚å ±å‘Š - å­¸è¡“è«–æ–‡æ ¼å¼ï¼ˆå€åˆ†å¼•ç”¨/æœªå¼•ç”¨ï¼‰"""
        self.logger.progress("final-report", "start")

        # è¨˜éŒ„æœ€çµ‚å ±å‘Šç”Ÿæˆ
        self.logger.info(
            f"ğŸ“‘ Final Report: Synthesizing {len(search_results)} search results",
            "deep_research",
            "final_report",
            phase="final-report",
            results_count=len(search_results),
            plan_length=len(report_plan)
        )

        # æº–å‚™ä¸Šä¸‹æ–‡å’Œåƒè€ƒæ–‡ç»
        combined_context = self._prepare_report_context(search_results)
        references_list = self._extract_references(search_results)

        # è¨˜éŒ„è¨˜æ†¶é«”æ“ä½œ
        self.logger.info(
            f"ğŸ’¾ Memory: Storing research context",
            "memory",
            "store",
            context_size=len(combined_context),
            chunks=len(search_results),
            type="research_report"
        )

        # æ§‹å»ºå¢å¼·çš„ promptï¼ŒåŒ…å«åƒè€ƒæ–‡ç»æŒ‡å¼•
        enhanced_prompt = self._build_academic_report_prompt(
            report_plan,
            combined_context,
            references_list,
            context.request.query
        )

        # æ¨ç†æœ€çµ‚å ±å‘Š
        self.logger.reasoning("ç¶œåˆæ‰€æœ‰ç ”ç©¶çµæœï¼Œç”Ÿæˆæœ€çµ‚å ±å‘Š...", streaming=True)

        # ç”Ÿæˆå ±å‘Šä¸»é«”
        report_body = await self._call_llm(enhanced_prompt, context)

        # åˆ†æå“ªäº›åƒè€ƒæ–‡ç»è¢«å¯¦éš›å¼•ç”¨
        cited_refs, uncited_refs = self._analyze_citations(report_body, references_list)

        # çµ„åˆå®Œæ•´å ±å‘Šï¼šä¸»é«” + å€åˆ†çš„åƒè€ƒæ–‡ç»
        final_report = self._format_report_with_categorized_references(
            report_body, cited_refs, uncited_refs, context
        )

        # è¨˜éŒ„è¨˜æ†¶é«”å›æ”¶
        self.logger.info(
            f"ğŸ’¾ Memory: Retrieved research synthesis",
            "memory",
            "retrieve",
            report_length=len(final_report),
            citations_included=True
        )

        # ç™¼é€æœ€çµ‚å ±å‘Š
        self.logger.message(final_report, streaming=False)

        # å ±å‘Šå…ƒæ•¸æ“š
        report_metadata = {
            "title": f"Research Report: {context.request.query[:50]}",
            "sections": self._extract_report_sections(final_report),
            "sources_used": sum(len(r['result'].get('sources', [])) for r in search_results),
            "word_count": len(final_report.split()),
            "timestamp": datetime.now().isoformat()
        }

        # è¨˜éŒ„å ±å‘Šå®Œæˆ
        self.logger.info(
            f"âœ… Report Completed: {report_metadata['word_count']} words, {report_metadata['sources_used']} sources",
            "deep_research",
            "complete",
            metadata=report_metadata
        )

        self.logger.progress("final-report", "end", {"data": report_metadata})

        return final_report

    def _prepare_report_context(self, search_results: List[Dict]) -> str:
        """æº–å‚™å ±å‘Šä¸Šä¸‹æ–‡"""
        context_parts = []
        for i, result in enumerate(search_results, 1):
            context_parts.append(f"""
            æœç´¢ {i}: {result['query']}
            ç›®æ¨™: {result['goal']}
            å„ªå…ˆç´š: {result.get('priority', 1)}
            çµæœæ‘˜è¦: {result['result'].get('summary', '')}
            è™•ç†çµæœ: {result['result'].get('processed', '')}
            ä¾†æºæ•¸é‡: {len(result['result'].get('sources', []))}
            """)
        return "\n\n".join(context_parts)

    def _extract_report_sections(self, report: str) -> List[str]:
        """æå–å ±å‘Šç« ç¯€"""
        import re
        # åŒ¹é… Markdown æ¨™é¡Œ
        headers = re.findall(r'^#{1,3}\s+(.+)$', report, re.MULTILINE)
        return headers[:10]  # è¿”å›å‰10å€‹ç« ç¯€æ¨™é¡Œ

    def _extract_references(self, search_results: List[Dict]) -> List[Dict]:
        """å¾æœç´¢çµæœä¸­æå–åƒè€ƒæ–‡ç»"""
        references = []
        ref_id = 1

        for result in search_results:
            sources = result.get('result', {}).get('sources', [])
            for source in sources:
                if source.get('url'):
                    references.append({
                        'id': ref_id,
                        'title': source.get('title', 'Untitled'),
                        'url': source.get('url'),
                        'query': result.get('query', ''),
                        'relevance': source.get('relevance', 0)
                    })
                    ref_id += 1

        # æŒ‰ç›¸é—œæ€§æ’åº
        references.sort(key=lambda x: x.get('relevance', 0), reverse=True)
        return references

    def _build_academic_report_prompt(self, plan: str, context: str,
                                     references: List[Dict], requirement: str) -> str:
        """æ§‹å»ºå­¸è¡“æ ¼å¼çš„å ±å‘Š prompt"""
        # æº–å‚™åƒè€ƒæ–‡ç»æ‘˜è¦
        ref_summary = "\n".join([
            f"[{ref['id']}] {ref['title']}"
            for ref in references[:20]  # æœ€å¤šä½¿ç”¨å‰20å€‹åƒè€ƒ
        ])

        prompt = f"""Generate a comprehensive research report based on the following information.

Research Plan:
{plan}

Research Context and Findings:
{context}

Available References:
{ref_summary}

Requirements:
1. Write in academic style with clear sections
2. Use inline citations like [1], [2], [3] when referencing information
3. Each claim should be supported by citations
4. DO NOT include a references section in your output (it will be added separately)
5. Focus on synthesis and analysis, not just summarization
6. Ensure logical flow between sections

User's Research Question:
{requirement}

IMPORTANT:
- Use citations [1] to [{len(references)}] naturally throughout the text
- Make the report comprehensive and detailed (aim for 1000+ words)
- Structure with clear headings using ## for main sections
- Write in professional, academic tone

Generate the report body (without references section):"""

        # åŠ ä¸Šè¼¸å‡ºæŒ‡å—
        output_guidelines = PromptTemplates.get_output_guidelines()
        return f"{prompt}\n\n{output_guidelines}"

    def _analyze_citations(self, report_body: str, references: List[Dict]) -> tuple:
        """åˆ†æå ±å‘Šä¸­å¯¦éš›å¼•ç”¨çš„åƒè€ƒæ–‡ç»"""
        import re

        # æ‰¾å‡ºæ‰€æœ‰å¼•ç”¨çš„ç·¨è™Ÿ
        citation_pattern = r'\[(\d+)\]'
        cited_numbers = set()

        for match in re.finditer(citation_pattern, report_body):
            try:
                ref_num = int(match.group(1))
                cited_numbers.add(ref_num)
            except ValueError:
                continue

        # åˆ†é¡åƒè€ƒæ–‡ç»
        cited_refs = []
        uncited_refs = []

        for ref in references:
            if ref['id'] in cited_numbers:
                cited_refs.append(ref)
            else:
                uncited_refs.append(ref)

        return cited_refs, uncited_refs

    def _format_report_with_categorized_references(self, report_body: str,
                                                   cited_refs: List[Dict],
                                                   uncited_refs: List[Dict],
                                                   context: ProcessingContext = None) -> str:
        """æ ¼å¼åŒ–å ±å‘Šï¼Œå€åˆ†å¼•ç”¨å’Œæœªå¼•ç”¨çš„åƒè€ƒæ–‡ç»"""

        # æ§‹å»ºåƒè€ƒæ–‡ç»éƒ¨åˆ†
        references_section = "\n\n---\n\n"

        # ç¬¬ä¸€éƒ¨åˆ†ï¼šå¼•ç”¨çš„åƒè€ƒæ–‡ç»
        if cited_refs:
            references_section += "## ğŸ“š åƒè€ƒæ–‡ç» (Cited References)\n\n"
            references_section += "*ä»¥ä¸‹ç‚ºå ±å‘Šä¸­å¯¦éš›å¼•ç”¨çš„æ–‡ç»ï¼š*\n\n"

            for ref in cited_refs[:30]:  # é™åˆ¶æœ€å¤š30å€‹
                ref_entry = f"[{ref['id']}] **{ref['title']}**\n"
                if ref.get('url'):
                    ref_entry += f"   ğŸ“ URL: {ref['url']}\n"
                if ref.get('query'):
                    ref_entry += f"   ğŸ” Search context: {ref['query'][:50]}...\n"
                references_section += f"{ref_entry}\n"

        # ç¬¬äºŒéƒ¨åˆ†ï¼šç›¸é—œä½†æœªå¼•ç”¨çš„åƒè€ƒæ–‡ç»
        if uncited_refs:
            references_section += "\n## ğŸ“– ç›¸é—œæ–‡ç» (Related Sources - Not Cited)\n\n"
            references_section += "*ä»¥ä¸‹ç‚ºç ”ç©¶éç¨‹ä¸­æŸ¥é–±ä½†æœªç›´æ¥å¼•ç”¨çš„ç›¸é—œè³‡æ–™ï¼š*\n\n"

            for ref in uncited_refs[:20]:  # é™åˆ¶æœ€å¤š20å€‹
                ref_entry = f"â€¢ {ref['title']}\n"
                if ref.get('url'):
                    ref_entry += f"  URL: {ref['url']}\n"
                references_section += f"{ref_entry}\n"

        # æ·»åŠ çµ±è¨ˆè³‡è¨Š
        references_section += f"\n---\n\n## ğŸ“Š å¼•ç”¨çµ±è¨ˆ (Citation Statistics)\n\n"
        references_section += f"- **å¯¦éš›å¼•ç”¨æ–‡ç»**: {len(cited_refs)} ç¯‡\n"
        references_section += f"- **ç›¸é—œæœªå¼•ç”¨æ–‡ç»**: {len(uncited_refs)} ç¯‡\n"
        references_section += f"- **ç¸½æŸ¥é–±æ–‡ç»**: {len(cited_refs) + len(uncited_refs)} ç¯‡\n"
        references_section += f"- **å¼•ç”¨ç‡**: {len(cited_refs) / max(1, len(cited_refs) + len(uncited_refs)) * 100:.1f}%\n"
        references_section += f"\n---\n"
        references_section += f"*Report generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"
        references_section += f"*Powered by OpenCode Deep Research Engine*"

        # çµ„åˆå®Œæ•´å ±å‘Š
        full_report = f"{report_body}{references_section}"

        # ä¿å­˜å ±å‘Šåˆ° Markdownï¼ˆå¦‚æœå¢å¼·æ—¥èªŒå™¨å¯ç”¨ï¼‰
        if self.enhanced_logger and context:
            try:
                # æº–å‚™å…ƒæ•¸æ“š
                metadata = {
                    "query": context.request.query if context.request else "N/A",
                    "mode": "deep_research",
                    "model": getattr(self.llm_client, 'model', 'unknown'),
                    "timestamp": datetime.now().isoformat(),
                    "duration_ms": context.intermediate_results.get("total_duration_ms", 0),
                    "tokens": context.intermediate_results.get("total_tokens", {}),
                    "citations": {
                        "cited_count": len(cited_refs),
                        "uncited_count": len(uncited_refs),
                        "total_count": len(cited_refs) + len(uncited_refs),
                        "citation_rate": len(cited_refs) / max(1, len(cited_refs) + len(uncited_refs)) * 100
                    },
                    "stages": context.intermediate_results.get("stages", [])
                }

                # ä¿å­˜åˆ° Markdown
                trace_id = context.trace_id if hasattr(context, 'trace_id') else str(hash(context.request.query))[:8]
                md_path = self.enhanced_logger.save_response_as_markdown(
                    full_report,
                    metadata,
                    trace_id
                )

                # è¨˜éŒ„é•·å…§å®¹ï¼ˆå¦‚æœè¶…éé™åˆ¶ï¼‰
                if len(full_report) > self.enhanced_logger.MAX_LOG_SIZE:
                    self.enhanced_logger.log_long_content(
                        "INFO",
                        "Deep Research Report Generated",
                        full_report,
                        trace_id,
                        "deep_research"
                    )

                self.logger.info(f"ğŸ“„ Report saved to: {md_path}", "deep_research", "markdown_saved")

            except Exception as e:
                self.logger.warning(f"Failed to save markdown report: {e}", "deep_research", "save_error")

        return full_report

    # ============================================================
    # Enhanced Deep Research Methods (SSE Streaming & Events)
    # ============================================================

    async def process_with_streaming(self, context: ProcessingContext) -> AsyncGenerator[str, None]:
        """
        æ”¯æ´ SSE Streaming çš„è™•ç†æ–¹æ³•

        Yields:
            SSE æ ¼å¼çš„äº‹ä»¶å­—ç¬¦ä¸²
        """
        self._streaming_enabled = True

        # å•Ÿå‹•äº‹ä»¶è™•ç†å”ç¨‹
        event_task = asyncio.create_task(self._event_stream_handler())

        try:
            # ç™¼é€é–‹å§‹äº‹ä»¶
            await self._emit_event(ResearchEvent(
                type="progress",
                step="init",
                data={"status": "start", "query": context.request.query}
            ))

            # åŸ·è¡Œç ”ç©¶æµç¨‹
            result = await self.process(context)

            # ç™¼é€å®Œæˆäº‹ä»¶
            await self._emit_event(ResearchEvent(
                type="progress",
                step="complete",
                data={"status": "complete", "result_length": len(result)}
            ))

            # ç™¼é€æœ€çµ‚çµæœ
            yield f"data: {json.dumps({'type': 'final_report', 'data': result}, ensure_ascii=False)}\n\n"

        finally:
            # æ¸…ç†
            self._streaming_enabled = False
            await self.event_queue.put(None)  # çµæŸä¿¡è™Ÿ
            await event_task

    async def _event_stream_handler(self):
        """è™•ç†äº‹ä»¶æµ"""
        while True:
            event = await self.event_queue.get()
            if event is None:
                break

            # å¦‚æœæœ‰å›èª¿å‡½æ•¸ï¼Œèª¿ç”¨å®ƒ
            if self.event_callback:
                try:
                    await self._call_event_callback(event)
                except Exception as e:
                    self.logger.warning(f"Event callback error: {e}", "deep_research", "callback_error")

    async def _call_event_callback(self, event: ResearchEvent):
        """å®‰å…¨èª¿ç”¨äº‹ä»¶å›èª¿"""
        if asyncio.iscoroutinefunction(self.event_callback):
            await self.event_callback(event)
        else:
            self.event_callback(event)

    async def _emit_event(self, event: ResearchEvent):
        """ç™¼é€äº‹ä»¶"""
        if hasattr(self, 'event_queue'):
            await self.event_queue.put(event)

        # åŒæ™‚è¨˜éŒ„åˆ°æ—¥èªŒ
        self.logger.info(
            f"ğŸ“¡ Event: {event.type} - {event.step}",
            "deep_research",
            "event",
            event_type=event.type,
            event_step=event.step
        )

    # ============================================================
    # Multi-Search Engine Support
    # ============================================================

    async def _perform_deep_search_enhanced(self, query: str, goal: str) -> Dict:
        """
        å¢å¼·ç‰ˆæ·±åº¦æœç´¢ - æ”¯æ´å¤šæœç´¢å¼•æ“å’Œæ™ºèƒ½é™ç´š
        """
        # ç™¼é€æœç´¢é–‹å§‹äº‹ä»¶
        await self._emit_event(ResearchEvent(
            type="progress",
            step="search",
            data={
                "status": "start",
                "query": query,
                "goal": goal,
                "provider": self.search_config.primary.value
            }
        ))

        # å˜—è©¦ä¸»è¦æœç´¢å¼•æ“
        search_result = await self._try_search_provider(
            self.search_config.primary,
            query,
            goal
        )

        # å¦‚æœä¸»è¦å¼•æ“å¤±æ•—ï¼Œå˜—è©¦å‚™ç”¨éˆ
        if not search_result or not search_result.get('sources'):
            for fallback_provider in self.search_config.fallback_chain:
                self.logger.info(
                    f"ğŸ”„ Switching to fallback: {fallback_provider.value}",
                    "deep_research",
                    "fallback"
                )

                await self._emit_event(ResearchEvent(
                    type="message",
                    step="search",
                    data={
                        "message": f"Switching to {fallback_provider.value}...",
                        "provider": fallback_provider.value
                    }
                ))

                search_result = await self._try_search_provider(
                    fallback_provider,
                    query,
                    goal
                )

                if search_result and search_result.get('sources'):
                    break

        # ç™¼é€æœç´¢çµæœäº‹ä»¶
        if search_result:
            await self._emit_event(ResearchEvent(
                type="search_result",
                step="search",
                data={
                    "query": query,
                    "sources_count": len(search_result.get('sources', [])),
                    "summary": search_result.get('summary', '')[:200]
                }
            ))

        return search_result or self._empty_search_result(query)

    async def _try_search_provider(self,
                                   provider: SearchProviderType,
                                   query: str,
                                   goal: str) -> Optional[Dict]:
        """å˜—è©¦ä½¿ç”¨ç‰¹å®šæœç´¢æä¾›å•†"""
        try:
            if provider == SearchProviderType.MODEL:
                # ä½¿ç”¨ AI æ¨¡å‹å…§å»ºæœç´¢
                return await self._model_based_search(query, goal)
            elif provider == SearchProviderType.EXA:
                # ä½¿ç”¨ Exa neural search
                return await self._exa_search(query, goal)

            # ä½¿ç”¨å…¶ä»–æœç´¢æœå‹™
            search_service = self.services.get("search")
            if search_service:
                # å¦‚æœæœå‹™æ”¯æ´è¨­ç½®æä¾›å•†
                if hasattr(search_service, 'set_provider'):
                    search_service.set_provider(provider.value.lower())

                results = await search_service.search(
                    query=query,
                    max_results=self.search_config.max_results
                )

                if results:
                    return self._format_search_results(results, provider.value)

        except Exception as e:
            self.logger.warning(
                f"Search error with {provider.value}: {e}",
                "deep_research",
                "search_error"
            )

        return None

    async def _exa_search(self, query: str, goal: str) -> Optional[Dict]:
        """ä½¿ç”¨ Exa API é€²è¡Œç¥ç¶“æœç´¢"""
        search_service = self.services.get("search")
        if not search_service:
            return None

        # åˆ¤æ–·æœç´¢é¡å‹
        search_type = "general"
        if "code" in goal.lower() or "programming" in goal.lower():
            search_type = "code"
        elif "research" in goal.lower() or "paper" in goal.lower():
            search_type = "research"
        elif "news" in goal.lower() or "latest" in goal.lower():
            search_type = "news"

        try:
            # ä½¿ç”¨æ•´åˆçš„æœç´¢æœå‹™ï¼ˆå·²åŒ…å« Exaï¼‰
            if hasattr(search_service, 'provider'):
                old_provider = search_service.provider
                search_service.provider = "exa"
                results = await search_service.search(
                    query=query,
                    max_results=self.search_config.max_results,
                    search_type=search_type
                )
                search_service.provider = old_provider

                if results:
                    return self._format_search_results(results, "exa")

        except Exception as e:
            self.logger.error(f"Exa search failed: {e}", "deep_research", "exa_error")

        return None

    async def _model_based_search(self, query: str, goal: str) -> Dict:
        """ä½¿ç”¨ AI æ¨¡å‹çš„å…§å»ºæœç´¢èƒ½åŠ›"""
        if not self.llm_client:
            return self._empty_search_result(query)

        search_prompt = f"""Please search and provide information about:
Query: {query}
Research Goal: {goal}

Provide a comprehensive answer based on your knowledge, formatted as:
1. Summary of findings
2. Key facts and details
3. Relevant context

Focus on accuracy and relevance."""

        try:
            response = await self._call_llm(search_prompt, None)

            return {
                'summary': response,
                'sources': [{
                    'title': 'AI Knowledge Base',
                    'url': 'model://knowledge',
                    'relevance': 0.8
                }],
                'relevance': 0.8,
                'timestamp': datetime.now().isoformat(),
                'provider': 'model'
            }
        except Exception as e:
            self.logger.error(f"Model search failed: {e}", "deep_research", "model_search_error")
            return self._empty_search_result(query)

    def _format_search_results(self, results: List, provider: str) -> Dict:
        """æ ¼å¼åŒ–æœç´¢çµæœ"""
        if not results:
            return None

        sources = []
        summary_parts = []

        for r in results[:self.search_config.max_results]:
            # é©é…ä¸åŒçš„çµæœæ ¼å¼
            if hasattr(r, 'url'):
                sources.append({
                    'url': r.url,
                    'title': getattr(r, 'title', 'Untitled'),
                    'relevance': getattr(r, 'score', 0.5)
                })
                summary_parts.append(f"- {r.title}: {getattr(r, 'snippet', '')[:100]}")
            elif isinstance(r, dict):
                sources.append({
                    'url': r.get('url', ''),
                    'title': r.get('title', 'Untitled'),
                    'relevance': r.get('score', 0.5)
                })
                summary_parts.append(f"- {r.get('title')}: {r.get('snippet', '')[:100]}")

        return {
            'summary': '\n'.join(summary_parts),
            'sources': sources,
            'relevance': sum(s['relevance'] for s in sources) / max(len(sources), 1),
            'timestamp': datetime.now().isoformat(),
            'provider': provider
        }

    def _empty_search_result(self, query: str) -> Dict:
        """è¿”å›ç©ºæœç´¢çµæœ"""
        return {
            'summary': f"[No search results available for: {query}]",
            'sources': [],
            'relevance': 0.0,
            'timestamp': datetime.now().isoformat(),
            'provider': 'none'
        }

    # Configuration methods
    def configure_search_engines(self, config: SearchEngineConfig):
        """å‹•æ…‹é…ç½®æœç´¢å¼•æ“"""
        self.search_config = config
        self.logger.info(
            f"Search engines configured: primary={config.primary.value}, "
            f"fallback={[p.value for p in config.fallback_chain]}",
            "deep_research",
            "config_update"
        )

    def enable_streaming(self, enabled: bool = True):
        """å•Ÿç”¨/ç¦ç”¨ streaming"""
        self._streaming_enabled = enabled

    def set_event_callback(self, callback: Callable[[ResearchEvent], None]):
        """è¨­ç½®äº‹ä»¶å›èª¿"""
        self.event_callback = callback


class ProcessorFactory:
    """è™•ç†å™¨å·¥å»  - å‰µå»ºå’Œç®¡ç†è™•ç†å™¨"""

    _processors: Dict[ProcessingMode, Type[BaseProcessor]] = {
        ProcessingMode.CHAT: ChatProcessor,
        ProcessingMode.KNOWLEDGE: KnowledgeProcessor,
        ProcessingMode.SEARCH: SearchProcessor,
        ProcessingMode.THINKING: ThinkingProcessor,
        ProcessingMode.CODE: CodeProcessor,
        ProcessingMode.DEEP_RESEARCH: DeepResearchProcessor,
    }

    # Cognitive level mapping for each processing mode
    COGNITIVE_MAPPING: Dict[str, str] = {
        "chat": "system1",
        "knowledge": "system1",
        "search": "system2",
        "code": "system2",
        "thinking": "system2",
        "deep_research": "agent",
    }

    def __init__(self, llm_client=None, services: Optional[Dict[str, Any]] = None):
        self.llm_client = llm_client
        self.services = services or {}
        self._instances: Dict[ProcessingMode, BaseProcessor] = {}

    def get_processor(self, mode: ProcessingMode) -> BaseProcessor:
        """ç²å–è™•ç†å™¨å¯¦ä¾‹"""
        if mode not in self._instances:
            processor_class = self._processors.get(mode, ChatProcessor)
            instance = processor_class(self.llm_client, services=self.services)
            instance._cognitive_level = self.COGNITIVE_MAPPING.get(mode.value)
            self._instances[mode] = instance

        return self._instances[mode]

    def register_processor(self, mode: ProcessingMode, processor_class: Type[BaseProcessor]):
        """è¨»å†Šè‡ªå®šç¾©è™•ç†å™¨"""
        self._processors[mode] = processor_class
        # æ¸…é™¤å·²æœ‰å¯¦ä¾‹ï¼Œä¸‹æ¬¡ç²å–æ™‚æœƒå‰µå»ºæ–°çš„
        if mode in self._instances:
            del self._instances[mode]