"""
Base Processor - Foundation for all processing strategies

Extracted from monolithic processor.py (2611 lines â†’ modular architecture)
Following Linus philosophy: simple data structures, no special cases, â‰¤500 lines
"""

from abc import ABC, abstractmethod
from typing import Dict, Optional, Any, List
import time

from ..models import ProcessingContext
from ..logger import structured_logger


class BaseProcessor(ABC):
    """è™•ç†å™¨åŸºé¡"""

    def __init__(self, llm_client=None, services: Optional[Dict[str, Any]] = None, mcp_client=None):
        self.llm_client = llm_client
        self.services = services or {}
        self.mcp_client = mcp_client
        self.logger = structured_logger
        self._cognitive_level: Optional[str] = None

    @abstractmethod
    async def process(self, context: ProcessingContext) -> str:
        """è™•ç†è«‹æ±‚ - å­é¡å¿…é ˆå¯¦ç¾"""
        pass

    async def _call_llm(self, prompt: str, context: ProcessingContext = None) -> str:
        """èª¿ç”¨ LLM - å…¬å…±æ–¹æ³•"""
        if not self.llm_client:
            raise RuntimeError("LLM client not configured â€” cannot process request")

        # # è¨˜éŒ„ prompt (æˆªå–å‰500å­—ç¬¦ç”¨æ–¼æ—¥èªŒ)
        # self.logger.info(
        #     f"ğŸ“ LLM Prompt: {prompt[:500]}...",
        #     "llm",
        #     "prompt",
        #     prompt_length=len(prompt),
        #     prompt_preview=prompt[:200]
        # )

        start_time = time.time()
        with self.logger.measure("llm_call"):
            # ä½¿ç”¨ return_token_info åƒæ•¸ç²å– token è³‡è¨Š
            result = await self.llm_client.generate(prompt, return_token_info=True)

            # è™•ç†è¿”å›å€¼
            if isinstance(result, tuple):
                response, token_info = result
                tokens_in = token_info.get("prompt_tokens", 0)
                tokens_out = token_info.get("completion_tokens", 0)
                total_tokens = token_info.get("total_tokens", 0)
            else:
                # å‘å¾Œå…¼å®¹ï¼šå¦‚æœè¿”å›çš„æ˜¯å­—ç¬¦ä¸²
                response = result
                tokens_in = len(prompt.split())  # ç²—ç•¥ä¼°ç®—
                tokens_out = len(response.split())  # ç²—ç•¥ä¼°ç®—
                total_tokens = tokens_in + tokens_out

            duration_ms = (time.time() - start_time) * 1000

            # è¨˜éŒ„ LLM èª¿ç”¨ (åŒ…å« token å’Œæ™‚é–“è³‡è¨Š)
            self.logger.log_llm_call(
                model=getattr(self.llm_client, 'model_name', getattr(self.llm_client, 'provider_name', 'unknown')),
                tokens_in=tokens_in,
                tokens_out=tokens_out,
                duration_ms=duration_ms
            )

            # è¨˜éŒ„ LLM Response (ç”¨æ–¼ debuggingï¼Œé¡¯ç¤ºå¯¦éš›è¼¸å‡º)
            # æª¢æŸ¥æ˜¯å¦éœ€è¦åˆ†å‰²é•·å…§å®¹
            try:
                from core.enhanced_logger import get_enhanced_logger
                enhanced_logger = get_enhanced_logger()

                if len(response) > 10000:  # è¶…é 10KB
                    # ä½¿ç”¨å¢å¼·æ—¥èªŒå™¨è™•ç†é•·å…§å®¹
                    trace_id = context.trace_id if context and hasattr(context, 'trace_id') else "unknown"
                    enhanced_logger.log_long_content(
                        "INFO",
                        f"LLM Response (Long: {len(response)} chars, {total_tokens} tokens)",
                        response,
                        trace_id,
                        "llm_response"
                    )
                    # ä¸»æ—¥èªŒåªè¨˜éŒ„æ‘˜è¦
                    self.logger.info(
                        f"ğŸ’¬ LLM Response [Long content: {len(response)} chars, see segments]",
                        "llm",
                        "response",
                        response_length=len(response),
                        total_tokens=total_tokens
                    )
                else:
                    # æ­£å¸¸è¨˜éŒ„
                    self.logger.info(
                        f"ğŸ’¬ LLM Response: {response[:5000]}...",
                        "llm",
                        "response",
                        response_length=len(response),
                        response_preview=response[:200]
                    )
            except ImportError:
                # å¦‚æœå¢å¼·æ—¥èªŒå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸå§‹æ–¹å¼
                self.logger.info(
                    f"ğŸ’¬ LLM Response: {response[:5000]}...",
                    "llm",
                    "response",
                    response_length=len(response),
                    response_preview=response[:200]
                )

            # æª¢æŸ¥éŸ¿æ‡‰æ˜¯å¦ç‚ºç©º
            if not response or response.strip() == "":
                self.logger.warning(
                    "LLM returned empty response",
                    "llm",
                    "empty_response",
                    model=getattr(self.llm_client, 'model_name', 'unknown')
                )
                response = "[LLM returned empty response - please check API configuration]"

            # æ›´æ–°ä¸Šä¸‹æ–‡çš„ token çµ±è¨ˆ
            if context:
                context.total_tokens += total_tokens

            return response

    async def _log_tool_decision(self, tool_name: str, reason: str, confidence: float = 0.9):
        """è¨˜éŒ„å·¥å…·æ±ºç­–"""
        self.logger.log_tool_decision(tool_name, confidence, reason)
        self.logger.info(
            f"ğŸ”§ Tool Decision: {tool_name}",
            "processor",
            "tool_decision",
            tool=tool_name,
            confidence=confidence,
            reason=reason
        )

    async def _call_mcp_tool(
        self, server_name: str, tool_name: str, arguments: Dict[str, Any]
    ) -> Any:
        """å‘¼å« MCP Server ä¸Šçš„å·¥å…·

        Args:
            server_name: MCP server åç¨±
            tool_name: Tool åç¨±
            arguments: Tool åƒæ•¸

        Returns:
            Tool å›å‚³çš„æ–‡å­—å…§å®¹

        Raises:
            RuntimeError: MCP client ä¸å¯ç”¨æˆ–å‘¼å«å¤±æ•—
        """
        if not self.mcp_client:
            raise RuntimeError(f"MCP client not available, cannot call {server_name}/{tool_name}")
        result = await self.mcp_client.call_tool(server_name, tool_name, arguments)
        if result.get("is_error"):
            raise RuntimeError(f"MCP tool error: {result.get('content')}")
        # Extract text from content items
        texts = [item.get("text", "") for item in result.get("content", [])]
        return "\n".join(texts)

    async def _get_mcp_tools(self) -> List[Dict[str, Any]]:
        """å–å¾—æ‰€æœ‰å¯ç”¨çš„ MCP tools"""
        if not self.mcp_client:
            return []
        return await self.mcp_client.list_tools()
