"""
Knowledge Processor - System 1 with RAG and Cache Support

Retrieves information from knowledge base with document reranking.
Extracted from monolithic processor.py
"""

import json
import re
from typing import List

from .base import BaseProcessor
from ..models import ProcessingContext
from ..prompts import PromptTemplates


class KnowledgeProcessor(BaseProcessor):
    """çŸ¥è­˜æª¢ç´¢è™•ç†å™¨ - System 1 with Cache Support"""

    async def process(self, context: ProcessingContext) -> str:
        self.logger.progress("knowledge-retrieval", "start")
        context.set_current_step("knowledge-retrieval")

        # Step 1: Cache Check (System 1 ç‰¹æ€§ - ç¬¦åˆç‹€æ…‹æ©Ÿ)
        cache_key = f"knowledge:{context.request.query}"
        cache = getattr(self, 'cache', None)

        if cache:
            cached_response = cache.get(cache_key)
            if cached_response:
                self.logger.info("ğŸ’¾ Cache HIT for knowledge query", "knowledge", "cache_hit")
                self.logger.message(cached_response)
                context.mark_step_complete("knowledge-retrieval")
                self.logger.progress("knowledge-retrieval", "end")
                return cached_response

        # è¨˜éŒ„ RAG æ±ºç­–
        await self._log_tool_decision(
            "rag_retrieval",
            "ä½¿ç”¨çŸ¥è­˜åº«æª¢ç´¢ç›¸é—œæ–‡æª”",
            0.9
        )

        # Step 2: Generate Embeddings (ç¬¦åˆç‹€æ…‹æ©Ÿ)
        self.logger.progress("embedding", "start")
        self.logger.info(
            f"ğŸ”¢ Generating embeddings for query: {context.request.query[:100]}",
            "knowledge",
            "embedding"
        )
        self.logger.progress("embedding", "end")

        # Step 2: æœç´¢
        self.logger.progress("search", "start")

        knowledge_service = self.services.get("knowledge")
        relevant_docs = []

        if knowledge_service:
            try:
                self.logger.info(
                    f"ğŸ“š RAG Search: {context.request.query[:50]}...",
                    "rag", "search",
                    query=context.request.query,
                    vector_db="qdrant"
                )
                docs = await knowledge_service.retrieve(context.request.query, top_k=5)
                if docs:
                    relevant_docs = [
                        doc.get("content", str(doc)) for doc in docs
                    ]
            except Exception as e:
                self.logger.warning(f"Knowledge service error, using fallback: {e}", "knowledge", "fallback")

        # Fallback: no knowledge base â†’ LLM direct answer
        if not relevant_docs:
            self.logger.warning(
                "Knowledge base unavailable â€” falling back to LLM direct answer",
                "knowledge", "no_rag"
            )
            system_prompt = PromptTemplates.get_system_instruction("knowledge")
            fallback_prompt = (
                f"{system_prompt}\n\n"
                f"[NOTE: Knowledge base is currently unavailable. "
                f"Answer based on your training data and clearly state that "
                f"this answer is NOT grounded in the local knowledge base.]\n\n"
                f"User: {context.request.query}"
            )
            response = await self._call_llm(fallback_prompt, context)
            self.logger.message(response)
            context.mark_step_complete("knowledge-retrieval")
            self.logger.progress("knowledge-retrieval", "end")
            return response

        # è¨˜éŒ„æª¢ç´¢çµæœåˆ°æ—¥èªŒ
        self.logger.info(
            f"ğŸ“– RAG Results: Found {len(relevant_docs)} relevant documents",
            "rag",
            "results",
            docs_count=len(relevant_docs)
        )

        self.logger.progress("search", "end", {"docs_found": len(relevant_docs)})

        # Step 3: æ–‡æª”é‡æ’åº (P1 å„ªåŒ–)
        if len(relevant_docs) > 1:
            self.logger.progress("rerank", "start")
            self.logger.info(
                f"ğŸ¯ Reranking {len(relevant_docs)} documents for relevance...",
                "knowledge",
                "reranking"
            )
            relevant_docs = await self._rerank_documents(relevant_docs, context.request.query)
            self.logger.progress("rerank", "end", {"reranked": len(relevant_docs)})

        # Step 4: ç”Ÿæˆç­”æ¡ˆ
        self.logger.info(
            f"ğŸ”„ Synthesizing answer from {len(relevant_docs)} retrieved documents...",
            "knowledge",
            "synthesis"
        )

        # ä½¿ç”¨çŸ¥è­˜æª¢ç´¢æç¤ºè©æ¨¡æ¿
        prompt = PromptTemplates.get_search_knowledge_result_prompt(
            query=context.request.query,
            research_goal="æä¾›æº–ç¢ºã€è©³ç´°çš„å›ç­”",
            context=' '.join(relevant_docs)
        )

        # åŠ ä¸Šå¼•ç”¨è¦å‰‡
        citation_rules = PromptTemplates.get_citation_rules()
        full_prompt = f"{prompt}\n\n{citation_rules}"

        response = await self._call_llm(full_prompt, context)

        # Step 5: Cache Put (System 1 ç‰¹æ€§ - ç¬¦åˆç‹€æ…‹æ©Ÿ)
        if cache:
            cache.put(cache_key, response, ttl=300)
            self.logger.info("ğŸ’¾ Cache PUT for knowledge response", "knowledge", "cache_put")

        # åªè¼¸å‡ºæœ€çµ‚ç­”æ¡ˆ
        self.logger.message(response)
        context.mark_step_complete("knowledge-retrieval")
        self.logger.progress("knowledge-retrieval", "end")

        return response

    async def _rerank_documents(self, docs: List[str], query: str) -> List[str]:
        """ä½¿ç”¨ LLM å°æ–‡æª”é€²è¡Œç›¸é—œæ€§é‡æ’åº"""
        # å¦‚æœæ–‡æª”å¤ªå¤šï¼Œåªé‡æ’å‰ 10 å€‹
        docs_to_rerank = docs[:10]

        # æº–å‚™é‡æ’åº prompt
        rerank_prompt = f"""Rank these documents by relevance to the query. Score each from 1-10.

Query: {query}

Documents:
{chr(10).join([f"{i+1}. {doc[:300]}..." for i, doc in enumerate(docs_to_rerank)])}

Output JSON format:
[{{"doc_id": 1, "score": 8}}, {{"doc_id": 2, "score": 6}}, ...]

Only include documents with score >= 5.
Output the ranking:"""

        try:
            response = await self._call_llm(rerank_prompt, None)

            # è§£ææ’å
            json_match = re.search(r'\[.*?\]', response, re.DOTALL)
            if json_match:
                rankings = json.loads(json_match.group(0))

                # æ ¹æ“šåˆ†æ•¸æ’åº
                rankings.sort(key=lambda x: x.get('score', 0), reverse=True)

                # é‡æ–°æ’åºæ–‡æª”
                reranked_docs = []
                for rank in rankings:
                    doc_id = rank.get('doc_id', 0) - 1  # è½‰ç‚º 0-based index
                    if 0 <= doc_id < len(docs_to_rerank) and rank.get('score', 0) >= 5:
                        reranked_docs.append(docs_to_rerank[doc_id])

                # å¦‚æœé‡æ’å¤±æ•—æˆ–çµæœå¤ªå°‘ï¼Œä¿ç•™åŸå§‹é †åºçš„å‰å¹¾å€‹
                if len(reranked_docs) < 2:
                    return docs[:5]

                self.logger.info(
                    f"âœ… Reranked {len(reranked_docs)} documents (filtered by relevance)",
                    "knowledge",
                    "rerank_complete"
                )
                return reranked_docs

        except Exception as e:
            self.logger.warning(f"Reranking failed, using original order: {e}", "knowledge", "rerank_error")

        # å¤±æ•—æ™‚è¿”å›åŸå§‹é †åº
        return docs[:5]
