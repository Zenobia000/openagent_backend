"""
Deep Research Processor - Agent-level research with multi-provider search

Comprehensive research processor with:
- Multi-iteration workflow with retry mechanism
- Multi-provider search engine support (Tavily, Exa, etc.)
- SSE streaming for real-time updates
- Academic-style reference formatting with citation tracking
- Critical analysis integration
- Event-driven architecture

Extracted from monolithic processor.py (1487 lines)
"""

import asyncio
import json
import re
from datetime import datetime
from typing import Dict, List, Optional, Any, Callable, AsyncGenerator
from collections import Counter

from ..base import BaseProcessor
from ...models_v2 import ProcessingContext
from ...prompts import PromptTemplates
from ...logger import structured_logger
from .config import SearchEngineConfig, SearchProviderType
from .events import ResearchEvent


class DeepResearchProcessor(BaseProcessor):
    """
    æ·±åº¦ç ”ç©¶è™•ç†å™¨ - Agent Level with Enhanced Features

    Features:
    - WorkflowState tracking and retry mechanism
    - SSE Streaming support
    - Multi-search engine configuration
    - Event-driven architecture
    - Closed-loop iteration (max 3 iterations)
    - Academic-style reference formatting
    """

    def __init__(self,
                 llm_client=None,
                 services: Optional[Dict[str, Any]] = None,
                 search_config: Optional[SearchEngineConfig] = None,
                 event_callback: Optional[Callable[[ResearchEvent], None]] = None,
                 mcp_client=None):
        """
        åˆå§‹åŒ–å¢å¼·ç‰ˆè™•ç†å™¨

        Args:
            llm_client: LLMå®¢æˆ¶ç«¯
            services: æœå‹™å­—å…¸
            search_config: æœç´¢å¼•æ“é…ç½®
            event_callback: äº‹ä»¶å›èª¿å‡½æ•¸
            mcp_client: MCP å®¢æˆ¶ç«¯ç®¡ç†å™¨
        """
        super().__init__(llm_client, services, mcp_client=mcp_client)
        self.search_config = search_config or SearchEngineConfig()
        self.event_callback = event_callback
        self.event_queue: asyncio.Queue = asyncio.Queue()

        # åˆå§‹åŒ–å¢å¼·æ—¥èªŒç³»çµ±
        try:
            from core.enhanced_logger import get_enhanced_logger
            self.enhanced_logger = get_enhanced_logger()
        except ImportError:
            self.enhanced_logger = None
        self._streaming_enabled = False

    async def process(self, context: ProcessingContext) -> str:
        """åŸ·è¡Œå®Œæ•´çš„æ·±åº¦ç ”ç©¶æµç¨‹ - ç¬¦åˆ AgentRuntime è¦ç¯„"""

        # Step 1: Init Workflow (ç¬¦åˆç‹€æ…‹æ©Ÿ InitWorkflow)
        workflow_state = {
            "status": "running",
            "steps": ["plan", "search", "synthesize"],
            "current_step": None,
            "iterations": 0,
            "errors": []
        }
        context.response.metadata["workflow_state"] = workflow_state

        # è¨˜éŒ„æ·±åº¦ç ”ç©¶æ±ºç­–
        await self._log_tool_decision(
            "deep_research",
            "åŸ·è¡Œå…¨é¢çš„æ·±åº¦ç ”ç©¶ä»¥å›ç­”è¤‡é›œå•é¡Œ",
            0.95
        )

        try:
            # åŸ·è¡Œç ”ç©¶æµç¨‹ (åŒ…è£¹åœ¨ retry é‚è¼¯ä¸­)
            return await self._execute_with_retry(context, workflow_state)
        except Exception as e:
            # WorkflowFailed: è¨˜éŒ„å¤±æ•—ç‹€æ…‹
            workflow_state["status"] = "failed"
            workflow_state["errors"].append({
                "error": str(e),
                "step": workflow_state["current_step"],
                "timestamp": datetime.now().isoformat()
            })
            self.logger.error(f"Research workflow failed: {e}", "deep_research", "workflow_failed")
            raise

    async def _execute_with_retry(self, context: ProcessingContext, workflow_state: dict) -> str:
        """åŸ·è¡Œç ”ç©¶æµç¨‹ï¼Œæ”¯æ´é‡è©¦æ©Ÿåˆ¶ (ç¬¦åˆç‹€æ…‹æ©Ÿ RetryBoundary)"""
        from core.errors import ErrorClassifier

        MAX_RETRIES = 2
        retry_count = 0
        last_error = None

        while retry_count <= MAX_RETRIES:
            try:
                # åŸ·è¡Œæ ¸å¿ƒç ”ç©¶æµç¨‹
                return await self._execute_research_workflow(context, workflow_state)

            except Exception as e:
                # Error Classification (ç¬¦åˆç‹€æ…‹æ©Ÿ ErrorHandling)
                error_category = ErrorClassifier.classify(e)

                workflow_state["errors"].append({
                    "error": str(e),
                    "category": error_category,
                    "retry_count": retry_count,
                    "step": workflow_state["current_step"]
                })

                if error_category in ["NETWORK", "LLM"] and retry_count < MAX_RETRIES:
                    # Retryable error - æŒ‡æ•¸é€€é¿é‡è©¦
                    retry_count += 1
                    delay = 2 ** retry_count  # Exponential backoff
                    self.logger.warning(
                        f"Retryable error ({error_category}), retrying {retry_count}/{MAX_RETRIES} after {delay}s",
                        "deep_research", "retry"
                    )
                    await asyncio.sleep(delay)
                    last_error = e
                else:
                    # Non-retryable or max retries exceeded
                    raise e

        # å¦‚æœæ‰€æœ‰é‡è©¦éƒ½å¤±æ•—
        if last_error:
            raise last_error

    async def _execute_research_workflow(self, context: ProcessingContext, workflow_state: dict) -> str:
        """åŸ·è¡Œæ ¸å¿ƒç ”ç©¶å·¥ä½œæµç¨‹"""

        # 0. å¦‚æœæŸ¥è©¢è¤‡é›œï¼Œå…ˆæ¾„æ¸…ç ”ç©¶æ–¹å‘
        workflow_state["current_step"] = "clarification"
        if await self._should_clarify(context):
            await self._ask_clarifying_questions(context)

        # 1. å ±å‘Šè¨ˆåŠƒéšæ®µ (WriteReportPlan)
        workflow_state["current_step"] = "plan"
        report_plan = await self._write_report_plan(context)

        # åˆå§‹åŒ–ç ”ç©¶è¿­ä»£
        MAX_ITERATIONS = 3
        all_search_results = []
        iteration = 0

        while iteration < MAX_ITERATIONS:
            iteration += 1
            workflow_state["iterations"] = iteration
            self.logger.info(f"ğŸ”„ Research Iteration {iteration}/{MAX_ITERATIONS}", "deep_research", "iteration")

            # 2. SERP æŸ¥è©¢ç”Ÿæˆ (GenerateSearchQueries)
            workflow_state["current_step"] = "search"
            if iteration == 1:
                search_tasks = await self._generate_serp_queries(context, report_plan)
            else:
                # å¾ŒçºŒè¿­ä»£ï¼šåŸºæ–¼å·²æœ‰çµæœç”Ÿæˆè£œå……æŸ¥è©¢
                search_tasks = await self._generate_followup_queries(
                    context, report_plan, all_search_results
                )

            if not search_tasks:  # æ²’æœ‰æ›´å¤šæŸ¥è©¢éœ€æ±‚
                break

            # 3. åŸ·è¡Œæœç´¢ä»»å‹™ (ExecuteSearchTasks)
            search_results = await self._execute_search_tasks(context, search_tasks)
            all_search_results.extend(search_results)

            # 4. è©•ä¼°ç ”ç©¶æ˜¯å¦å……åˆ†
            is_sufficient = await self._review_research_completeness(
                context, report_plan, all_search_results, iteration
            )

            if is_sufficient:
                self.logger.info("âœ… Research is sufficient, proceeding to final report", "deep_research", "complete")
                break

            self.logger.info(f"ğŸ“Š Research needs more depth, continuing...", "deep_research", "continue")

        # 4.5. æ‰¹åˆ¤æ€§åˆ†æéšæ®µ (å¯é¸ - å€Ÿé‘’ ThinkingProcessor)
        critical_analysis = None
        if await self._requires_critical_analysis(context.request.query):
            workflow_state["current_step"] = "critical_analysis"
            critical_analysis = await self._critical_analysis_stage(context, all_search_results, report_plan)

        # 5. ç”Ÿæˆæœ€çµ‚å ±å‘Š (WriteFinalReport)
        workflow_state["current_step"] = "synthesize"
        final_report = await self._write_final_report(context, all_search_results, report_plan, critical_analysis)

        # WorkflowComplete: æ¨™è¨˜æˆåŠŸå®Œæˆ
        workflow_state["status"] = "completed"
        self.logger.info("âœ… Research workflow completed successfully", "deep_research", "workflow_complete")

        return final_report

    async def _should_clarify(self, context: ProcessingContext) -> bool:
        """åˆ¤æ–·æ˜¯å¦éœ€è¦æ¾„æ¸…ç ”ç©¶æ–¹å‘"""
        # åŸºæ–¼æŸ¥è©¢è¤‡é›œåº¦åˆ¤æ–·
        complexity_indicators = ['æ¯”è¼ƒ', 'åˆ†æ', 'è©•ä¼°', 'æ·±åº¦', 'å…¨é¢', 'è©³ç´°', 'å°æ¯”']
        query_lower = context.request.query.lower()
        return any(indicator in query_lower for indicator in complexity_indicators)

    async def _ask_clarifying_questions(self, context: ProcessingContext):
        """è©¢å•æ¾„æ¸…å•é¡Œä»¥æ›´å¥½ç†è§£ç ”ç©¶éœ€æ±‚"""
        self.logger.progress("clarification", "start")

        question_prompt = PromptTemplates.get_system_question_prompt(context.request.query)
        questions = await self._call_llm(question_prompt, context)

        self.logger.info(
            f"â“ Clarifying Questions Generated:\n{questions}",
            "deep_research",
            "clarification"
        )

        # é€™è£¡å¯ä»¥å¯¦éš›ç™¼é€çµ¦ç”¨æˆ¶ä¸¦ç²å–å›æ‡‰
        # ç›®å‰å…ˆè¨˜éŒ„ä¾›åƒè€ƒ
        context.response.metadata["clarifying_questions"] = questions

        self.logger.progress("clarification", "end")

    async def _requires_critical_analysis(self, query: str) -> bool:
        """åˆ¤æ–·æ˜¯å¦éœ€è¦æ‰¹åˆ¤æ€§åˆ†æéšæ®µ"""

        # æ‰¹åˆ¤æ€§æ€è€ƒé—œéµè©
        critical_keywords = [
            # åˆ†æé¡
            'åˆ†æ', 'è©•ä¼°', 'æ‰¹åˆ¤', 'æª¢è¦–', 'æ€è€ƒ', 'åæ€',
            # æ¯”è¼ƒé¡
            'æ¯”è¼ƒ', 'å°æ¯”', 'å·®ç•°', 'å„ªç¼ºé»', 'åˆ©å¼Š',
            # æ·±åº¦æ€è€ƒé¡
            'ç‚ºä»€éº¼', 'å¦‚ä½•çœ‹å¾…', 'æ€éº¼çœ‹', 'è§€é»', 'çœ‹æ³•',
            # è¤‡é›œå•é¡Œé¡
            'å½±éŸ¿', 'åŸå› ', 'å¾Œæœ', 'è¶¨å‹¢', 'é æ¸¬',
            # å¤šè§’åº¦é¡
            'å„æ–¹é¢', 'å…¨é¢', 'æ·±å…¥', 'ç¶œåˆ', 'æ•´é«”'
        ]

        # å¯¦è­‰ç ”ç©¶ + æŠ½è±¡æ€è€ƒçš„æ··åˆé—œéµè©
        mixed_patterns = [
            ('è¶¨å‹¢', 'åˆ†æ'), ('ç™¼å±•', 'è©•ä¼°'), ('å¸‚å ´', 'è§€é»'),
            ('æ•¸æ“š', 'æ€è€ƒ'), ('ç ”ç©¶', 'æ‰¹åˆ¤'), ('å ±å‘Š', 'åæ€')
        ]

        query_lower = query.lower()

        # æª¢æŸ¥å–®ä¸€é—œéµè©
        has_critical_keywords = any(kw in query_lower for kw in critical_keywords)

        # æª¢æŸ¥æ··åˆæ¨¡å¼
        has_mixed_patterns = any(
            kw1 in query_lower and kw2 in query_lower
            for kw1, kw2 in mixed_patterns
        )

        # é•·æŸ¥è©¢ï¼ˆ>50å­—ç¬¦ï¼‰é€šå¸¸éœ€è¦æ›´æ·±åº¦çš„åˆ†æ
        is_complex_query = len(query) > 50

        # å¦‚æœç¬¦åˆä»¥ä¸Šä»»ä¸€æ¢ä»¶ï¼Œå•Ÿç”¨æ‰¹åˆ¤æ€§åˆ†æ
        return has_critical_keywords or has_mixed_patterns or is_complex_query

    async def _critical_analysis_stage(self, context: ProcessingContext,
                                     search_results: List[Dict],
                                     report_plan: str) -> str:
        """æ‰¹åˆ¤æ€§åˆ†æéšæ®µ - å€Ÿé‘’ ThinkingProcessor çš„èƒ½åŠ›"""

        self.logger.progress("critical-analysis", "start")
        self.logger.info(
            f"ğŸ§  Critical Analysis: Analyzing research findings from multiple perspectives",
            "deep_research",
            "critical_analysis",
            phase="critical-analysis"
        )

        # æº–å‚™åˆ†æä¸Šä¸‹æ–‡
        research_summary = self._summarize_search_results(search_results)

        # å€Ÿç”¨ ThinkingProcessor çš„æ‰¹åˆ¤æ€§æ€ç¶­æç¤ºè©
        critical_prompt = PromptTemplates.get_critical_thinking_prompt(
            question=context.request.query,
            context=f"Research Plan:\n{report_plan}\n\nResearch Findings:\n{research_summary}"
        )

        # åŸ·è¡Œæ‰¹åˆ¤æ€§åˆ†æ
        self.logger.reasoning("é€²è¡Œæ‰¹åˆ¤æ€§åˆ†æå’Œå¤šè§’åº¦æ€è€ƒ...", streaming=True)
        critical_analysis = await self._call_llm(critical_prompt, context)

        # è¨˜éŒ„åˆ†æçµæœåˆ°æ—¥èªŒ
        self.logger.info(
            f"ğŸ’­ Critical Analysis Result: {critical_analysis[:300]}...",
            "deep_research",
            "critical_analysis_result",
            full_length=len(critical_analysis)
        )

        # å„²å­˜åˆ°ä¸­é–“çµæœ
        context.response.metadata["critical_analysis"] = critical_analysis

        self.logger.progress("critical-analysis", "end")
        return critical_analysis

    def _summarize_search_results(self, search_results: List[Dict]) -> str:
        """å°‡æœç´¢çµæœç¸½çµç‚ºç°¡æ½”çš„ä¸Šä¸‹æ–‡"""

        summaries = []
        for i, result in enumerate(search_results[:5], 1):  # é™åˆ¶å‰5å€‹çµæœé¿å…ä¸Šä¸‹æ–‡éé•·
            query = result.get('query', 'Unknown')
            content = result.get('results', '')

            # æˆªå–æ¯å€‹çµæœçš„å‰200å­—ç¬¦
            content_preview = content[:200] + "..." if len(content) > 200 else content
            summaries.append(f"Search {i} - Query: {query}\nFindings: {content_preview}")

        return "\n\n".join(summaries)

    async def _generate_followup_queries(self, context: ProcessingContext,
                                        report_plan: str,
                                        existing_results: List[Dict]) -> List[Dict]:
        """ç”Ÿæˆå¾ŒçºŒæŸ¥è©¢ä»¥å¡«è£œç ”ç©¶ç©ºç¼º"""
        self.logger.progress("followup-query", "start")

        # æº–å‚™å·²æœ‰å­¸ç¿’æˆæœ
        learnings = self._prepare_report_context(existing_results)

        # ä½¿ç”¨ review prompt ä¾†ç”Ÿæˆè£œå……æŸ¥è©¢
        output_schema = {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "query": {"type": "string"},
                    "researchGoal": {"type": "string"},
                    "priority": {"type": "number"}
                }
            }
        }

        review_prompt = PromptTemplates.get_review_prompt(
            plan=report_plan,
            learnings=learnings,
            suggestion="Focus on filling knowledge gaps and getting more specific details",
            output_schema=output_schema
        )

        response = await self._call_llm(review_prompt, context)

        # è§£ææ–°æŸ¥è©¢
        try:
            import re
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response)
            if json_match:
                queries = json.loads(json_match.group(1))
            else:
                queries = json.loads(response)
        except:
            queries = []

        self.logger.info(
            f"ğŸ“‹ Follow-up Queries: Generated {len(queries)} additional queries",
            "deep_research",
            "followup"
        )

        self.logger.progress("followup-query", "end")
        return queries

    async def _review_research_completeness(self, context: ProcessingContext,
                                           report_plan: str,
                                           search_results: List[Dict],
                                           iteration: int) -> bool:
        """è©•ä¼°ç ”ç©¶æ˜¯å¦å……åˆ†å®Œæ•´"""
        self.logger.progress("review", "start")

        # æº–å‚™è©•ä¼°ä¸Šä¸‹æ–‡
        learnings = self._prepare_report_context(search_results)

        # ç°¡å–®çš„å®Œæ•´æ€§æª¢æŸ¥
        review_prompt = f"""Based on the research plan and collected information, evaluate if the research is sufficient.

Research Plan:
{report_plan[:500]}

Collected Information Summary:
- Number of sources: {sum(len(r['result'].get('sources', [])) for r in search_results)}
- Topics covered: {len(search_results)}
- Current iteration: {iteration}

Learnings:
{learnings[:1000]}

Answer with YES if research is sufficient, NO if more research is needed.
Consider: coverage of all plan sections, depth of information, quality of sources.

Answer (YES/NO):"""

        response = await self._call_llm(review_prompt, context)

        is_sufficient = "YES" in response.upper()[:10]

        self.logger.info(
            f"ğŸ“Š Research Completeness: {'Sufficient' if is_sufficient else 'Needs more'}",
            "deep_research",
            "review",
            iteration=iteration,
            is_sufficient=is_sufficient
        )

        self.logger.progress("review", "end", {"is_sufficient": is_sufficient})

        return is_sufficient

    async def _write_report_plan(self, context: ProcessingContext) -> str:
        """Phase 1: ç”Ÿæˆç ”ç©¶å ±å‘Šè¨ˆç•«"""
        self.logger.progress("report-plan", "start")

        # è¨˜éŒ„è¨ˆåŠƒéšæ®µ
        self.logger.info(
            f"ğŸ“ Planning: Creating research plan for '{context.request.query[:50]}...'",
            "deep_research",
            "planning",
            phase="report-plan",
            query_length=len(context.request.query)
        )

        # ä½¿ç”¨å ±å‘Šè¨ˆåŠƒ prompt
        plan_prompt = PromptTemplates.get_report_plan_prompt(context.request.query)

        # æ¨ç†éç¨‹
        self.logger.reasoning("é–‹å§‹åˆ†æç ”ç©¶éœ€æ±‚...", streaming=True)
        plan = await self._call_llm(plan_prompt, context)

        # è¨˜éŒ„è¨ˆåŠƒåˆ°æ—¥èªŒ
        self.logger.info(
            f"ğŸ“‹ Research plan created: {plan[:300]}...",
            "deep_research",
            "plan_result",
            plan_length=len(plan)
        )

        self.logger.progress("report-plan", "end", {"plan": plan[:200]})

        return plan

    async def _generate_serp_queries(self, context: ProcessingContext, plan: str) -> List[Dict]:
        """Phase 2: ç”Ÿæˆ SERP æŸ¥è©¢"""
        self.logger.progress("serp-query", "start")

        # è¨˜éŒ„æŸ¥è©¢ç”Ÿæˆ
        self.logger.info(
            f"ğŸ” SERP Generation: Extracting search queries from plan",
            "deep_research",
            "serp",
            phase="serp-query"
        )

        # å®šç¾©è¼¸å‡º schema
        output_schema = {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "query": {"type": "string"},
                    "researchGoal": {"type": "string"},
                    "priority": {"type": "number"}
                }
            }
        }

        # ä½¿ç”¨ SERP æŸ¥è©¢ prompt
        serp_prompt = PromptTemplates.get_serp_queries_prompt(plan, output_schema)
        response = await self._call_llm(serp_prompt, context)

        # è§£ææŸ¥è©¢
        try:
            import re
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response)
            if json_match:
                queries = json.loads(json_match.group(1))
            else:
                queries = json.loads(response)
        except:
            queries = [{"query": context.request.query, "researchGoal": "General research", "priority": 1}]

        # è¨˜éŒ„ç”Ÿæˆçš„æŸ¥è©¢
        self.logger.info(
            f"ğŸ“‹ SERP Queries: Generated {len(queries)} search queries",
            "deep_research",
            "serp",
            queries_count=len(queries),
            queries=queries[:3]  # åªè¨˜éŒ„å‰3å€‹
        )

        self.logger.progress("serp-query", "end", {"queries": queries})

        return queries

    async def _execute_search_tasks(self, context: ProcessingContext, search_tasks: List[Dict]) -> List[Dict]:
        """Phase 3: åŸ·è¡Œæœç´¢ä»»å‹™ - æ”¯æ´æ‰¹æ¬¡å¹³è¡Œæœç´¢"""
        self.logger.progress("task-list", "start")

        # è¨˜éŒ„ä»»å‹™åˆ—è¡¨
        self.logger.info(
            f"ğŸ“‹ Task List: Executing {len(search_tasks)} search tasks (parallel batch size: {self.search_config.parallel_searches})",
            "deep_research",
            "tasks",
            phase="task-list",
            total_tasks=len(search_tasks),
            parallel_batch_size=self.search_config.parallel_searches
        )

        results = []

        # å°‡ä»»å‹™åˆ†æ‰¹åŸ·è¡Œ
        batch_size = self.search_config.parallel_searches

        for batch_start in range(0, len(search_tasks), batch_size):
            batch_end = min(batch_start + batch_size, len(search_tasks))
            batch_tasks = search_tasks[batch_start:batch_end]

            self.logger.info(
                f"ğŸš€ Executing batch {batch_start//batch_size + 1}: Tasks {batch_start+1}-{batch_end}",
                "deep_research",
                "batch_execution",
                batch_index=batch_start//batch_size + 1,
                batch_size=len(batch_tasks)
            )

            # æº–å‚™æ‰¹æ¬¡æœç´¢ä»»å‹™
            async_tasks = []
            for i, task in enumerate(batch_tasks, batch_start + 1):
                query = task.get('query', '')
                goal = task.get('researchGoal', '')
                priority = task.get('priority', 1)

                # è¨˜éŒ„æœç´¢ä»»å‹™
                self.logger.info(
                    f"ğŸ” Search Task {i}/{len(search_tasks)}: {query}",
                    "deep_research",
                    "search_task",
                    task_index=i,
                    query=query,
                    goal=goal,
                    priority=priority,
                    provider=self.search_config.primary.value
                )

                # æ·»åŠ åˆ°ç•°æ­¥ä»»å‹™åˆ—è¡¨
                async_tasks.append(
                    self._execute_single_search_task(i, task, query, goal, priority)
                )

            # å¹³è¡ŒåŸ·è¡Œæ‰¹æ¬¡æœç´¢
            batch_results = await asyncio.gather(*async_tasks, return_exceptions=True)

            # è™•ç†æ‰¹æ¬¡çµæœ
            for task, result in zip(batch_tasks, batch_results):
                if isinstance(result, Exception):
                    self.logger.error(
                        f"âŒ Search task failed: {str(result)}",
                        "deep_research",
                        "search_error",
                        error=str(result)
                    )
                    # å‰µå»ºéŒ¯èª¤çµæœ
                    result = {
                        'query': task.get('query', ''),
                        'goal': task.get('researchGoal', ''),
                        'priority': task.get('priority', 1),
                        'result': {
                            'error': str(result),
                            'sources': [],
                            'summary': f"Search failed: {str(result)}"
                        }
                    }

                results.append(result)

        self.logger.progress("task-list", "end")

        # è¨˜éŒ„ç¸½çµ
        total_sources = sum(len(r.get('result', {}).get('sources', [])) for r in results)
        self.logger.info(
            f"ğŸ“Š Search Summary: {total_sources} total sources from {len(search_tasks)} tasks",
            "deep_research",
            "summary",
            phase="search-complete",
            total_sources=total_sources,
            total_tasks=len(search_tasks)
        )

        return results

    async def _execute_single_search_task(self, index: int, task: Dict, query: str, goal: str, priority: int) -> Dict:
        """åŸ·è¡Œå–®å€‹æœç´¢ä»»å‹™"""
        try:
            # é–‹å§‹å–®å€‹æœç´¢ä»»å‹™
            self.logger.progress("search-task", "start", {"name": query})

            # æ¨ç†éç¨‹
            self.logger.reasoning(f"æ­£åœ¨æœç´¢ï¼š{query}...", streaming=True)

            # åŸ·è¡Œæœç´¢ï¼ˆæ”¯æ´å¤šå¼•æ“å¹³è¡Œæœç´¢ï¼‰
            search_result = await self._perform_parallel_deep_search(query, goal)

            # è¨˜éŒ„æœç´¢çµæœ
            self.logger.info(
                f"âœ… Search Result {index}: Found {len(search_result.get('sources', []))} sources",
                "deep_research",
                "search_result",
                task_index=index,
                sources_count=len(search_result.get('sources', [])),
                relevance_score=search_result.get('relevance', 0)
            )

            # æ¶ˆæ¯è¼¸å‡º
            self.logger.message(f"æœç´¢ {index}: {query}\nçµæœ: {search_result.get('summary', '')[:200]}...")

            # çµæŸå–®å€‹æœç´¢ä»»å‹™
            self.logger.progress("search-task", "end", {
                "name": query,
                "data": search_result
            })

            return {
                'query': query,
                'goal': goal,
                'priority': priority,
                'result': search_result
            }
        except Exception as e:
            self.logger.error(
                f"Error in search task: {str(e)}",
                "deep_research",
                "task_error"
            )
            raise

    async def _perform_parallel_deep_search(self, query: str, goal: str) -> Dict:
        """åŸ·è¡Œå¹³è¡Œæ·±åº¦æœç´¢ - åŒæ™‚ä½¿ç”¨å¤šå€‹æœç´¢å¼•æ“"""

        # å¦‚æœå•Ÿç”¨äº† race æ¨¡å¼ï¼ŒåŒæ™‚å•Ÿå‹•æ‰€æœ‰æœç´¢å¼•æ“
        if hasattr(self.search_config, 'enable_race_mode') and self.search_config.enable_race_mode:
            return await self._perform_race_search(query, goal)

        # å¦å‰‡ä½¿ç”¨å¢å¼·ç‰ˆæœç´¢ï¼ˆå¸¶é™ç´šï¼‰
        return await self._perform_deep_search_enhanced(query, goal)

    async def _perform_race_search(self, query: str, goal: str) -> Dict:
        """ç«¶é€Ÿæ¨¡å¼ï¼šåŒæ™‚åŸ·è¡Œå¤šå€‹æœç´¢å¼•æ“ï¼Œè¿”å›ç¬¬ä¸€å€‹æˆåŠŸçš„çµæœ"""

        # æº–å‚™æ‰€æœ‰æœç´¢æä¾›å•†
        providers = [self.search_config.primary] + (self.search_config.fallback_chain or [])

        self.logger.info(
            f"ğŸ Race mode: Starting {len(providers)} search engines in parallel",
            "deep_research",
            "race_mode",
            providers=[p.value for p in providers]
        )

        # å‰µå»ºæ‰€æœ‰æœç´¢ä»»å‹™
        search_tasks = [
            self._try_search_provider_with_timeout(provider, query, goal)
            for provider in providers
        ]

        # ä½¿ç”¨ asyncio.as_completed ç²å–ç¬¬ä¸€å€‹æˆåŠŸçš„çµæœ
        for future in asyncio.as_completed(search_tasks):
            try:
                result = await future
                if result and result.get('sources'):
                    provider_name = result.get('provider', 'unknown')
                    self.logger.info(
                        f"ğŸ† Race winner: {provider_name} returned first with {len(result.get('sources', []))} sources",
                        "deep_research",
                        "race_winner",
                        provider=provider_name
                    )
                    return result
            except Exception as e:
                # å¿½ç•¥å–®å€‹å¼•æ“çš„éŒ¯èª¤ï¼Œç¹¼çºŒç­‰å¾…å…¶ä»–å¼•æ“
                continue

        # å¦‚æœæ‰€æœ‰å¼•æ“éƒ½å¤±æ•—ï¼Œè¿”å›ç©ºçµæœ
        return {
            'summary': 'No search results available',
            'sources': [],
            'relevance': 0
        }

    async def _try_search_provider_with_timeout(self, provider: SearchProviderType, query: str, goal: str) -> Optional[Dict]:
        # å¸¶è¶…æ™‚çš„æœç´¢æä¾›å•†å˜—è©¦
        try:
            # ä½¿ç”¨é…ç½®çš„è¶…æ™‚æ™‚é–“
            timeout = getattr(self.search_config, 'timeout', 30.0)

            result = await asyncio.wait_for(
                self._try_search_provider(provider, query, goal),
                timeout=timeout
            )

            if result:
                result['provider'] = provider.value

            return result

        except asyncio.TimeoutError:
            self.logger.warning(
                f"â±ï¸ Search timeout for {provider.value} after {timeout}s",
                "deep_research",
                "timeout"
            )
            return None
        except Exception as e:
            self.logger.error(
                f"Search error with {provider.value}: {str(e)}",
                "deep_research",
                "provider_error"
            )
            return None

    async def _perform_deep_search(self, query: str, goal: str) -> Dict:
        # åŸ·è¡Œæ·±åº¦æœç´¢ â€” ä½¿ç”¨çœŸå¯¦æœç´¢æœå‹™ï¼Œç„¡å‰‡è¿”å›ç©ºçµæœ

        search_service = self.services.get("search")

        # è¨˜éŒ„ Web Query
        self.logger.info(
            f"ğŸŒ Web Query: {query}",
            "web",
            "query",
            query=query,
            goal=goal,
            search_engine="web" if search_service else "none",
            max_results=10
        )

        # Use real search service if available
        search_result = None
        if search_service:
            try:
                results = await search_service.search(query, max_results=10)
                if results:
                    sources = [
                        {'url': r.url, 'title': r.title, 'relevance': 0.9}
                        for r in results
                    ]
                    summary = "\n".join(
                        f"- {r.title}: {r.snippet}" for r in results[:5]
                    )
                    search_result = {
                        'summary': summary,
                        'sources': sources,
                        'relevance': 0.92,
                        'timestamp': datetime.now().isoformat()
                    }
            except Exception as e:
                self.logger.warning(f"Search service error in deep research: {e}", "web", "fallback")

        # Fallback: search unavailable â€” return empty result with disclaimer
        if not search_result:
            self.logger.warning(
                f"Web search unavailable for deep research query: {query}",
                "web", "no_results"
            )
            search_result = {
                'summary': f"[Web search unavailable] Unable to retrieve real-time results for '{query}'. "
                           f"The final report will be based on the AI model's training data only.",
                'sources': [],
                'relevance': 0.0,
                'timestamp': datetime.now().isoformat()
            }

        # è¨˜éŒ„æœç´¢çµæœè©³æƒ…
        self.logger.info(
            f"ğŸ”— Web Results: Retrieved {len(search_result['sources'])} sources",
            "web",
            "results",
            sources=search_result['sources'][:5],
            avg_relevance=search_result['relevance']
        )

        # å¦‚æœæœ‰ LLMï¼Œè™•ç†æœç´¢çµæœ
        if self.llm_client:
            result_prompt = PromptTemplates.get_search_result_prompt(
                query=query,
                research_goal=goal,
                context=json.dumps(search_result, ensure_ascii=False)
            )
            processed = await self._call_llm(result_prompt, None)
            search_result['processed'] = processed

        return search_result

    async def _write_final_report(self, context: ProcessingContext,
                                  search_results: List[Dict],
                                  report_plan: str,
                                  critical_analysis: Optional[str] = None) -> str:
        # Phase 4: ç”Ÿæˆæœ€çµ‚å ±å‘Š - å­¸è¡“è«–æ–‡æ ¼å¼ï¼ˆå€åˆ†å¼•ç”¨/æœªå¼•ç”¨ï¼‰
        self.logger.progress("final-report", "start")

        # è¨˜éŒ„æœ€çµ‚å ±å‘Šç”Ÿæˆ
        self.logger.info(
            f"ğŸ“‘ Final Report: Synthesizing {len(search_results)} search results",
            "deep_research",
            "final_report",
            phase="final-report",
            results_count=len(search_results),
            plan_length=len(report_plan)
        )

        # æº–å‚™ä¸Šä¸‹æ–‡å’Œåƒè€ƒæ–‡ç»
        combined_context = self._prepare_report_context(search_results)
        references_list = self._extract_references(search_results)

        # è¨˜éŒ„è¨˜æ†¶é«”æ“ä½œ
        self.logger.info(
            f"ğŸ’¾ Memory: Storing research context",
            "memory",
            "store",
            context_size=len(combined_context),
            chunks=len(search_results),
            type="research_report"
        )

        # æ§‹å»ºå¢å¼·çš„ promptï¼ŒåŒ…å«åƒè€ƒæ–‡ç»æŒ‡å¼•å’Œæ‰¹åˆ¤æ€§åˆ†æ
        enhanced_prompt = self._build_academic_report_prompt(
            report_plan,
            combined_context,
            references_list,
            context.request.query,
            critical_analysis
        )

        # æ¨ç†æœ€çµ‚å ±å‘Š
        self.logger.reasoning("ç¶œåˆæ‰€æœ‰ç ”ç©¶çµæœï¼Œç”Ÿæˆæœ€çµ‚å ±å‘Š...", streaming=True)

        # ç”Ÿæˆå ±å‘Šä¸»é«”
        report_body = await self._call_llm(enhanced_prompt, context)

        # åˆ†æå“ªäº›åƒè€ƒæ–‡ç»è¢«å¯¦éš›å¼•ç”¨ï¼ˆå¢å¼·ç‰ˆï¼‰
        cited_refs, uncited_refs, citation_stats = self._analyze_citations(report_body, references_list)

        # çµ„åˆå®Œæ•´å ±å‘Šï¼šä¸»é«” + å€åˆ†çš„åƒè€ƒæ–‡ç»
        final_report = self._format_report_with_categorized_references(
            report_body, cited_refs, uncited_refs, context, critical_analysis is not None, citation_stats
        )

        # è¨˜éŒ„è¨˜æ†¶é«”å›æ”¶
        self.logger.info(
            f"ğŸ’¾ Memory: Retrieved research synthesis",
            "memory",
            "retrieve",
            report_length=len(final_report),
            citations_included=True
        )

        # ç™¼é€æœ€çµ‚å ±å‘Š
        self.logger.message(final_report, streaming=False)

        # å ±å‘Šå…ƒæ•¸æ“š
        report_metadata = {
            "title": f"Research Report: {context.request.query[:50]}",
            "sections": self._extract_report_sections(final_report),
            "sources_used": sum(len(r['result'].get('sources', [])) for r in search_results),
            "word_count": len(final_report.split()),
            "timestamp": datetime.now().isoformat()
        }

        # è¨˜éŒ„å ±å‘Šå®Œæˆ
        self.logger.info(
            f"âœ… Report Completed: {report_metadata['word_count']} words, {report_metadata['sources_used']} sources",
            "deep_research",
            "complete",
            metadata=report_metadata
        )

        self.logger.progress("final-report", "end", {"data": report_metadata})

        return final_report

    def _prepare_report_context(self, search_results: List[Dict]) -> str:
        # æº–å‚™å ±å‘Šä¸Šä¸‹æ–‡
        context_parts = []
        for i, result in enumerate(search_results, 1):
            context_parts.append(f"""
            æœç´¢ {i}: {result['query']}
            ç›®æ¨™: {result['goal']}
            å„ªå…ˆç´š: {result.get('priority', 1)}
            çµæœæ‘˜è¦: {result['result'].get('summary', '')}
            è™•ç†çµæœ: {result['result'].get('processed', '')}
            ä¾†æºæ•¸é‡: {len(result['result'].get('sources', []))}
            """)
        return "\n\n".join(context_parts)

    def _extract_report_sections(self, report: str) -> List[str]:
        # æå–å ±å‘Šç« ç¯€
        import re
        # åŒ¹é… Markdown æ¨™é¡Œ
        headers = re.findall(r'^#{1,3}\s+(.+)$', report, re.MULTILINE)
        return headers[:10]  # è¿”å›å‰10å€‹ç« ç¯€æ¨™é¡Œ

    def _extract_references(self, search_results: List[Dict]) -> List[Dict]:
        # å¾æœç´¢çµæœä¸­æå–åƒè€ƒæ–‡ç»
        references = []
        ref_id = 1

        for result in search_results:
            sources = result.get('result', {}).get('sources', [])
            for source in sources:
                if source.get('url'):
                    references.append({
                        'id': ref_id,
                        'title': source.get('title', 'Untitled'),
                        'url': source.get('url'),
                        'query': result.get('query', ''),
                        'relevance': source.get('relevance', 0)
                    })
                    ref_id += 1

        # æŒ‰ç›¸é—œæ€§æ’åº
        references.sort(key=lambda x: x.get('relevance', 0), reverse=True)
        return references

    def _build_academic_report_prompt(self, plan: str, context: str,
                                     references: List[Dict], requirement: str,
                                     critical_analysis: Optional[str] = None) -> str:
        # æ§‹å»ºå­¸è¡“æ ¼å¼çš„å ±å‘Š promptï¼ˆå«æ‰¹åˆ¤æ€§åˆ†æï¼‰
        # æº–å‚™åƒè€ƒæ–‡ç»æ‘˜è¦
        ref_summary = "\n".join([
            f"[{ref['id']}] {ref['title']}"
            for ref in references[:20]  # æœ€å¤šä½¿ç”¨å‰20å€‹åƒè€ƒ
        ])

        # åŸºç¤ prompt
        prompt = f"""Generate a comprehensive research report based on the following information.

Research Plan:
{plan}

Research Context and Findings:
{context}

Available References:
{ref_summary}"""

        # å¦‚æœæœ‰æ‰¹åˆ¤æ€§åˆ†æï¼Œæ·»åŠ åˆ° prompt ä¸­
        if critical_analysis:
            prompt += f"""

Critical Analysis (Multi-Perspective Thinking):
{critical_analysis}

IMPORTANT: Integrate the insights from the critical analysis throughout your report.
Use the multi-perspective thinking to enrich your conclusions and provide more nuanced views."""

        # æ·»åŠ è¦æ±‚
        prompt += f"""

Requirements:
1. Write in academic style with clear sections
2. Use inline citations like [1], [2], [3] when referencing information
3. Each claim should be supported by citations
4. DO NOT include a references section in your output (it will be added separately)
5. Focus on synthesis and analysis, not just summarization
6. Ensure logical flow between sections"""

        # å¦‚æœæœ‰æ‰¹åˆ¤æ€§åˆ†æï¼Œæ·»åŠ ç‰¹æ®Šè¦æ±‚
        if critical_analysis:
            prompt += """
7. Incorporate critical analysis insights to provide balanced, multi-perspective conclusions
8. Address potential limitations, counterarguments, or alternative interpretations
9. Demonstrate analytical depth beyond surface-level findings"""

        prompt += f"""

User's Research Question:
{requirement}"""

        prompt += f"""

IMPORTANT:
- Use citations [1] to [{len(references)}] naturally throughout the text
- Make the report comprehensive and detailed (aim for 1000+ words)
- Structure with clear headings using ## for main sections
- Write in professional, academic tone

Generate the report body (without references section):
"""

        return prompt

        # åŠ ä¸Šè¼¸å‡ºæŒ‡å—
        output_guidelines = PromptTemplates.get_output_guidelines()
        return f"{prompt}\n\n{output_guidelines}"

    def _analyze_citations(self, report_body: str, references: List[Dict]) -> tuple:
        """
        åˆ†æå ±å‘Šä¸­å¯¦éš›å¼•ç”¨çš„åƒè€ƒæ–‡ç»ï¼ˆå¢å¼·ç‰ˆï¼‰

        Returns:
            tuple: (cited_refs, uncited_refs, citation_stats)
            - cited_refs: è¢«å¼•ç”¨çš„æ–‡ç»åˆ—è¡¨ï¼ˆåŒ…å« citation_count å­—æ®µï¼‰
            - uncited_refs: æœªè¢«å¼•ç”¨çš„æ–‡ç»åˆ—è¡¨
            - citation_stats: è©³ç´°çµ±è¨ˆä¿¡æ¯å­—å…¸
        """
        import re
        from collections import Counter

        # æ‰¾å‡ºæ‰€æœ‰å¼•ç”¨çš„ç·¨è™ŸåŠå…¶å‡ºç¾æ¬¡æ•¸
        citation_pattern = r'\[(\d+)\]'
        citation_counts = Counter()
        invalid_citations = set()  # ç„¡æ•ˆå¼•ç”¨ï¼ˆæ²’æœ‰å°æ‡‰æ–‡ç»ï¼‰

        # å»ºç«‹æœ‰æ•ˆåƒè€ƒæ–‡ç» ID é›†åˆ
        valid_ref_ids = {ref['id'] for ref in references}

        # æƒæå ±å‘Šä¸­çš„æ‰€æœ‰å¼•ç”¨
        for match in re.finditer(citation_pattern, report_body):
            try:
                ref_num = int(match.group(1))
                citation_counts[ref_num] += 1

                # æª¢æ¸¬ç„¡æ•ˆå¼•ç”¨
                if ref_num not in valid_ref_ids:
                    invalid_citations.add(ref_num)
            except ValueError:
                continue

        # åˆ†é¡åƒè€ƒæ–‡ç»ä¸¦æ·»åŠ å¼•ç”¨æ¬¡æ•¸ä¿¡æ¯
        cited_refs = []
        uncited_refs = []

        for ref in references:
            if ref['id'] in citation_counts:
                # æ·»åŠ å¼•ç”¨æ¬¡æ•¸ä¿¡æ¯ï¼ˆä¸ä¿®æ”¹åŸå§‹ refï¼‰
                ref_with_count = ref.copy()
                ref_with_count['citation_count'] = citation_counts[ref['id']]
                cited_refs.append(ref_with_count)
            else:
                uncited_refs.append(ref)

        # æŒ‰å¼•ç”¨æ¬¡æ•¸æ’åºï¼ˆå¾é«˜åˆ°ä½ï¼‰
        cited_refs.sort(key=lambda x: x.get('citation_count', 0), reverse=True)

        # æ§‹å»ºè©³ç´°çµ±è¨ˆä¿¡æ¯
        citation_stats = {
            'total_citations': sum(citation_counts.values()),  # ç¸½å¼•ç”¨æ¬¡æ•¸
            'unique_citations': len(citation_counts),  # å”¯ä¸€å¼•ç”¨æ•¸
            'invalid_citations': list(invalid_citations),  # ç„¡æ•ˆå¼•ç”¨åˆ—è¡¨
            'most_cited': citation_counts.most_common(5),  # æœ€å¸¸å¼•ç”¨çš„å‰5å€‹
            'avg_citations_per_source': sum(citation_counts.values()) / max(1, len(citation_counts)),  # å¹³å‡æ¯å€‹ä¾†æºçš„å¼•ç”¨æ¬¡æ•¸
            'citation_distribution': dict(citation_counts)  # å®Œæ•´çš„å¼•ç”¨åˆ†ä½ˆ
        }

        return cited_refs, uncited_refs, citation_stats

    def _format_report_with_categorized_references(self, report_body: str,
                                                   cited_refs: List[Dict],
                                                   uncited_refs: List[Dict],
                                                   context: ProcessingContext = None,
                                                   has_critical_analysis: bool = False,
                                                   citation_stats: Dict = None) -> str:
        """
        æ ¼å¼åŒ–å ±å‘Šï¼Œå€åˆ†å¼•ç”¨å’Œæœªå¼•ç”¨çš„åƒè€ƒæ–‡ç»ï¼ˆå¢å¼·ç‰ˆï¼‰

        Args:
            citation_stats: è©³ç´°çš„å¼•ç”¨çµ±è¨ˆä¿¡æ¯ï¼ˆå¯é¸ï¼‰
        """
        # æ§‹å»ºåƒè€ƒæ–‡ç»éƒ¨åˆ†
        references_section = "\n\n---\n\n"

        # ç¬¬ä¸€éƒ¨åˆ†ï¼šå¼•ç”¨çš„åƒè€ƒæ–‡ç»ï¼ˆæŒ‰å¼•ç”¨æ¬¡æ•¸æ’åºï¼‰
        if cited_refs:
            references_section += "## ğŸ“š åƒè€ƒæ–‡ç» (Cited References)\n\n"
            references_section += "*ä»¥ä¸‹ç‚ºå ±å‘Šä¸­å¯¦éš›å¼•ç”¨çš„æ–‡ç»ï¼ˆæŒ‰å¼•ç”¨æ¬¡æ•¸æ’åºï¼‰ï¼š*\n\n"

            for ref in cited_refs[:30]:  # é™åˆ¶æœ€å¤š30å€‹
                citation_count = ref.get('citation_count', 0)
                citation_indicator = f" `Ã—{citation_count}`" if citation_count > 1 else ""

                ref_entry = f"[{ref['id']}] **{ref['title']}**{citation_indicator}\n"
                if ref.get('url'):
                    ref_entry += f"   ğŸ“ URL: {ref['url']}\n"
                if ref.get('query'):
                    ref_entry += f"   ğŸ” Search context: {ref['query'][:50]}...\n"
                references_section += f"{ref_entry}\n"

        # ç¬¬äºŒéƒ¨åˆ†ï¼šç›¸é—œä½†æœªå¼•ç”¨çš„åƒè€ƒæ–‡ç»
        if uncited_refs:
            references_section += "\n## ğŸ“– ç›¸é—œæ–‡ç» (Related Sources - Not Cited)\n\n"
            references_section += "*ä»¥ä¸‹ç‚ºç ”ç©¶éç¨‹ä¸­æŸ¥é–±ä½†æœªç›´æ¥å¼•ç”¨çš„ç›¸é—œè³‡æ–™ï¼š*\n\n"

            for ref in uncited_refs[:20]:  # é™åˆ¶æœ€å¤š20å€‹
                ref_entry = f"â€¢ {ref['title']}\n"
                if ref.get('url'):
                    ref_entry += f"  URL: {ref['url']}\n"
                references_section += f"{ref_entry}\n"

        # æ·»åŠ çµ±è¨ˆè³‡è¨Šï¼ˆå¢å¼·ç‰ˆï¼‰
        references_section += f"\n---\n\n## ğŸ“Š å¼•ç”¨çµ±è¨ˆ (Citation Statistics)\n\n"

        # åŸºæœ¬çµ±è¨ˆ
        references_section += f"### åŸºæœ¬æŒ‡æ¨™\n"
        references_section += f"- **å¯¦éš›å¼•ç”¨æ–‡ç»**: {len(cited_refs)} ç¯‡\n"
        references_section += f"- **ç›¸é—œæœªå¼•ç”¨æ–‡ç»**: {len(uncited_refs)} ç¯‡\n"
        references_section += f"- **ç¸½æŸ¥é–±æ–‡ç»**: {len(cited_refs) + len(uncited_refs)} ç¯‡\n"
        references_section += f"- **å¼•ç”¨ç‡**: {len(cited_refs) / max(1, len(cited_refs) + len(uncited_refs)) * 100:.1f}%\n"

        # å¢å¼·çµ±è¨ˆï¼ˆå¦‚æœæœ‰ citation_statsï¼‰
        if citation_stats:
            references_section += f"\n### å¼•ç”¨æ·±åº¦åˆ†æ\n"
            references_section += f"- **ç¸½å¼•ç”¨æ¬¡æ•¸**: {citation_stats['total_citations']} æ¬¡\n"
            references_section += f"- **å¹³å‡æ¯ç¯‡æ–‡ç»è¢«å¼•ç”¨**: {citation_stats['avg_citations_per_source']:.1f} æ¬¡\n"

            # æœ€å¸¸å¼•ç”¨çš„æ–‡ç»
            if citation_stats['most_cited']:
                references_section += f"- **æœ€å¸¸å¼•ç”¨**: "
                most_cited_strs = [f"[{ref_id}] ({count}æ¬¡)" for ref_id, count in citation_stats['most_cited'][:3]]
                references_section += ", ".join(most_cited_strs) + "\n"

            # ç„¡æ•ˆå¼•ç”¨è­¦å‘Š
            if citation_stats['invalid_citations']:
                references_section += f"\nâš ï¸ **è­¦å‘Š**: æª¢æ¸¬åˆ° {len(citation_stats['invalid_citations'])} å€‹ç„¡æ•ˆå¼•ç”¨ç·¨è™Ÿ: {citation_stats['invalid_citations']}\n"

        # å¦‚æœæœ‰æ‰¹åˆ¤æ€§åˆ†æï¼Œæ·»åŠ èªªæ˜
        references_section += f"\n### åˆ†ææ¨¡å¼\n"
        if has_critical_analysis:
            references_section += f"- **ç ”ç©¶æ¨¡å¼**: æ·±åº¦ç ”ç©¶ + æ‰¹åˆ¤æ€§æ€è€ƒ ğŸ§ \n"
            references_section += f"- **åˆ†æå±¤æ¬¡**: å¤šè§’åº¦æ‰¹åˆ¤æ€§åˆ†æ\n"
        else:
            references_section += f"- **ç ”ç©¶æ¨¡å¼**: æ·±åº¦ç ”ç©¶\n"

        references_section += f"\n---\n"
        references_section += f"*Report generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"
        references_section += f"*Powered by OpenCode Deep Research Engine"

        if has_critical_analysis:
            references_section += f" with Critical Analysis*"
        else:
            references_section += f"*"

        # çµ„åˆå®Œæ•´å ±å‘Š
        full_report = f"{report_body}{references_section}"

        # ä¿å­˜å ±å‘Šåˆ° Markdownï¼ˆå¦‚æœå¢å¼·æ—¥èªŒå™¨å¯ç”¨ï¼‰
        if self.enhanced_logger and context:
            try:
                # æº–å‚™å…ƒæ•¸æ“š
                metadata = {
                    "query": context.request.query if context.request else "N/A",
                    "mode": "deep_research",
                    "model": getattr(self.llm_client, 'model', 'unknown'),
                    "timestamp": datetime.now().isoformat(),
                    "duration_ms": context.response.metadata.get("total_duration_ms", 0),
                    "tokens": context.response.metadata.get("total_tokens", {}),
                    "citations": {
                        "cited_count": len(cited_refs),
                        "uncited_count": len(uncited_refs),
                        "total_count": len(cited_refs) + len(uncited_refs),
                        "citation_rate": len(cited_refs) / max(1, len(cited_refs) + len(uncited_refs)) * 100
                    },
                    "stages": context.response.metadata.get("stages", [])
                }

                # ä¿å­˜åˆ° Markdown
                trace_id = context.trace_id if hasattr(context, 'trace_id') else str(hash(context.request.query))[:8]
                md_path = self.enhanced_logger.save_response_as_markdown(
                    full_report,
                    metadata,
                    trace_id
                )

                # è¨˜éŒ„é•·å…§å®¹ï¼ˆå¦‚æœè¶…éé™åˆ¶ï¼‰
                if len(full_report) > self.enhanced_logger.MAX_LOG_SIZE:
                    self.enhanced_logger.log_long_content(
                        "INFO",
                        "Deep Research Report Generated",
                        full_report,
                        trace_id,
                        "deep_research"
                    )

                self.logger.info(f"ğŸ“„ Report saved to: {md_path}", "deep_research", "markdown_saved")

            except Exception as e:
                self.logger.warning(f"Failed to save markdown report: {e}", "deep_research", "save_error")

        return full_report

    # ============================================================
    # Enhanced Deep Research Methods (SSE Streaming & Events)
    # ============================================================

    async def process_with_streaming(self, context: ProcessingContext) -> AsyncGenerator[str, None]:
        # æ”¯æ´ SSE Streaming çš„è™•ç†æ–¹æ³•
        # Yields: SSE æ ¼å¼çš„äº‹ä»¶å­—ç¬¦ä¸²
        self._streaming_enabled = True

        # å•Ÿå‹•äº‹ä»¶è™•ç†å”ç¨‹
        event_task = asyncio.create_task(self._event_stream_handler())

        try:
            # ç™¼é€é–‹å§‹äº‹ä»¶
            await self._emit_event(ResearchEvent(
                type="progress",
                step="init",
                data={"status": "start", "query": context.request.query}
            ))

            # åŸ·è¡Œç ”ç©¶æµç¨‹
            result = await self.process(context)

            # ç™¼é€å®Œæˆäº‹ä»¶
            await self._emit_event(ResearchEvent(
                type="progress",
                step="complete",
                data={"status": "complete", "result_length": len(result)}
            ))

            # ç™¼é€æœ€çµ‚çµæœ
            yield f"data: {json.dumps({'type': 'final_report', 'data': result}, ensure_ascii=False)}\n\n"

        finally:
            # æ¸…ç†
            self._streaming_enabled = False
            await self.event_queue.put(None)  # çµæŸä¿¡è™Ÿ
            await event_task

    async def _event_stream_handler(self):
        # è™•ç†äº‹ä»¶æµ
        while True:
            event = await self.event_queue.get()
            if event is None:
                break

            # å¦‚æœæœ‰å›èª¿å‡½æ•¸ï¼Œèª¿ç”¨å®ƒ
            if self.event_callback:
                try:
                    await self._call_event_callback(event)
                except Exception as e:
                    self.logger.warning(f"Event callback error: {e}", "deep_research", "callback_error")

    async def _call_event_callback(self, event: ResearchEvent):
        # å®‰å…¨èª¿ç”¨äº‹ä»¶å›èª¿
        if asyncio.iscoroutinefunction(self.event_callback):
            await self.event_callback(event)
        else:
            self.event_callback(event)

    async def _emit_event(self, event: ResearchEvent):
        # ç™¼é€äº‹ä»¶
        if hasattr(self, 'event_queue'):
            await self.event_queue.put(event)

        # åŒæ™‚è¨˜éŒ„åˆ°æ—¥èªŒ
        self.logger.info(
            f"ğŸ“¡ Event: {event.type} - {event.step}",
            "deep_research",
            "event",
            event_type=event.type,
            event_step=event.step
        )

    # ============================================================
    # Multi-Search Engine Support
    # ============================================================

    async def _perform_deep_search_enhanced(self, query: str, goal: str) -> Dict:
        # å¢å¼·ç‰ˆæ·±åº¦æœç´¢ - æ”¯æ´å¤šæœç´¢å¼•æ“å’Œæ™ºèƒ½é™ç´š
        # ç™¼é€æœç´¢é–‹å§‹äº‹ä»¶
        await self._emit_event(ResearchEvent(
            type="progress",
            step="search",
            data={
                "status": "start",
                "query": query,
                "goal": goal,
                "provider": self.search_config.primary.value
            }
        ))

        # å˜—è©¦ä¸»è¦æœç´¢å¼•æ“
        search_result = await self._try_search_provider(
            self.search_config.primary,
            query,
            goal
        )

        # å¦‚æœä¸»è¦å¼•æ“å¤±æ•—ï¼Œå˜—è©¦å‚™ç”¨éˆ
        if not search_result or not search_result.get('sources'):
            for fallback_provider in self.search_config.fallback_chain:
                self.logger.info(
                    f"ğŸ”„ Switching to fallback: {fallback_provider.value}",
                    "deep_research",
                    "fallback"
                )

                await self._emit_event(ResearchEvent(
                    type="message",
                    step="search",
                    data={
                        "message": f"Switching to {fallback_provider.value}...",
                        "provider": fallback_provider.value
                    }
                ))

                search_result = await self._try_search_provider(
                    fallback_provider,
                    query,
                    goal
                )

                if search_result and search_result.get('sources'):
                    break

        # ç™¼é€æœç´¢çµæœäº‹ä»¶
        if search_result:
            await self._emit_event(ResearchEvent(
                type="search_result",
                step="search",
                data={
                    "query": query,
                    "sources_count": len(search_result.get('sources', [])),
                    "summary": search_result.get('summary', '')[:200]
                }
            ))

        return search_result or self._empty_search_result(query)

    async def _try_search_provider(self,
                                   provider: SearchProviderType,
                                   query: str,
                                   goal: str) -> Optional[Dict]:
        # å˜—è©¦ä½¿ç”¨ç‰¹å®šæœç´¢æä¾›å•†
        try:
            if provider == SearchProviderType.MODEL:
                # ä½¿ç”¨ AI æ¨¡å‹å…§å»ºæœç´¢
                return await self._model_based_search(query, goal)
            elif provider == SearchProviderType.EXA:
                # ä½¿ç”¨ Exa neural search
                return await self._exa_search(query, goal)

            # ä½¿ç”¨å…¶ä»–æœç´¢æœå‹™
            search_service = self.services.get("search")
            if search_service:
                # å¦‚æœæœå‹™æ”¯æ´è¨­ç½®æä¾›å•†
                if hasattr(search_service, 'set_provider'):
                    search_service.set_provider(provider.value.lower())

                results = await search_service.search(
                    query=query,
                    max_results=self.search_config.max_results
                )

                if results:
                    return self._format_search_results(results, provider.value)

        except Exception as e:
            self.logger.warning(
                f"Search error with {provider.value}: {e}",
                "deep_research",
                "search_error"
            )

        return None

    async def _exa_search(self, query: str, goal: str) -> Optional[Dict]:
        # ä½¿ç”¨ Exa API é€²è¡Œç¥ç¶“æœç´¢
        search_service = self.services.get("search")
        if not search_service:
            return None

        # åˆ¤æ–·æœç´¢é¡å‹
        search_type = "general"
        if "code" in goal.lower() or "programming" in goal.lower():
            search_type = "code"
        elif "research" in goal.lower() or "paper" in goal.lower():
            search_type = "research"
        elif "news" in goal.lower() or "latest" in goal.lower():
            search_type = "news"

        try:
            # ä½¿ç”¨æ•´åˆçš„æœç´¢æœå‹™ï¼ˆå·²åŒ…å« Exaï¼‰
            if hasattr(search_service, 'provider'):
                old_provider = search_service.provider
                search_service.provider = "exa"
                results = await search_service.search(
                    query=query,
                    max_results=self.search_config.max_results,
                    search_type=search_type
                )
                search_service.provider = old_provider

                if results:
                    return self._format_search_results(results, "exa")

        except Exception as e:
            self.logger.error(f"Exa search failed: {e}", "deep_research", "exa_error")

        return None

    async def _model_based_search(self, query: str, goal: str) -> Dict:
        # ä½¿ç”¨ AI æ¨¡å‹çš„å…§å»ºæœç´¢èƒ½åŠ›
        if not self.llm_client:
            return self._empty_search_result(query)

        search_prompt = (
            f"Please search and provide information about:\n"
            f"Query: {query}\n"
            f"Research Goal: {goal}\n\n"
            f"Provide a comprehensive answer based on your knowledge, formatted as:\n"
            f"1. Summary of findings\n"
            f"2. Key facts and details\n"
            f"3. Relevant context\n\n"
            f"Focus on accuracy and relevance."
        )

        try:
            response = await self._call_llm(search_prompt, None)

            return {
                'summary': response,
                'sources': [{
                    'title': 'AI Knowledge Base',
                    'url': 'model://knowledge',
                    'relevance': 0.8
                }],
                'relevance': 0.8,
                'timestamp': datetime.now().isoformat(),
                'provider': 'model'
            }
        except Exception as e:
            self.logger.error(f"Model search failed: {e}", "deep_research", "model_search_error")
            return self._empty_search_result(query)

    def _format_search_results(self, results: List, provider: str) -> Dict:
        # æ ¼å¼åŒ–æœç´¢çµæœ
        if not results:
            return None

        sources = []
        summary_parts = []

        for r in results[:self.search_config.max_results]:
            # é©é…ä¸åŒçš„çµæœæ ¼å¼
            if hasattr(r, 'url'):
                sources.append({
                    'url': r.url,
                    'title': getattr(r, 'title', 'Untitled'),
                    'relevance': getattr(r, 'score', 0.5)
                })
                summary_parts.append(f"- {r.title}: {getattr(r, 'snippet', '')[:100]}")
            elif isinstance(r, dict):
                sources.append({
                    'url': r.get('url', ''),
                    'title': r.get('title', 'Untitled'),
                    'relevance': r.get('score', 0.5)
                })
                summary_parts.append(f"- {r.get('title')}: {r.get('snippet', '')[:100]}")

        return {
            'summary': '\n'.join(summary_parts),
            'sources': sources,
            'relevance': sum(s['relevance'] for s in sources) / max(len(sources), 1),
            'timestamp': datetime.now().isoformat(),
            'provider': provider
        }

    def _empty_search_result(self, query: str) -> Dict:
        # è¿”å›ç©ºæœç´¢çµæœ
        return {
            'summary': f"[No search results available for: {query}]",
            'sources': [],
            'relevance': 0.0,
            'timestamp': datetime.now().isoformat(),
            'provider': 'none'
        }

    # Configuration methods
    def configure_search_engines(self, config: SearchEngineConfig):
        # å‹•æ…‹é…ç½®æœç´¢å¼•æ“
        self.search_config = config
        self.logger.info(
            f"Search engines configured: primary={config.primary.value}, "
            f"fallback={[p.value for p in config.fallback_chain]}",
            "deep_research",
            "config_update"
        )

    def enable_streaming(self, enabled: bool = True):
        # å•Ÿç”¨/ç¦ç”¨ streaming
        self._streaming_enabled = enabled

    def set_event_callback(self, callback: Callable[[ResearchEvent], None]):
        # è¨­ç½®äº‹ä»¶å›èª¿
        self.event_callback = callback

