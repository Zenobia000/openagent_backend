"""
Deep Research Processor - Agent-level research with multi-provider search

Comprehensive research processor with:
- Multi-iteration workflow with retry mechanism
- Multi-provider search engine support (Tavily, Exa, etc.)
- SSE streaming for real-time updates
- Academic-style reference formatting with citation tracking
- Critical analysis integration
- Event-driven architecture

Extracted from monolithic processor.py (1487 lines)
"""

import asyncio
import json
import re
from datetime import datetime
from typing import Dict, List, Optional, Any, Callable, AsyncGenerator
from collections import Counter

from ..base import BaseProcessor
from ...models_v2 import ProcessingContext
from ...prompts import PromptTemplates
from ...logger import structured_logger
from .config import SearchEngineConfig, SearchProviderType
from .events import ResearchEvent


class DeepResearchProcessor(BaseProcessor):
    """
    æ·±åº¦ç ”ç©¶è™•ç†å™¨ - Agent Level with Enhanced Features

    Features:
    - WorkflowState tracking and retry mechanism
    - SSE Streaming support
    - Multi-search engine configuration
    - Event-driven architecture
    - Closed-loop iteration (max 3 iterations)
    - Academic-style reference formatting
    """

    def __init__(self,
                 llm_client=None,
                 services: Optional[Dict[str, Any]] = None,
                 search_config: Optional[SearchEngineConfig] = None,
                 event_callback: Optional[Callable[[ResearchEvent], None]] = None,
                 mcp_client=None):
        """
        åˆå§‹åŒ–å¢å¼·ç‰ˆè™•ç†å™¨

        Args:
            llm_client: LLMå®¢æˆ¶ç«¯
            services: æœå‹™å­—å…¸
            search_config: æœç´¢å¼•æ“é…ç½®
            event_callback: äº‹ä»¶å›èª¿å‡½æ•¸
            mcp_client: MCP å®¢æˆ¶ç«¯ç®¡ç†å™¨
        """
        super().__init__(llm_client, services, mcp_client=mcp_client)
        self.search_config = search_config or SearchEngineConfig()
        self.event_callback = event_callback
        self.event_queue: asyncio.Queue = asyncio.Queue()

        self._streaming_enabled = False

    async def process(self, context: ProcessingContext) -> str:
        """åŸ·è¡Œå®Œæ•´çš„æ·±åº¦ç ”ç©¶æµç¨‹ - ç¬¦åˆ AgentRuntime è¦ç¯„"""

        # Step 1: Init Workflow (ç¬¦åˆç‹€æ…‹æ©Ÿ InitWorkflow)
        workflow_state = {
            "status": "running",
            "steps": ["plan", "search", "synthesize"],
            "current_step": None,
            "iterations": 0,
            "errors": []
        }
        context.response.metadata["workflow_state"] = workflow_state

        # è¨˜éŒ„æ·±åº¦ç ”ç©¶æ±ºç­–
        await self._log_tool_decision(
            "deep_research",
            "åŸ·è¡Œå…¨é¢çš„æ·±åº¦ç ”ç©¶ä»¥å›ç­”è¤‡é›œå•é¡Œ",
            0.95
        )

        try:
            # åŸ·è¡Œç ”ç©¶æµç¨‹ (åŒ…è£¹åœ¨ retry é‚è¼¯ä¸­)
            return await self._execute_with_retry(context, workflow_state)
        except Exception as e:
            # WorkflowFailed: è¨˜éŒ„å¤±æ•—ç‹€æ…‹
            workflow_state["status"] = "failed"
            workflow_state["errors"].append({
                "error": str(e),
                "step": workflow_state["current_step"],
                "timestamp": datetime.now().isoformat()
            })
            self.logger.error(f"Research workflow failed: {e}", "deep_research", "workflow_failed")
            raise

    async def _execute_with_retry(self, context: ProcessingContext, workflow_state: dict) -> str:
        """åŸ·è¡Œç ”ç©¶æµç¨‹ï¼Œæ”¯æ´é‡è©¦æ©Ÿåˆ¶ (ç¬¦åˆç‹€æ…‹æ©Ÿ RetryBoundary)"""
        from core.errors import ErrorClassifier

        MAX_RETRIES = 2
        retry_count = 0
        last_error = None

        while retry_count <= MAX_RETRIES:
            try:
                # åŸ·è¡Œæ ¸å¿ƒç ”ç©¶æµç¨‹
                return await self._execute_research_workflow(context, workflow_state)

            except Exception as e:
                # Error Classification (ç¬¦åˆç‹€æ…‹æ©Ÿ ErrorHandling)
                error_category = ErrorClassifier.classify(e)

                workflow_state["errors"].append({
                    "error": str(e),
                    "category": error_category,
                    "retry_count": retry_count,
                    "step": workflow_state["current_step"]
                })

                if error_category in ["NETWORK", "LLM"] and retry_count < MAX_RETRIES:
                    # Retryable error - æŒ‡æ•¸é€€é¿é‡è©¦
                    retry_count += 1
                    delay = 2 ** retry_count  # Exponential backoff
                    self.logger.warning(
                        f"Retryable error ({error_category}), retrying {retry_count}/{MAX_RETRIES} after {delay}s",
                        "deep_research", "retry"
                    )
                    await asyncio.sleep(delay)
                    last_error = e
                else:
                    # Non-retryable or max retries exceeded
                    raise e

        # å¦‚æœæ‰€æœ‰é‡è©¦éƒ½å¤±æ•—
        if last_error:
            raise last_error

    async def _execute_research_workflow(self, context: ProcessingContext, workflow_state: dict) -> str:
        """Execute core research workflow with progressive synthesis."""

        # NOTE: clarification is disabled â€” awaiting SSE interactive implementation.
        # When ready, re-enable _should_clarify / _ask_clarifying_questions here.

        # 1. Report plan (WriteReportPlan)
        workflow_state["current_step"] = "plan"
        report_plan = await self._write_report_plan(context)

        # 1.5. Domain identification (drives multi-domain query distribution)
        workflow_state["current_step"] = "domain_identification"
        research_domains = await self._identify_research_domains(context, report_plan)

        # Research iteration loop with progressive synthesis
        MAX_ITERATIONS = 3
        all_search_results = []
        executed_queries: List[str] = []
        accumulated_synthesis = None
        iteration = 0

        while iteration < MAX_ITERATIONS:
            iteration += 1
            workflow_state["iterations"] = iteration
            self.logger.info(f"Research Iteration {iteration}/{MAX_ITERATIONS}", "deep_research", "iteration")

            # 2. Generate search queries
            workflow_state["current_step"] = "search"
            if iteration == 1:
                search_tasks = await self._generate_serp_queries(
                    context, report_plan, domains=research_domains
                )
            else:
                search_tasks = await self._generate_followup_queries(
                    context, report_plan, all_search_results,
                    executed_queries=executed_queries,
                )

            if not search_tasks:
                break

            # 3. Execute search tasks
            search_results = await self._execute_search_tasks(context, search_tasks)
            all_search_results.extend(search_results)
            executed_queries.extend(t.get('query', '') for t in search_tasks)

            # 4. Progressive intermediate synthesis
            workflow_state["current_step"] = "synthesis"
            synthesis_result = await self._intermediate_synthesis(
                context, report_plan, search_results, accumulated_synthesis,
            )
            accumulated_synthesis = synthesis_result.get("synthesis", "")
            section_coverage = synthesis_result.get("section_coverage", {})

            # 5. Structured completeness review
            is_sufficient, gap_report = await self._review_research_completeness(
                context, report_plan, all_search_results, iteration,
                section_coverage=section_coverage,
            )

            if is_sufficient:
                self.logger.info("Research is sufficient, proceeding to report", "deep_research", "complete")
                break

            # Budget exhaustion check
            if len(executed_queries) >= self.search_config.max_total_queries:
                self.logger.info(
                    f"Search budget exhausted ({len(executed_queries)}/{self.search_config.max_total_queries})",
                    "deep_research", "budget_exhausted"
                )
                break

            self.logger.info("Research needs more depth, continuing...", "deep_research", "continue")

        # Reversible compression: save full research data to file,
        # use accumulated_synthesis (LLM-condensed) for downstream prompts
        research_data_path = self._save_research_data(context, all_search_results)
        synthesis = accumulated_synthesis or self._summarize_search_results(all_search_results)

        # 6. Critical analysis (always-on for deep research)
        workflow_state["current_step"] = "critical_analysis"
        critical_analysis = await self._critical_analysis_stage(
            context, all_search_results, report_plan,
            synthesis=synthesis,
        )

        # 7. Chart planning (always-on) + execution (requires sandbox)
        chart_specs = await self._plan_report_charts(
            context, all_search_results, report_plan,
            synthesis=synthesis,
        )

        computational_result = None
        if chart_specs and self.services.get("sandbox"):
            workflow_state["current_step"] = "computational_analysis"
            computational_result = await self._execute_chart_plan(
                context, chart_specs, all_search_results,
                synthesis=synthesis,
            )

        # 8. Final report generation
        workflow_state["current_step"] = "synthesize"
        final_report = await self._write_final_report(
            context, all_search_results, report_plan,
            critical_analysis, computational_result,
            synthesis=synthesis,
        )

        # WorkflowComplete: æ¨™è¨˜æˆåŠŸå®Œæˆ
        workflow_state["status"] = "completed"
        self.logger.info("âœ… Research workflow completed successfully", "deep_research", "workflow_complete")

        return final_report

    async def _should_clarify(self, context: ProcessingContext) -> bool:
        """åˆ¤æ–·æ˜¯å¦éœ€è¦æ¾„æ¸…ç ”ç©¶æ–¹å‘"""
        # åŸºæ–¼æŸ¥è©¢è¤‡é›œåº¦åˆ¤æ–·
        complexity_indicators = ['æ¯”è¼ƒ', 'åˆ†æ', 'è©•ä¼°', 'æ·±åº¦', 'å…¨é¢', 'è©³ç´°', 'å°æ¯”']
        query_lower = context.request.query.lower()
        return any(indicator in query_lower for indicator in complexity_indicators)

    async def _ask_clarifying_questions(self, context: ProcessingContext):
        """è©¢å•æ¾„æ¸…å•é¡Œä»¥æ›´å¥½ç†è§£ç ”ç©¶éœ€æ±‚"""
        self.logger.progress("clarification", "start")

        question_prompt = PromptTemplates.get_system_question_prompt(context.request.query)
        questions = await self._call_llm(question_prompt, context)

        self.logger.info(
            f"â“ Clarifying Questions Generated:\n{questions}",
            "deep_research",
            "clarification"
        )

        # é€™è£¡å¯ä»¥å¯¦éš›ç™¼é€çµ¦ç”¨æˆ¶ä¸¦ç²å–å›æ‡‰
        # ç›®å‰å…ˆè¨˜éŒ„ä¾›åƒè€ƒ
        context.response.metadata["clarifying_questions"] = questions

        self.logger.progress("clarification", "end")

    async def _identify_research_domains(self, context: ProcessingContext,
                                          report_plan: str) -> List[Dict[str, Any]]:
        """Identify research domains and search angles for multi-domain coverage."""

        self.logger.progress("domain-identification", "start")

        prompt = PromptTemplates.get_domain_identification_prompt(
            query=context.request.query,
            report_plan=report_plan,
        )

        response = await self._call_llm(prompt, context)

        try:
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response)
            if json_match:
                result = json.loads(json_match.group(1))
            else:
                result = json.loads(response)
            domains = result.get("domains", [])
        except (json.JSONDecodeError, KeyError, TypeError):
            # Fallback: empty domains â€” SERP generation proceeds without domain guidance
            domains = []

        self.logger.info(
            f"Identified {len(domains)} research domains",
            "deep_research", "domains",
            domains=[d.get("name") for d in domains]
        )

        context.response.metadata["research_domains"] = [d.get("name") for d in domains]
        self.logger.progress("domain-identification", "end")

        return domains

    async def _intermediate_synthesis(self, context: ProcessingContext,
                                      report_plan: str,
                                      wave_results: List[Dict],
                                      previous_synthesis: Optional[str] = None) -> Dict[str, Any]:
        """Progressive synthesis â€” integrate new findings with prior understanding."""

        self.logger.progress("intermediate-synthesis", "start")

        wave_summary = self._summarize_search_results(wave_results)
        prompt = PromptTemplates.get_intermediate_synthesis_prompt(
            query=context.request.query,
            report_plan=report_plan,
            wave_results=wave_summary,
            previous_synthesis=previous_synthesis,
        )

        response = await self._call_llm(prompt, context)

        # Parse JSON
        try:
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response)
            if json_match:
                result = json.loads(json_match.group(1))
            else:
                result = json.loads(response)
        except (json.JSONDecodeError, TypeError):
            # Fallback: treat entire response as synthesis text
            result = {
                "synthesis": response,
                "section_coverage": {},
                "knowledge_gaps": [],
                "cross_domain_links": [],
            }

        # Ensure required keys exist
        result.setdefault("synthesis", "")
        result.setdefault("section_coverage", {})
        result.setdefault("knowledge_gaps", [])
        result.setdefault("cross_domain_links", [])

        context.response.metadata.setdefault("synthesis_history", [])
        context.response.metadata["synthesis_history"].append(result.get("synthesis", "")[:500])

        self.logger.progress("intermediate-synthesis", "end")
        return result

    async def _critical_analysis_stage(self, context: ProcessingContext,
                                     search_results: List[Dict],
                                     report_plan: str,
                                     synthesis: str = None) -> str:
        """æ‰¹åˆ¤æ€§åˆ†æéšæ®µ - å€Ÿé‘’ ThinkingProcessor çš„èƒ½åŠ›"""

        self.logger.progress("critical-analysis", "start")
        self.logger.info(
            f"ğŸ§  Critical Analysis: Analyzing research findings from multiple perspectives",
            "deep_research",
            "critical_analysis",
            phase="critical-analysis"
        )

        # Use accumulated synthesis (bounded) instead of raw search results
        research_summary = synthesis or self._summarize_search_results(search_results)

        # å€Ÿç”¨ ThinkingProcessor çš„æ‰¹åˆ¤æ€§æ€ç¶­æç¤ºè©
        critical_prompt = PromptTemplates.get_critical_thinking_prompt(
            question=context.request.query,
            context=f"Research Plan:\n{report_plan}\n\nResearch Findings:\n{research_summary}"
        )

        # åŸ·è¡Œæ‰¹åˆ¤æ€§åˆ†æ
        self.logger.reasoning("é€²è¡Œæ‰¹åˆ¤æ€§åˆ†æå’Œå¤šè§’åº¦æ€è€ƒ...", streaming=True)
        critical_analysis = await self._call_llm(critical_prompt, context)

        # è¨˜éŒ„åˆ†æçµæœåˆ°æ—¥èªŒ
        self.logger.info(
            f"ğŸ’­ Critical Analysis Result: {critical_analysis[:300]}...",
            "deep_research",
            "critical_analysis_result",
            full_length=len(critical_analysis)
        )

        # å„²å­˜åˆ°ä¸­é–“çµæœ
        context.response.metadata["critical_analysis"] = critical_analysis

        self.logger.progress("critical-analysis", "end")
        return critical_analysis

    # ============================================================
    # Chart Planning Phase (always-on)
    # ============================================================

    async def _plan_report_charts(self, context: ProcessingContext,
                                   search_results: List[Dict],
                                   report_plan: str,
                                   synthesis: str = None) -> List[Dict]:
        """Plan specific charts for the report â€” always runs, no sandbox gate."""
        self.logger.info("Planning report charts...", "deep_research", "chart_plan")

        research_summary = synthesis or self._summarize_search_results(search_results)
        prompt = PromptTemplates.get_chart_planning_prompt(
            query=context.request.query,
            research_summary=research_summary,
            report_plan=report_plan,
        )

        try:
            response = await self._call_llm(prompt, context)
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response)
            if json_match:
                result = json.loads(json_match.group(1))
            else:
                result = json.loads(response)
            charts = result.get("charts", [])
            self.logger.info(
                f"Planned {len(charts)} charts",
                "deep_research", "chart_plan_done"
            )
            return charts[:5]
        except Exception as e:
            self.logger.warning(f"Chart planning failed: {e}", "deep_research", "chart_plan_fail")
            return []

    # ============================================================
    # Computational Analysis Phase
    # ============================================================

    async def _requires_computational_analysis(self, context: ProcessingContext,
                                                search_results: List[Dict]) -> bool:
        """Determine if computational analysis would add value â€” LLM decides dynamically."""

        # Hard gate: sandbox must be available
        sandbox_service = self.services.get("sandbox")
        if not sandbox_service:
            return False

        # LLM triage: ask whether the data warrants computation
        research_summary = self._summarize_search_results(search_results)
        triage_prompt = PromptTemplates.get_computational_triage_prompt(
            query=context.request.query,
            research_summary=research_summary
        )

        try:
            response = await self._call_llm(triage_prompt, context)
            return response.strip().upper().startswith("YES")
        except Exception:
            return False

    async def _computational_analysis_stage(self, context: ProcessingContext,
                                             search_results: List[Dict],
                                             report_plan: str) -> Optional[Dict[str, Any]]:
        """Computational analysis â€” generate Python code, execute in sandbox, return results."""

        self.logger.progress("computational-analysis", "start")
        self.logger.info(
            "Computational Analysis: Generating analysis code from research data",
            "deep_research", "computational_analysis"
        )

        # Step 1: Generate analysis code
        self.logger.reasoning("Generating computational analysis code...", streaming=True)
        code = await self._generate_analysis_code(context, search_results, report_plan)

        if not code:
            self.logger.info(
                "Computational analysis: no viable code generated, skipping",
                "deep_research", "compute_skip"
            )
            self.logger.progress("computational-analysis", "end")
            return None

        # Step 2: Execute the code
        self.logger.reasoning("Executing computational analysis in sandbox...", streaming=True)
        result = await self._execute_analysis_code(code)

        if not result:
            self.logger.info(
                "Computational analysis: execution failed, continuing without computation",
                "deep_research", "compute_failed"
            )
            self.logger.progress("computational-analysis", "end")
            return None

        # Store in context metadata
        figure_count = len(result.get("figures", []))
        context.response.metadata["computational_analysis"] = {
            "code": result["code"],
            "stdout": result["stdout"][:500],
            "figure_count": figure_count,
            "execution_time": result["execution_time"]
        }

        self.logger.info(
            f"Computational analysis complete: {figure_count} figures, "
            f"{len(result.get('stdout', ''))} chars output, "
            f"{result.get('execution_time', 0):.2f}s",
            "deep_research", "compute_complete"
        )
        self.logger.progress("computational-analysis", "end")
        return result

    async def _execute_chart_plan(self, context: ProcessingContext,
                                   chart_specs: List[Dict],
                                   search_results: List[Dict],
                                   synthesis: str = None) -> Optional[Dict[str, Any]]:
        """Execute chart plan: generate and run code for each chart individually."""
        self.logger.progress("computational-analysis", "start")

        research_summary = synthesis or self._summarize_search_results(search_results)
        all_figures = []
        all_stdout = []
        total_time = 0.0

        for i, spec in enumerate(chart_specs):
            self.logger.info(
                f"Generating chart {i+1}/{len(chart_specs)}: {spec.get('title', '?')}",
                "deep_research", "chart_gen"
            )
            try:
                prompt = PromptTemplates.get_single_chart_code_prompt(spec, research_summary)
                response = await self._call_llm(prompt, context)
                code = self._extract_code_block(response)
                if not code:
                    continue

                result = await self._execute_analysis_code(code)
                if result and result.get("figures"):
                    for fig in result["figures"]:
                        all_figures.append({"base64": fig, "spec": spec})
                    all_stdout.append(result.get("stdout", ""))
                    total_time += result.get("execution_time", 0)
            except Exception as e:
                self.logger.warning(
                    f"Chart {i+1} failed: {e}", "deep_research", "chart_fail"
                )
                continue

        if not all_figures:
            self.logger.info(
                "No charts generated successfully", "deep_research", "no_charts"
            )
            self.logger.progress("computational-analysis", "end")
            return None

        combined = {
            "figures": [f["base64"] for f in all_figures],
            "figure_specs": [f["spec"] for f in all_figures],
            "stdout": "\n".join(all_stdout),
            "code": f"# {len(all_figures)} charts generated individually",
            "execution_time": total_time,
        }

        context.response.metadata["computational_analysis"] = {
            "figure_count": len(all_figures),
            "execution_time": total_time,
            "chart_titles": [f["spec"].get("title", "") for f in all_figures],
        }

        self.logger.info(
            f"Chart plan complete: {len(all_figures)} figures in {total_time:.2f}s",
            "deep_research", "chart_plan_complete"
        )
        self.logger.progress("computational-analysis", "end")
        return combined

    async def _generate_analysis_code(self, context: ProcessingContext,
                                       search_results: List[Dict],
                                       report_plan: str) -> Optional[str]:
        """Ask LLM to generate Python code for computational analysis."""

        research_summary = self._summarize_search_results(search_results)
        prompt = PromptTemplates.get_computational_analysis_prompt(
            query=context.request.query,
            research_summary=research_summary,
            report_plan=report_plan
        )

        response = await self._call_llm(prompt, context)
        code = self._extract_code_block(response)
        if not code:
            self.logger.warning(
                "LLM did not produce a valid code block for analysis",
                "deep_research", "compute_no_code"
            )
        return code

    def _extract_code_block(self, response: str) -> Optional[str]:
        """Extract Python code from markdown fenced code block."""
        import re
        match = re.search(r'```(?:python)?\s*\n(.*?)\n```', response, re.DOTALL)
        if match:
            return match.group(1).strip()
        # Fallback: if entire response looks like bare code
        stripped = response.strip()
        if stripped.startswith(('import ', 'from ', '# ', 'def ', 'class ')):
            return stripped
        return None

    async def _execute_analysis_code(self, code: str, retry: bool = True) -> Optional[Dict[str, Any]]:
        """Execute analysis code in sandbox. Retry once on failure with error feedback."""

        sandbox_service = self.services.get("sandbox")
        if not sandbox_service:
            return None

        try:
            result = await sandbox_service.execute("execute_python", {
                "code": code,
                "timeout": 30
            })

            if result.get("success"):
                return {
                    "stdout": result.get("stdout", ""),
                    "figures": result.get("figures", []),
                    "return_value": result.get("return_value"),
                    "code": code,
                    "execution_time": result.get("execution_time", 0)
                }

            error_msg = result.get("error", "Unknown error")
            self.logger.warning(
                f"Computational analysis code failed: {error_msg}",
                "deep_research", "compute_exec_fail"
            )

            if retry:
                fixed_code = await self._fix_analysis_code(code, error_msg)
                if fixed_code:
                    return await self._execute_analysis_code(fixed_code, retry=False)

            return None

        except Exception as e:
            self.logger.warning(
                f"Sandbox execution exception: {e}",
                "deep_research", "compute_exception"
            )
            return None

    async def _fix_analysis_code(self, original_code: str, error: str) -> Optional[str]:
        """Ask LLM to fix failed analysis code."""

        prompt = f"""The following Python code failed with an error. Fix it and return ONLY the corrected code in a ```python code block.

Original code:
```python
{original_code}
```

Error:
{error}

Rules:
- Fix the error while preserving the original intent
- Use only: numpy, scipy, sympy, pandas, matplotlib, seaborn, plotly, sklearn
- Store final results in a variable named `result`
- Use print() for key findings"""

        response = await self._call_llm(prompt, None)
        return self._extract_code_block(response)

    async def _enrich_with_full_content(self, search_result: Dict, top_n: int = None) -> Dict:
        """Fetch full page content for top search result URLs.

        Uses search_service.fetch_multiple() to grab full text from the
        highest-relevance URLs, then stores the combined content in
        search_result['full_content']. Fails silently â€” if anything goes
        wrong the original result is returned as-is.
        """
        if top_n is None:
            top_n = self.search_config.urls_per_query
        search_service = self.services.get("search")
        if not search_service or not hasattr(search_service, 'fetch_multiple'):
            return search_result

        sources = search_result.get('sources', [])
        if not sources:
            return search_result

        # Pick top N by relevance (already sorted in most code paths)
        top_sources = sorted(sources, key=lambda s: s.get('relevance', 0), reverse=True)[:top_n]
        urls = [s['url'] for s in top_sources if s.get('url')]

        if not urls:
            return search_result

        try:
            content_map = await search_service.fetch_multiple(urls)
            if content_map:
                full_texts = []
                for url in urls:
                    text = content_map.get(url)
                    if text:
                        full_texts.append(text)
                if full_texts:
                    search_result['full_content'] = "\n\n---\n\n".join(full_texts)
        except Exception as e:
            self.logger.warning(
                f"Full-content extraction failed: {e}",
                "deep_research", "fetch_content_error"
            )

        return search_result

    def _save_research_data(self, context: ProcessingContext,
                             search_results: List[Dict]) -> Optional[str]:
        """Reversible compression: save full search results to file.

        Full raw data is preserved on disk. Downstream stages use the
        LLM-condensed accumulated_synthesis instead, keeping prompts bounded.
        """
        try:
            from pathlib import Path

            trace_id = context.request.trace_id
            data_dir = Path(self.logger.log_dir) / "research_data"
            data_dir.mkdir(parents=True, exist_ok=True)

            filepath = data_dir / f"{trace_id[:8]}_search_results.json"

            # Extract serializable subset (skip raw HTML etc.)
            serializable = []
            for r in search_results:
                inner = r.get('result', {}) if isinstance(r.get('result'), dict) else {}
                serializable.append({
                    "query": r.get("query", ""),
                    "goal": r.get("goal", ""),
                    "priority": r.get("priority", 1),
                    "summary": inner.get("summary", ""),
                    "processed": inner.get("processed", ""),
                    "sources": inner.get("sources", []),
                })

            filepath.write_text(
                json.dumps(serializable, ensure_ascii=False, indent=2),
                encoding="utf-8",
            )
            self.logger.info(
                f"Research data saved: {filepath} ({len(search_results)} results)",
                "deep_research", "data_saved",
            )
            return str(filepath)
        except Exception as e:
            self.logger.warning(f"Failed to save research data: {e}", "deep_research", "data_save_error")
            return None

    def _summarize_search_results(self, search_results: List[Dict],
                                   max_per_result: int = 8000,
                                   max_total: int = 200000) -> str:
        """Summarize search results â€” prefer full_content over snippets.

        Truncates per-result and total output to stay within LLM context limits.
        """

        summaries = []
        total_chars = 0
        for i, result in enumerate(search_results, 1):
            query = result.get('query', 'Unknown')
            # Prefer full_content (from fetch) â†’ result.full_content â†’ result.results
            inner = result.get('result', {}) if isinstance(result.get('result'), dict) else {}
            content = (
                inner.get('full_content')
                or result.get('results', '')
                or inner.get('processed', '')
                or inner.get('summary', '')
            )
            if isinstance(content, str) and len(content) > max_per_result:
                content = content[:max_per_result] + "... [truncated]"
            entry = f"Search {i} - Query: {query}\nFindings: {content}"
            total_chars += len(entry)
            if total_chars > max_total:
                summaries.append(f"... [{len(search_results) - i} more results truncated for context limit]")
                break
            summaries.append(entry)

        return "\n\n".join(summaries)

    async def _generate_followup_queries(self, context: ProcessingContext,
                                        report_plan: str,
                                        existing_results: List[Dict],
                                        executed_queries: List[str] = None) -> List[Dict]:
        """Generate follow-up queries to fill research gaps â€” budget-aware."""
        # Calculate remaining budget
        used = len(executed_queries) if executed_queries else 0
        remaining = max(0, self.search_config.max_total_queries - used)
        budget = min(self.search_config.queries_followup_iteration, remaining)

        if budget <= 0:
            self.logger.info("Search budget exhausted, skipping follow-up", "deep_research", "budget")
            return []

        self.logger.progress("followup-query", "start")

        learnings = self._prepare_report_context(existing_results)

        output_schema = {
            "type": "array",
            "maxItems": budget,
            "items": {
                "type": "object",
                "properties": {
                    "query": {"type": "string"},
                    "researchGoal": {"type": "string"},
                    "priority": {"type": "number"}
                }
            }
        }

        # Dedup notice
        dedup_suggestion = "Focus on filling knowledge gaps and getting more specific details"
        if executed_queries:
            queries_list = "\n".join(f"- {q}" for q in executed_queries)
            dedup_suggestion += (
                f"\n\nIMPORTANT: The following queries have already been executed. "
                f"Do NOT generate similar or duplicate queries:\n{queries_list}"
            )

        review_prompt = PromptTemplates.get_review_prompt(
            plan=report_plan,
            learnings=learnings,
            suggestion=dedup_suggestion,
            output_schema=output_schema,
            remaining_budget=budget,
        )

        response = await self._call_llm(review_prompt, context)

        try:
            import re
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response)
            if json_match:
                queries = json.loads(json_match.group(1))
            else:
                queries = json.loads(response)
        except:
            queries = []

        # Safety net: enforce budget
        if len(queries) > budget:
            queries = sorted(queries, key=lambda q: q.get("priority", 0), reverse=True)[:budget]

        self.logger.info(
            f"ğŸ“‹ Follow-up Queries: {len(queries)} queries (budget: {budget}, total used: {used})",
            "deep_research",
            "followup"
        )

        self.logger.progress("followup-query", "end")
        return queries

    async def _review_research_completeness(self, context: ProcessingContext,
                                           report_plan: str,
                                           search_results: List[Dict],
                                           iteration: int,
                                           section_coverage: Optional[Dict] = None) -> tuple:
        """Evaluate research completeness â€” returns (is_sufficient, gap_report)."""
        self.logger.progress("review", "start")

        review_prompt = PromptTemplates.get_completeness_review_prompt(
            report_plan=report_plan,
            section_coverage=section_coverage or {},
            iteration=iteration,
            max_iterations=3,
        )

        response = await self._call_llm(review_prompt, context)

        # Parse structured JSON response
        try:
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response)
            if json_match:
                result = json.loads(json_match.group(1))
            else:
                result = json.loads(response)

            is_sufficient = bool(result.get("is_sufficient", False))
            gap_report = {
                "overall_coverage": result.get("overall_coverage", 0),
                "sections": result.get("sections", []),
                "priority_gaps": result.get("priority_gaps", []),
            }
        except (json.JSONDecodeError, KeyError, TypeError):
            # Fallback: simple YES/NO parsing for backward compatibility
            is_sufficient = "YES" in response.upper()[:10]
            gap_report = {"overall_coverage": 0, "sections": [], "priority_gaps": []}

        self.logger.info(
            f"Research Completeness: {'Sufficient' if is_sufficient else 'Needs more'}",
            "deep_research", "review",
            iteration=iteration, is_sufficient=is_sufficient
        )

        self.logger.progress("review", "end", {"is_sufficient": is_sufficient})

        return is_sufficient, gap_report

    async def _write_report_plan(self, context: ProcessingContext) -> str:
        """Phase 1: ç”Ÿæˆç ”ç©¶å ±å‘Šè¨ˆç•«"""
        self.logger.progress("report-plan", "start")

        # è¨˜éŒ„è¨ˆåŠƒéšæ®µ
        self.logger.info(
            f"ğŸ“ Planning: Creating research plan for '{context.request.query[:50]}...'",
            "deep_research",
            "planning",
            phase="report-plan",
            query_length=len(context.request.query)
        )

        # ä½¿ç”¨å ±å‘Šè¨ˆåŠƒ prompt
        plan_prompt = PromptTemplates.get_report_plan_prompt(context.request.query)

        # æ¨ç†éç¨‹
        self.logger.reasoning("é–‹å§‹åˆ†æç ”ç©¶éœ€æ±‚...", streaming=True)
        plan = await self._call_llm(plan_prompt, context)

        # è¨˜éŒ„è¨ˆåŠƒåˆ°æ—¥èªŒ
        self.logger.info(
            f"ğŸ“‹ Research plan created: {plan[:300]}...",
            "deep_research",
            "plan_result",
            plan_length=len(plan)
        )

        self.logger.progress("report-plan", "end", {"plan": plan[:200]})

        return plan

    async def _generate_serp_queries(self, context: ProcessingContext, plan: str,
                                      domains: Optional[List[Dict]] = None) -> List[Dict]:
        """Phase 2: ç”Ÿæˆ SERP æŸ¥è©¢ â€” domain-aware when domains are provided."""
        self.logger.progress("serp-query", "start")

        # è¨˜éŒ„æŸ¥è©¢ç”Ÿæˆ
        self.logger.info(
            f"ğŸ” SERP Generation: Extracting search queries from plan",
            "deep_research",
            "serp",
            phase="serp-query"
        )

        # Budget from config
        budget = self.search_config.queries_first_iteration

        output_schema = {
            "type": "array",
            "maxItems": budget,
            "items": {
                "type": "object",
                "properties": {
                    "query": {"type": "string"},
                    "researchGoal": {"type": "string"},
                    "priority": {"type": "number"}
                }
            }
        }

        # Build domain-aware plan supplement
        domain_supplement = ""
        if domains:
            domain_lines = []
            for d in domains:
                angles = ", ".join(d.get("search_angles", []))
                domain_lines.append(f"- {d['name']} (weight {d.get('weight', 0):.1f}): {angles}")
            domain_supplement = (
                "\n\nResearch Domains (ensure queries cover ALL domains proportionally):\n"
                + "\n".join(domain_lines)
            )

        # Budget-aware SERP prompt
        serp_prompt = PromptTemplates.get_serp_queries_prompt(
            plan + domain_supplement, output_schema, query_budget=budget
        )
        response = await self._call_llm(serp_prompt, context)

        # Parse queries
        try:
            import re
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response)
            if json_match:
                queries = json.loads(json_match.group(1))
            else:
                queries = json.loads(response)
        except:
            queries = [{"query": context.request.query, "researchGoal": "General research", "priority": 1}]

        # Safety net: if LLM ignores budget, keep top N by priority
        if len(queries) > budget:
            queries = sorted(queries, key=lambda q: q.get("priority", 0), reverse=True)[:budget]

        # è¨˜éŒ„ç”Ÿæˆçš„æŸ¥è©¢
        self.logger.info(
            f"ğŸ“‹ SERP Queries: Generated {len(queries)} search queries",
            "deep_research",
            "serp",
            queries_count=len(queries),
            queries=queries[:3]  # åªè¨˜éŒ„å‰3å€‹
        )

        self.logger.progress("serp-query", "end", {"queries": queries})

        return queries

    async def _execute_search_tasks(self, context: ProcessingContext, search_tasks: List[Dict]) -> List[Dict]:
        """Phase 3: åŸ·è¡Œæœç´¢ä»»å‹™ - æ”¯æ´æ‰¹æ¬¡å¹³è¡Œæœç´¢"""
        self.logger.progress("task-list", "start")

        # è¨˜éŒ„ä»»å‹™åˆ—è¡¨
        self.logger.info(
            f"ğŸ“‹ Task List: Executing {len(search_tasks)} search tasks (parallel batch size: {self.search_config.parallel_searches})",
            "deep_research",
            "tasks",
            phase="task-list",
            total_tasks=len(search_tasks),
            parallel_batch_size=self.search_config.parallel_searches
        )

        results = []

        # å°‡ä»»å‹™åˆ†æ‰¹åŸ·è¡Œ
        batch_size = self.search_config.parallel_searches

        for batch_start in range(0, len(search_tasks), batch_size):
            batch_end = min(batch_start + batch_size, len(search_tasks))
            batch_tasks = search_tasks[batch_start:batch_end]

            self.logger.info(
                f"ğŸš€ Executing batch {batch_start//batch_size + 1}: Tasks {batch_start+1}-{batch_end}",
                "deep_research",
                "batch_execution",
                batch_index=batch_start//batch_size + 1,
                batch_size=len(batch_tasks)
            )

            # æº–å‚™æ‰¹æ¬¡æœç´¢ä»»å‹™
            async_tasks = []
            for i, task in enumerate(batch_tasks, batch_start + 1):
                query = task.get('query', '')
                goal = task.get('researchGoal', '')
                priority = task.get('priority', 1)

                # è¨˜éŒ„æœç´¢ä»»å‹™
                self.logger.info(
                    f"ğŸ” Search Task {i}/{len(search_tasks)}: {query}",
                    "deep_research",
                    "search_task",
                    task_index=i,
                    query=query,
                    goal=goal,
                    priority=priority,
                    provider=self.search_config.primary.value
                )

                # æ·»åŠ åˆ°ç•°æ­¥ä»»å‹™åˆ—è¡¨
                async_tasks.append(
                    self._execute_single_search_task(i, task, query, goal, priority)
                )

            # å¹³è¡ŒåŸ·è¡Œæ‰¹æ¬¡æœç´¢
            batch_results = await asyncio.gather(*async_tasks, return_exceptions=True)

            # è™•ç†æ‰¹æ¬¡çµæœ
            for task, result in zip(batch_tasks, batch_results):
                if isinstance(result, Exception):
                    self.logger.error(
                        f"âŒ Search task failed: {str(result)}",
                        "deep_research",
                        "search_error",
                        error=str(result)
                    )
                    # å‰µå»ºéŒ¯èª¤çµæœ
                    result = {
                        'query': task.get('query', ''),
                        'goal': task.get('researchGoal', ''),
                        'priority': task.get('priority', 1),
                        'result': {
                            'error': str(result),
                            'sources': [],
                            'summary': f"Search failed: {str(result)}"
                        }
                    }

                results.append(result)

        self.logger.progress("task-list", "end")

        # è¨˜éŒ„ç¸½çµ
        total_sources = sum(len(r.get('result', {}).get('sources', [])) for r in results)
        self.logger.info(
            f"ğŸ“Š Search Summary: {total_sources} total sources from {len(search_tasks)} tasks",
            "deep_research",
            "summary",
            phase="search-complete",
            total_sources=total_sources,
            total_tasks=len(search_tasks)
        )

        return results

    async def _execute_single_search_task(self, index: int, task: Dict, query: str, goal: str, priority: int) -> Dict:
        """åŸ·è¡Œå–®å€‹æœç´¢ä»»å‹™"""
        try:
            # é–‹å§‹å–®å€‹æœç´¢ä»»å‹™
            self.logger.progress("search-task", "start", {"name": query})

            # æ¨ç†éç¨‹
            self.logger.reasoning(f"æ­£åœ¨æœç´¢ï¼š{query}...", streaming=True)

            # åŸ·è¡Œæœç´¢ï¼ˆæ”¯æ´å¤šå¼•æ“å¹³è¡Œæœç´¢ï¼‰
            search_result = await self._perform_parallel_deep_search(query, goal)

            # Enrich with full page content
            search_result = await self._enrich_with_full_content(search_result)

            # è¨˜éŒ„æœç´¢çµæœ
            self.logger.info(
                f"âœ… Search Result {index}: Found {len(search_result.get('sources', []))} sources",
                "deep_research",
                "search_result",
                task_index=index,
                sources_count=len(search_result.get('sources', [])),
                relevance_score=search_result.get('relevance', 0)
            )

            # æ¶ˆæ¯è¼¸å‡º
            self.logger.message(f"æœç´¢ {index}: {query}\nçµæœ: {search_result.get('summary', '')[:200]}...")

            # çµæŸå–®å€‹æœç´¢ä»»å‹™
            self.logger.progress("search-task", "end", {
                "name": query,
                "data": search_result
            })

            return {
                'query': query,
                'goal': goal,
                'priority': priority,
                'result': search_result
            }
        except Exception as e:
            self.logger.error(
                f"Error in search task: {str(e)}",
                "deep_research",
                "task_error"
            )
            raise

    async def _perform_parallel_deep_search(self, query: str, goal: str) -> Dict:
        """åŸ·è¡Œå¹³è¡Œæ·±åº¦æœç´¢ - åŒæ™‚ä½¿ç”¨å¤šå€‹æœç´¢å¼•æ“"""

        # å¦‚æœå•Ÿç”¨äº† race æ¨¡å¼ï¼ŒåŒæ™‚å•Ÿå‹•æ‰€æœ‰æœç´¢å¼•æ“
        if hasattr(self.search_config, 'enable_race_mode') and self.search_config.enable_race_mode:
            return await self._perform_race_search(query, goal)

        # å¦å‰‡ä½¿ç”¨å¢å¼·ç‰ˆæœç´¢ï¼ˆå¸¶é™ç´šï¼‰
        return await self._perform_deep_search_enhanced(query, goal)

    async def _perform_race_search(self, query: str, goal: str) -> Dict:
        """ç«¶é€Ÿæ¨¡å¼ï¼šåŒæ™‚åŸ·è¡Œå¤šå€‹æœç´¢å¼•æ“ï¼Œè¿”å›ç¬¬ä¸€å€‹æˆåŠŸçš„çµæœ"""

        # æº–å‚™æ‰€æœ‰æœç´¢æä¾›å•†
        providers = [self.search_config.primary] + (self.search_config.fallback_chain or [])

        self.logger.info(
            f"ğŸ Race mode: Starting {len(providers)} search engines in parallel",
            "deep_research",
            "race_mode",
            providers=[p.value for p in providers]
        )

        # å‰µå»ºæ‰€æœ‰æœç´¢ä»»å‹™
        search_tasks = [
            self._try_search_provider_with_timeout(provider, query, goal)
            for provider in providers
        ]

        # ä½¿ç”¨ asyncio.as_completed ç²å–ç¬¬ä¸€å€‹æˆåŠŸçš„çµæœ
        for future in asyncio.as_completed(search_tasks):
            try:
                result = await future
                if result and result.get('sources'):
                    provider_name = result.get('provider', 'unknown')
                    self.logger.info(
                        f"ğŸ† Race winner: {provider_name} returned first with {len(result.get('sources', []))} sources",
                        "deep_research",
                        "race_winner",
                        provider=provider_name
                    )
                    return result
            except Exception as e:
                # å¿½ç•¥å–®å€‹å¼•æ“çš„éŒ¯èª¤ï¼Œç¹¼çºŒç­‰å¾…å…¶ä»–å¼•æ“
                continue

        # å¦‚æœæ‰€æœ‰å¼•æ“éƒ½å¤±æ•—ï¼Œè¿”å›ç©ºçµæœ
        return {
            'summary': 'No search results available',
            'sources': [],
            'relevance': 0
        }

    async def _try_search_provider_with_timeout(self, provider: SearchProviderType, query: str, goal: str) -> Optional[Dict]:
        # å¸¶è¶…æ™‚çš„æœç´¢æä¾›å•†å˜—è©¦
        try:
            # ä½¿ç”¨é…ç½®çš„è¶…æ™‚æ™‚é–“
            timeout = getattr(self.search_config, 'timeout', 30.0)

            result = await asyncio.wait_for(
                self._try_search_provider(provider, query, goal),
                timeout=timeout
            )

            if result:
                result['provider'] = provider.value

            return result

        except asyncio.TimeoutError:
            self.logger.warning(
                f"â±ï¸ Search timeout for {provider.value} after {timeout}s",
                "deep_research",
                "timeout"
            )
            return None
        except Exception as e:
            self.logger.error(
                f"Search error with {provider.value}: {str(e)}",
                "deep_research",
                "provider_error"
            )
            return None

    async def _perform_deep_search(self, query: str, goal: str) -> Dict:
        # åŸ·è¡Œæ·±åº¦æœç´¢ â€” ä½¿ç”¨çœŸå¯¦æœç´¢æœå‹™ï¼Œç„¡å‰‡è¿”å›ç©ºçµæœ

        search_service = self.services.get("search")

        # è¨˜éŒ„ Web Query
        self.logger.info(
            f"ğŸŒ Web Query: {query}",
            "web",
            "query",
            query=query,
            goal=goal,
            search_engine="web" if search_service else "none",
            max_results=10
        )

        # Use real search service if available
        search_result = None
        if search_service:
            try:
                results = await search_service.search(query, max_results=10)
                if results:
                    sources = [
                        {'url': r.url, 'title': r.title, 'relevance': 0.9}
                        for r in results
                    ]
                    summary = "\n".join(
                        f"- {r.title}: {r.snippet}" for r in results[:5]
                    )
                    search_result = {
                        'summary': summary,
                        'sources': sources,
                        'relevance': 0.92,
                        'timestamp': datetime.now().isoformat()
                    }
            except Exception as e:
                self.logger.warning(f"Search service error in deep research: {e}", "web", "fallback")

        # Fallback: search unavailable â€” return empty result with disclaimer
        if not search_result:
            self.logger.warning(
                f"Web search unavailable for deep research query: {query}",
                "web", "no_results"
            )
            search_result = {
                'summary': f"[Web search unavailable] Unable to retrieve real-time results for '{query}'. "
                           f"The final report will be based on the AI model's training data only.",
                'sources': [],
                'relevance': 0.0,
                'timestamp': datetime.now().isoformat()
            }

        # è¨˜éŒ„æœç´¢çµæœè©³æƒ…
        self.logger.info(
            f"ğŸ”— Web Results: Retrieved {len(search_result['sources'])} sources",
            "web",
            "results",
            sources=search_result['sources'][:5],
            avg_relevance=search_result['relevance']
        )

        # å¦‚æœæœ‰ LLMï¼Œè™•ç†æœç´¢çµæœ
        if self.llm_client:
            result_prompt = PromptTemplates.get_search_result_prompt(
                query=query,
                research_goal=goal,
                context=json.dumps(search_result, ensure_ascii=False)
            )
            processed = await self._call_llm(result_prompt, None)
            search_result['processed'] = processed

        return search_result

    async def _write_final_report(self, context: ProcessingContext,
                                  search_results: List[Dict],
                                  report_plan: str,
                                  critical_analysis: Optional[str] = None,
                                  computational_result: Optional[Dict[str, Any]] = None,
                                  synthesis: str = None) -> str:
        # Phase 4: ç”Ÿæˆæœ€çµ‚å ±å‘Š - å­¸è¡“è«–æ–‡æ ¼å¼ï¼ˆå€åˆ†å¼•ç”¨/æœªå¼•ç”¨ï¼‰
        self.logger.progress("final-report", "start")

        # è¨˜éŒ„æœ€çµ‚å ±å‘Šç”Ÿæˆ
        self.logger.info(
            f"ğŸ“‘ Final Report: Synthesizing {len(search_results)} search results",
            "deep_research",
            "final_report",
            phase="final-report",
            results_count=len(search_results),
            plan_length=len(report_plan)
        )

        # Use accumulated synthesis (bounded, LLM-condensed) as primary context.
        # Raw search results saved to file earlier (reversible compression).
        research_context = synthesis or self._prepare_report_context(search_results)
        references_list = self._extract_references(search_results)

        # è¨˜éŒ„è¨˜æ†¶é«”æ“ä½œ
        self.logger.info(
            f"ğŸ’¾ Memory: Research context for report",
            "memory",
            "store",
            context_size=len(research_context),
            chunks=len(search_results),
            type="research_report"
        )

        # æ§‹å»ºå¢å¼·çš„ promptï¼ŒåŒ…å«åƒè€ƒæ–‡ç»æŒ‡å¼•å’Œæ‰¹åˆ¤æ€§åˆ†æ
        enhanced_prompt = self._build_academic_report_prompt(
            report_plan,
            research_context,
            references_list,
            context.request.query,
            critical_analysis,
            computational_result
        )

        # æ¨ç†æœ€çµ‚å ±å‘Š
        self.logger.reasoning("ç¶œåˆæ‰€æœ‰ç ”ç©¶çµæœï¼Œç”Ÿæˆæœ€çµ‚å ±å‘Š...", streaming=True)

        # ç”Ÿæˆå ±å‘Šä¸»é«”
        report_body = await self._call_llm(enhanced_prompt, context)

        # åˆ†æå“ªäº›åƒè€ƒæ–‡ç»è¢«å¯¦éš›å¼•ç”¨ï¼ˆå¢å¼·ç‰ˆï¼‰
        cited_refs, uncited_refs, citation_stats = self._analyze_citations(report_body, references_list)

        # çµ„åˆå®Œæ•´å ±å‘Šï¼šä¸»é«” + å€åˆ†çš„åƒè€ƒæ–‡ç»
        final_report = self._format_report_with_categorized_references(
            report_body, cited_refs, uncited_refs, context,
            critical_analysis is not None, citation_stats, computational_result
        )

        # è¨˜éŒ„è¨˜æ†¶é«”å›æ”¶
        self.logger.info(
            f"ğŸ’¾ Memory: Retrieved research synthesis",
            "memory",
            "retrieve",
            report_length=len(final_report),
            citations_included=True
        )

        # ç™¼é€æœ€çµ‚å ±å‘Š
        self.logger.message(final_report, streaming=False)

        # å ±å‘Šå…ƒæ•¸æ“š
        report_metadata = {
            "title": f"Research Report: {context.request.query[:50]}",
            "sections": self._extract_report_sections(final_report),
            "sources_used": sum(len(r['result'].get('sources', [])) for r in search_results),
            "word_count": len(final_report.split()),
            "timestamp": datetime.now().isoformat()
        }

        # è¨˜éŒ„å ±å‘Šå®Œæˆ
        self.logger.info(
            f"âœ… Report Completed: {report_metadata['word_count']} words, {report_metadata['sources_used']} sources",
            "deep_research",
            "complete",
            metadata=report_metadata
        )

        self.logger.progress("final-report", "end", {"data": report_metadata})

        return final_report

    def _prepare_report_context(self, search_results: List[Dict],
                                max_per_result: int = 6000,
                                max_total: int = 200000) -> str:
        # æº–å‚™å ±å‘Šä¸Šä¸‹æ–‡ï¼ˆå«æˆªæ–·ä¿è­·ï¼‰
        context_parts = []
        total_chars = 0
        for i, result in enumerate(search_results, 1):
            summary = result['result'].get('summary', '')
            processed = result['result'].get('processed', '')
            if isinstance(processed, str) and len(processed) > max_per_result:
                processed = processed[:max_per_result] + "... [truncated]"
            entry = f"""
            æœç´¢ {i}: {result['query']}
            ç›®æ¨™: {result['goal']}
            å„ªå…ˆç´š: {result.get('priority', 1)}
            çµæœæ‘˜è¦: {summary}
            è™•ç†çµæœ: {processed}
            ä¾†æºæ•¸é‡: {len(result['result'].get('sources', []))}
            """
            total_chars += len(entry)
            if total_chars > max_total:
                context_parts.append(f"... [{len(search_results) - i} more results truncated]")
                break
            context_parts.append(entry)
        return "\n\n".join(context_parts)

    def _extract_report_sections(self, report: str) -> List[str]:
        # æå–å ±å‘Šç« ç¯€
        import re
        # åŒ¹é… Markdown æ¨™é¡Œ
        headers = re.findall(r'^#{1,3}\s+(.+)$', report, re.MULTILINE)
        return headers[:10]  # è¿”å›å‰10å€‹ç« ç¯€æ¨™é¡Œ

    def _extract_references(self, search_results: List[Dict]) -> List[Dict]:
        # å¾æœç´¢çµæœä¸­æå–åƒè€ƒæ–‡ç»
        references = []
        ref_id = 1

        for result in search_results:
            sources = result.get('result', {}).get('sources', [])
            for source in sources:
                if source.get('url'):
                    references.append({
                        'id': ref_id,
                        'title': source.get('title', 'Untitled'),
                        'url': source.get('url'),
                        'query': result.get('query', ''),
                        'relevance': source.get('relevance', 0)
                    })
                    ref_id += 1

        # æŒ‰ç›¸é—œæ€§æ’åº
        references.sort(key=lambda x: x.get('relevance', 0), reverse=True)
        return references

    def _build_academic_report_prompt(self, plan: str, context: str,
                                     references: List[Dict], requirement: str,
                                     critical_analysis: Optional[str] = None,
                                     computational_result: Optional[Dict[str, Any]] = None) -> str:
        # æ§‹å»ºå­¸è¡“æ ¼å¼çš„å ±å‘Š promptï¼ˆå«æ‰¹åˆ¤æ€§åˆ†æï¼‰
        # æº–å‚™åƒè€ƒæ–‡ç»æ‘˜è¦
        ref_summary = "\n".join([
            f"[{ref['id']}] {ref['title']}"
            for ref in references
        ])

        # åŸºç¤ prompt
        prompt = f"""Generate a comprehensive research report based on the following information.

Research Plan:
{plan}

Research Context and Findings:
{context}

Available References:
{ref_summary}"""

        # å¦‚æœæœ‰æ‰¹åˆ¤æ€§åˆ†æï¼Œæ·»åŠ åˆ° prompt ä¸­
        if critical_analysis:
            prompt += f"""

Critical Analysis (Multi-Perspective Thinking):
{critical_analysis}

IMPORTANT: Integrate the insights from the critical analysis throughout your report.
Use the multi-perspective thinking to enrich your conclusions and provide more nuanced views."""

        # æ·»åŠ è¦æ±‚
        prompt += f"""

Requirements:

=== STRUCTURE (MECE + Pyramid Principle) ===
1. Open with Executive Summary (3-5 bullet conclusions FIRST, then supporting evidence below)
2. Structure every section using MECE: sub-sections must be mutually exclusive and collectively exhaustive â€” no overlaps, no gaps
3. Each section follows the Pyramid Principle: state the conclusion/claim as the section heading, then provide supporting evidence underneath
4. Every factual claim ends with a So-What: "This means..." or "The implication is..." â€” never leave raw data without interpretation

=== ANALYTICAL DEPTH (Claim-Evidence-Implication) ===
5. Every analytical paragraph follows CEI: Claim (one sentence) â†’ Evidence (data, citations) â†’ Implication (so-what for the reader)
6. Cross-domain synthesis is mandatory: connect findings from different fields (e.g., regulatory changes â†’ business model impact â†’ technology response)
7. Include forward-looking analysis with specific trend predictions (2-5 year horizon with quantified estimates)

=== TABLES (Analytical, Not Listing) ===
8. Include 3-5 ANALYTICAL tables. Each table MUST have a "So-What" interpretation paragraph immediately after it. BANNED table types: simple feature lists, timeline-only tables, raw data dumps
9. Required analytical table types (use at least 2): Cross-tabulation matrix (rows vs columns with scores/ratings), Comparative scoring matrix (weighted criteria evaluation), Decomposition waterfall (breaking totals into components), Risk-impact quadrant (2x2 or 3x3 matrix with strategic implications)

=== QUANTIFICATION ===
10. Every market claim must include specific numbers: market size ($B), growth rate (CAGR%), adoption rate (%), company names with revenue/headcount
11. Use inline citations [1], [2], [3] for every factual claim â€” minimum 15 unique citations across the report
12. DO NOT include a references section in your output (it will be added separately)

=== OUTPUT STANDARDS ===
13. Aim for 3000+ words with deep analysis, not surface-level summarization
14. Structure with ## for main sections, ### for sub-sections
15. Write in professional analytical tone â€” BANNED vague phrases: "é‡è¦çš„æ˜¯", "å€¼å¾—æ³¨æ„çš„æ˜¯", "çœ¾æ‰€å‘¨çŸ¥" â€” replace with specific analytical claims
16. Include specific company names, product names, statistics, and real-world examples"""

        # å¦‚æœæœ‰æ‰¹åˆ¤æ€§åˆ†æï¼Œæ·»åŠ ç‰¹æ®Šè¦æ±‚
        req_num = 17
        if critical_analysis:
            prompt += f"""
{req_num}. Incorporate critical analysis insights to provide balanced, multi-perspective conclusions
{req_num + 1}. Address potential limitations, counterarguments, or alternative interpretations
{req_num + 2}. Demonstrate analytical depth beyond surface-level findings"""
            req_num += 3

        # å¦‚æœæœ‰è¨ˆç®—åˆ†æçµæœï¼Œæ·»åŠ åˆ° prompt ä¸­
        if computational_result:
            stdout = computational_result.get('stdout', '')
            figure_count = len(computational_result.get('figures', []))
            prompt += f"""

Computational Analysis Results:
The following quantitative analysis was performed programmatically:

Output:
{stdout}

Number of charts/figures generated: {figure_count}

IMPORTANT: Integrate these computational findings into your report:
- Reference specific numbers, calculations, and statistical results from the output
- If charts were generated, reference them as "Figure 1", "Figure 2", etc. in the appropriate sections
- Explain what the computational analysis reveals in plain language"""

            prompt += f"""
{req_num}. Integrate computational analysis results with specific numbers and findings
{req_num + 1}. Reference generated figures as "Figure 1", "Figure 2" etc. where relevant"""

        prompt += f"""

User's Research Question:
{requirement}"""

        prompt += f"""

IMPORTANT:
- Use citations [1] to [{len(references)}] naturally throughout the text
- Make the report comprehensive and detailed (aim for 3000+ words)
- Structure with clear headings using ## for main sections
- Write in professional, academic tone

Generate the report body (without references section):
"""

        return prompt


    def _analyze_citations(self, report_body: str, references: List[Dict]) -> tuple:
        """
        åˆ†æå ±å‘Šä¸­å¯¦éš›å¼•ç”¨çš„åƒè€ƒæ–‡ç»ï¼ˆå¢å¼·ç‰ˆï¼‰

        Returns:
            tuple: (cited_refs, uncited_refs, citation_stats)
            - cited_refs: è¢«å¼•ç”¨çš„æ–‡ç»åˆ—è¡¨ï¼ˆåŒ…å« citation_count å­—æ®µï¼‰
            - uncited_refs: æœªè¢«å¼•ç”¨çš„æ–‡ç»åˆ—è¡¨
            - citation_stats: è©³ç´°çµ±è¨ˆä¿¡æ¯å­—å…¸
        """
        import re
        from collections import Counter

        # æ‰¾å‡ºæ‰€æœ‰å¼•ç”¨çš„ç·¨è™ŸåŠå…¶å‡ºç¾æ¬¡æ•¸
        citation_pattern = r'\[(\d+)\]'
        citation_counts = Counter()
        invalid_citations = set()  # ç„¡æ•ˆå¼•ç”¨ï¼ˆæ²’æœ‰å°æ‡‰æ–‡ç»ï¼‰

        # å»ºç«‹æœ‰æ•ˆåƒè€ƒæ–‡ç» ID é›†åˆ
        valid_ref_ids = {ref['id'] for ref in references}

        # æƒæå ±å‘Šä¸­çš„æ‰€æœ‰å¼•ç”¨
        for match in re.finditer(citation_pattern, report_body):
            try:
                ref_num = int(match.group(1))
                citation_counts[ref_num] += 1

                # æª¢æ¸¬ç„¡æ•ˆå¼•ç”¨
                if ref_num not in valid_ref_ids:
                    invalid_citations.add(ref_num)
            except ValueError:
                continue

        # åˆ†é¡åƒè€ƒæ–‡ç»ä¸¦æ·»åŠ å¼•ç”¨æ¬¡æ•¸ä¿¡æ¯
        cited_refs = []
        uncited_refs = []

        for ref in references:
            if ref['id'] in citation_counts:
                # æ·»åŠ å¼•ç”¨æ¬¡æ•¸ä¿¡æ¯ï¼ˆä¸ä¿®æ”¹åŸå§‹ refï¼‰
                ref_with_count = ref.copy()
                ref_with_count['citation_count'] = citation_counts[ref['id']]
                cited_refs.append(ref_with_count)
            else:
                uncited_refs.append(ref)

        # æŒ‰å¼•ç”¨æ¬¡æ•¸æ’åºï¼ˆå¾é«˜åˆ°ä½ï¼‰
        cited_refs.sort(key=lambda x: x.get('citation_count', 0), reverse=True)

        # æ§‹å»ºè©³ç´°çµ±è¨ˆä¿¡æ¯
        citation_stats = {
            'total_citations': sum(citation_counts.values()),  # ç¸½å¼•ç”¨æ¬¡æ•¸
            'unique_citations': len(citation_counts),  # å”¯ä¸€å¼•ç”¨æ•¸
            'invalid_citations': list(invalid_citations),  # ç„¡æ•ˆå¼•ç”¨åˆ—è¡¨
            'most_cited': citation_counts.most_common(5),  # æœ€å¸¸å¼•ç”¨çš„å‰5å€‹
            'avg_citations_per_source': sum(citation_counts.values()) / max(1, len(citation_counts)),  # å¹³å‡æ¯å€‹ä¾†æºçš„å¼•ç”¨æ¬¡æ•¸
            'citation_distribution': dict(citation_counts)  # å®Œæ•´çš„å¼•ç”¨åˆ†ä½ˆ
        }

        return cited_refs, uncited_refs, citation_stats

    def _format_report_with_categorized_references(self, report_body: str,
                                                   cited_refs: List[Dict],
                                                   uncited_refs: List[Dict],
                                                   context: ProcessingContext = None,
                                                   has_critical_analysis: bool = False,
                                                   citation_stats: Dict = None,
                                                   computational_result: Optional[Dict[str, Any]] = None) -> str:
        """
        æ ¼å¼åŒ–å ±å‘Šï¼Œå€åˆ†å¼•ç”¨å’Œæœªå¼•ç”¨çš„åƒè€ƒæ–‡ç»ï¼ˆå¢å¼·ç‰ˆï¼‰

        Args:
            citation_stats: è©³ç´°çš„å¼•ç”¨çµ±è¨ˆä¿¡æ¯ï¼ˆå¯é¸ï¼‰
        """
        # æ§‹å»ºåƒè€ƒæ–‡ç»éƒ¨åˆ†
        references_section = "\n\n---\n\n"

        # ç¬¬ä¸€éƒ¨åˆ†ï¼šå¼•ç”¨çš„åƒè€ƒæ–‡ç»ï¼ˆæŒ‰å¼•ç”¨æ¬¡æ•¸æ’åºï¼‰
        if cited_refs:
            references_section += "## ğŸ“š åƒè€ƒæ–‡ç» (Cited References)\n\n"
            references_section += "*ä»¥ä¸‹ç‚ºå ±å‘Šä¸­å¯¦éš›å¼•ç”¨çš„æ–‡ç»ï¼ˆæŒ‰å¼•ç”¨æ¬¡æ•¸æ’åºï¼‰ï¼š*\n\n"

            for ref in cited_refs[:30]:  # é™åˆ¶æœ€å¤š30å€‹
                citation_count = ref.get('citation_count', 0)
                citation_indicator = f" `Ã—{citation_count}`" if citation_count > 1 else ""

                ref_entry = f"[{ref['id']}] **{ref['title']}**{citation_indicator}\n"
                if ref.get('url'):
                    ref_entry += f"   ğŸ“ URL: {ref['url']}\n"
                if ref.get('query'):
                    ref_entry += f"   ğŸ” Search context: {ref['query'][:50]}...\n"
                references_section += f"{ref_entry}\n"

        # ç¬¬äºŒéƒ¨åˆ†ï¼šç›¸é—œä½†æœªå¼•ç”¨çš„åƒè€ƒæ–‡ç»
        if uncited_refs:
            references_section += "\n## ğŸ“– ç›¸é—œæ–‡ç» (Related Sources - Not Cited)\n\n"
            references_section += "*ä»¥ä¸‹ç‚ºç ”ç©¶éç¨‹ä¸­æŸ¥é–±ä½†æœªç›´æ¥å¼•ç”¨çš„ç›¸é—œè³‡æ–™ï¼š*\n\n"

            for ref in uncited_refs[:20]:  # é™åˆ¶æœ€å¤š20å€‹
                ref_entry = f"â€¢ {ref['title']}\n"
                if ref.get('url'):
                    ref_entry += f"  URL: {ref['url']}\n"
                references_section += f"{ref_entry}\n"

        # æ·»åŠ çµ±è¨ˆè³‡è¨Šï¼ˆå¢å¼·ç‰ˆï¼‰
        references_section += f"\n---\n\n## ğŸ“Š å¼•ç”¨çµ±è¨ˆ (Citation Statistics)\n\n"

        # åŸºæœ¬çµ±è¨ˆ
        references_section += f"### åŸºæœ¬æŒ‡æ¨™\n"
        references_section += f"- **å¯¦éš›å¼•ç”¨æ–‡ç»**: {len(cited_refs)} ç¯‡\n"
        references_section += f"- **ç›¸é—œæœªå¼•ç”¨æ–‡ç»**: {len(uncited_refs)} ç¯‡\n"
        references_section += f"- **ç¸½æŸ¥é–±æ–‡ç»**: {len(cited_refs) + len(uncited_refs)} ç¯‡\n"
        references_section += f"- **å¼•ç”¨ç‡**: {len(cited_refs) / max(1, len(cited_refs) + len(uncited_refs)) * 100:.1f}%\n"

        # å¢å¼·çµ±è¨ˆï¼ˆå¦‚æœæœ‰ citation_statsï¼‰
        if citation_stats:
            references_section += f"\n### å¼•ç”¨æ·±åº¦åˆ†æ\n"
            references_section += f"- **ç¸½å¼•ç”¨æ¬¡æ•¸**: {citation_stats['total_citations']} æ¬¡\n"
            references_section += f"- **å¹³å‡æ¯ç¯‡æ–‡ç»è¢«å¼•ç”¨**: {citation_stats['avg_citations_per_source']:.1f} æ¬¡\n"

            # æœ€å¸¸å¼•ç”¨çš„æ–‡ç»
            if citation_stats['most_cited']:
                references_section += f"- **æœ€å¸¸å¼•ç”¨**: "
                most_cited_strs = [f"[{ref_id}] ({count}æ¬¡)" for ref_id, count in citation_stats['most_cited'][:3]]
                references_section += ", ".join(most_cited_strs) + "\n"

            # ç„¡æ•ˆå¼•ç”¨è­¦å‘Š
            if citation_stats['invalid_citations']:
                references_section += f"\nâš ï¸ **è­¦å‘Š**: æª¢æ¸¬åˆ° {len(citation_stats['invalid_citations'])} å€‹ç„¡æ•ˆå¼•ç”¨ç·¨è™Ÿ: {citation_stats['invalid_citations']}\n"

        # åˆ†ææ¨¡å¼èªªæ˜
        references_section += f"\n### åˆ†ææ¨¡å¼\n"
        modes = ["æ·±åº¦ç ”ç©¶"]
        if has_critical_analysis:
            modes.append("æ‰¹åˆ¤æ€§æ€è€ƒ")
        if computational_result:
            fig_count = len(computational_result.get("figures", []))
            modes.append(f"è¨ˆç®—åˆ†æ ({fig_count} figures)")
        references_section += f"- **ç ”ç©¶æ¨¡å¼**: {' + '.join(modes)}\n"

        references_section += f"\n---\n"
        references_section += f"*Report generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"
        references_section += f"*Powered by OpenCode Deep Research Engine"

        if has_critical_analysis:
            references_section += f" with Critical Analysis*"
        else:
            references_section += f"*"

        # Embed figures inline at target sections (or fallback to bottom)
        overflow_figures = ""
        if computational_result and computational_result.get("figures"):
            figure_specs = computational_result.get("figure_specs", [])
            for i, fig_base64 in enumerate(computational_result["figures"], 1):
                spec = figure_specs[i - 1] if i - 1 < len(figure_specs) else {}
                title = spec.get("title", f"Figure {i}")
                insight = spec.get("insight", "")
                target_section = spec.get("target_section", "")

                figure_md = f"\n\n**Figure {i}: {title}**\n\n"
                figure_md += f"![Figure {i}: {title}](data:image/png;base64,{fig_base64})\n\n"
                if insight:
                    figure_md += f"*{insight}*\n\n"

                # Try to insert after the target section's first paragraph
                inserted = False
                if target_section and len(target_section) >= 4:
                    escaped = re.escape(target_section[:20])
                    pattern = re.compile(
                        rf'(#{1,3}\s+[^\n]*{escaped}[^\n]*\n(?:(?!#{1,3}\s).+\n)*)',
                        re.IGNORECASE,
                    )
                    match = pattern.search(report_body)
                    if match:
                        insert_pos = match.end()
                        report_body = report_body[:insert_pos] + figure_md + report_body[insert_pos:]
                        inserted = True

                if not inserted:
                    overflow_figures += figure_md

        # çµ„åˆå®Œæ•´å ±å‘Š
        full_report = f"{report_body}{overflow_figures}{references_section}"

        # Save report as structured bundle
        if context:
            try:
                save_path = self._save_report_bundle(
                    full_report, context, computational_result, cited_refs
                )
                if save_path:
                    self.logger.info(f"Report saved to: {save_path}", "deep_research", "report_saved")
            except Exception as e:
                self.logger.warning(f"Failed to save report: {e}", "deep_research", "save_error")

        return full_report

    def _save_report_bundle(self, full_report: str, context: ProcessingContext,
                             computational_result: Optional[Dict[str, Any]] = None,
                             cited_refs: List[Dict] = None) -> Optional[str]:
        """Save report as a structured bundle: report.md + metadata.json + figures/."""
        from pathlib import Path
        import base64

        trace_id = context.request.trace_id
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        bundle_name = f"{trace_id[:8]}_{timestamp}"

        reports_dir = Path(self.logger.log_dir) / "reports"
        bundle_dir = reports_dir / bundle_name
        bundle_dir.mkdir(parents=True, exist_ok=True)

        # 1. Save figures as individual PNG files + replace base64 inline
        report_for_bundle = full_report
        if computational_result and computational_result.get("figures"):
            figures_dir = bundle_dir / "figures"
            figures_dir.mkdir(exist_ok=True)
            for i, fig_b64 in enumerate(computational_result["figures"], 1):
                fig_bytes = base64.b64decode(fig_b64)
                (figures_dir / f"figure_{i}.png").write_bytes(fig_bytes)
                # Replace base64 inline image with relative path
                report_for_bundle = re.sub(
                    rf'!\[Figure {i}[^\]]*\]\(data:image/png;base64,[A-Za-z0-9+/=]+\)',
                    f'![Figure {i}](figures/figure_{i}.png)',
                    report_for_bundle,
                )

        (bundle_dir / "report.md").write_text(report_for_bundle, encoding="utf-8")

        # 2. Save metadata.json
        figure_specs = (computational_result or {}).get("figure_specs", [])
        metadata = {
            "query": context.request.query if context.request else "N/A",
            "mode": "deep_research",
            "model": getattr(self.llm_client, 'model', 'unknown'),
            "timestamp": datetime.now().isoformat(),
            "duration_ms": context.response.metadata.get("total_duration_ms", 0),
            "tokens": context.response.metadata.get("total_tokens", {}),
            "citations": {
                "cited_count": len(cited_refs) if cited_refs else 0,
            },
            "figures": {
                "count": len(computational_result.get("figures", [])) if computational_result else 0,
                "titles": [s.get("title", "") for s in figure_specs],
            },
            "stages": context.response.metadata.get("stages", []),
        }
        (bundle_dir / "metadata.json").write_text(
            json.dumps(metadata, ensure_ascii=False, indent=2), encoding="utf-8"
        )

        self.logger.info(f"Report bundle saved: {bundle_dir}", "deep_research", "bundle_saved")
        return str(bundle_dir)

    # ============================================================
    # Enhanced Deep Research Methods (SSE Streaming & Events)
    # ============================================================

    async def process_with_streaming(self, context: ProcessingContext) -> AsyncGenerator[str, None]:
        # æ”¯æ´ SSE Streaming çš„è™•ç†æ–¹æ³•
        # Yields: SSE æ ¼å¼çš„äº‹ä»¶å­—ç¬¦ä¸²
        self._streaming_enabled = True

        # å•Ÿå‹•äº‹ä»¶è™•ç†å”ç¨‹
        event_task = asyncio.create_task(self._event_stream_handler())

        try:
            # ç™¼é€é–‹å§‹äº‹ä»¶
            await self._emit_event(ResearchEvent(
                type="progress",
                step="init",
                data={"status": "start", "query": context.request.query}
            ))

            # åŸ·è¡Œç ”ç©¶æµç¨‹
            result = await self.process(context)

            # ç™¼é€å®Œæˆäº‹ä»¶
            await self._emit_event(ResearchEvent(
                type="progress",
                step="complete",
                data={"status": "complete", "result_length": len(result)}
            ))

            # ç™¼é€æœ€çµ‚çµæœ
            yield f"data: {json.dumps({'type': 'final_report', 'data': result}, ensure_ascii=False)}\n\n"

        finally:
            # æ¸…ç†
            self._streaming_enabled = False
            await self.event_queue.put(None)  # çµæŸä¿¡è™Ÿ
            await event_task

    async def _event_stream_handler(self):
        # è™•ç†äº‹ä»¶æµ
        while True:
            event = await self.event_queue.get()
            if event is None:
                break

            # å¦‚æœæœ‰å›èª¿å‡½æ•¸ï¼Œèª¿ç”¨å®ƒ
            if self.event_callback:
                try:
                    await self._call_event_callback(event)
                except Exception as e:
                    self.logger.warning(f"Event callback error: {e}", "deep_research", "callback_error")

    async def _call_event_callback(self, event: ResearchEvent):
        # å®‰å…¨èª¿ç”¨äº‹ä»¶å›èª¿
        if asyncio.iscoroutinefunction(self.event_callback):
            await self.event_callback(event)
        else:
            self.event_callback(event)

    async def _emit_event(self, event: ResearchEvent):
        # ç™¼é€äº‹ä»¶
        if hasattr(self, 'event_queue'):
            await self.event_queue.put(event)

        # åŒæ™‚è¨˜éŒ„åˆ°æ—¥èªŒ
        self.logger.info(
            f"ğŸ“¡ Event: {event.type} - {event.step}",
            "deep_research",
            "event",
            event_type=event.type,
            event_step=event.step
        )

    # ============================================================
    # Multi-Search Engine Support
    # ============================================================

    async def _perform_deep_search_enhanced(self, query: str, goal: str) -> Dict:
        # å¢å¼·ç‰ˆæ·±åº¦æœç´¢ - æ”¯æ´å¤šæœç´¢å¼•æ“å’Œæ™ºèƒ½é™ç´š
        # ç™¼é€æœç´¢é–‹å§‹äº‹ä»¶
        await self._emit_event(ResearchEvent(
            type="progress",
            step="search",
            data={
                "status": "start",
                "query": query,
                "goal": goal,
                "provider": self.search_config.primary.value
            }
        ))

        # å˜—è©¦ä¸»è¦æœç´¢å¼•æ“
        search_result = await self._try_search_provider(
            self.search_config.primary,
            query,
            goal
        )

        # å¦‚æœä¸»è¦å¼•æ“å¤±æ•—ï¼Œå˜—è©¦å‚™ç”¨éˆ
        if not search_result or not search_result.get('sources'):
            for fallback_provider in self.search_config.fallback_chain:
                self.logger.info(
                    f"ğŸ”„ Switching to fallback: {fallback_provider.value}",
                    "deep_research",
                    "fallback"
                )

                await self._emit_event(ResearchEvent(
                    type="message",
                    step="search",
                    data={
                        "message": f"Switching to {fallback_provider.value}...",
                        "provider": fallback_provider.value
                    }
                ))

                search_result = await self._try_search_provider(
                    fallback_provider,
                    query,
                    goal
                )

                if search_result and search_result.get('sources'):
                    break

        # ç™¼é€æœç´¢çµæœäº‹ä»¶
        if search_result:
            await self._emit_event(ResearchEvent(
                type="search_result",
                step="search",
                data={
                    "query": query,
                    "sources_count": len(search_result.get('sources', [])),
                    "summary": search_result.get('summary', '')[:200]
                }
            ))

        return search_result or self._empty_search_result(query)

    async def _try_search_provider(self,
                                   provider: SearchProviderType,
                                   query: str,
                                   goal: str) -> Optional[Dict]:
        # å˜—è©¦ä½¿ç”¨ç‰¹å®šæœç´¢æä¾›å•†
        try:
            if provider == SearchProviderType.MODEL:
                # ä½¿ç”¨ AI æ¨¡å‹å…§å»ºæœç´¢
                return await self._model_based_search(query, goal)
            elif provider == SearchProviderType.EXA:
                # ä½¿ç”¨ Exa neural search
                return await self._exa_search(query, goal)

            # ä½¿ç”¨å…¶ä»–æœç´¢æœå‹™
            search_service = self.services.get("search")
            if search_service:
                # å¦‚æœæœå‹™æ”¯æ´è¨­ç½®æä¾›å•†
                if hasattr(search_service, 'set_provider'):
                    search_service.set_provider(provider.value.lower())

                results = await search_service.search(
                    query=query,
                    max_results=self.search_config.max_results
                )

                if results:
                    return self._format_search_results(results, provider.value)

        except Exception as e:
            self.logger.warning(
                f"Search error with {provider.value}: {e}",
                "deep_research",
                "search_error"
            )

        return None

    async def _exa_search(self, query: str, goal: str) -> Optional[Dict]:
        # ä½¿ç”¨ Exa API é€²è¡Œç¥ç¶“æœç´¢
        search_service = self.services.get("search")
        if not search_service:
            return None

        # åˆ¤æ–·æœç´¢é¡å‹
        search_type = "general"
        if "code" in goal.lower() or "programming" in goal.lower():
            search_type = "code"
        elif "research" in goal.lower() or "paper" in goal.lower():
            search_type = "research"
        elif "news" in goal.lower() or "latest" in goal.lower():
            search_type = "news"

        try:
            # ä½¿ç”¨æ•´åˆçš„æœç´¢æœå‹™ï¼ˆå·²åŒ…å« Exaï¼‰
            if hasattr(search_service, 'provider'):
                old_provider = search_service.provider
                search_service.provider = "exa"
                results = await search_service.search(
                    query=query,
                    max_results=self.search_config.max_results,
                    search_type=search_type
                )
                search_service.provider = old_provider

                if results:
                    return self._format_search_results(results, "exa")

        except Exception as e:
            self.logger.error(f"Exa search failed: {e}", "deep_research", "exa_error")

        return None

    async def _model_based_search(self, query: str, goal: str) -> Dict:
        # ä½¿ç”¨ AI æ¨¡å‹çš„å…§å»ºæœç´¢èƒ½åŠ›
        if not self.llm_client:
            return self._empty_search_result(query)

        search_prompt = (
            f"Please search and provide information about:\n"
            f"Query: {query}\n"
            f"Research Goal: {goal}\n\n"
            f"Provide a comprehensive answer based on your knowledge, formatted as:\n"
            f"1. Summary of findings\n"
            f"2. Key facts and details\n"
            f"3. Relevant context\n\n"
            f"Focus on accuracy and relevance."
        )

        try:
            response = await self._call_llm(search_prompt, None)

            return {
                'summary': response,
                'sources': [{
                    'title': 'AI Knowledge Base',
                    'url': 'model://knowledge',
                    'relevance': 0.8
                }],
                'relevance': 0.8,
                'timestamp': datetime.now().isoformat(),
                'provider': 'model'
            }
        except Exception as e:
            self.logger.error(f"Model search failed: {e}", "deep_research", "model_search_error")
            return self._empty_search_result(query)

    def _format_search_results(self, results: List, provider: str) -> Dict:
        # æ ¼å¼åŒ–æœç´¢çµæœ
        if not results:
            return None

        sources = []
        summary_parts = []

        for r in results[:self.search_config.max_results]:
            # é©é…ä¸åŒçš„çµæœæ ¼å¼
            if hasattr(r, 'url'):
                sources.append({
                    'url': r.url,
                    'title': getattr(r, 'title', 'Untitled'),
                    'relevance': getattr(r, 'score', 0.5)
                })
                summary_parts.append(f"- {r.title}: {getattr(r, 'snippet', '')[:100]}")
            elif isinstance(r, dict):
                sources.append({
                    'url': r.get('url', ''),
                    'title': r.get('title', 'Untitled'),
                    'relevance': r.get('score', 0.5)
                })
                summary_parts.append(f"- {r.get('title')}: {r.get('snippet', '')[:100]}")

        return {
            'summary': '\n'.join(summary_parts),
            'sources': sources,
            'relevance': sum(s['relevance'] for s in sources) / max(len(sources), 1),
            'timestamp': datetime.now().isoformat(),
            'provider': provider
        }

    def _empty_search_result(self, query: str) -> Dict:
        # è¿”å›ç©ºæœç´¢çµæœ
        return {
            'summary': f"[No search results available for: {query}]",
            'sources': [],
            'relevance': 0.0,
            'timestamp': datetime.now().isoformat(),
            'provider': 'none'
        }

    # Configuration methods
    def configure_search_engines(self, config: SearchEngineConfig):
        # å‹•æ…‹é…ç½®æœç´¢å¼•æ“
        self.search_config = config
        self.logger.info(
            f"Search engines configured: primary={config.primary.value}, "
            f"fallback={[p.value for p in config.fallback_chain]}",
            "deep_research",
            "config_update"
        )

    def enable_streaming(self, enabled: bool = True):
        # å•Ÿç”¨/ç¦ç”¨ streaming
        self._streaming_enabled = enabled

    def set_event_callback(self, callback: Callable[[ResearchEvent], None]):
        # è¨­ç½®äº‹ä»¶å›èª¿
        self.event_callback = callback

