"""
Search Processor - Iterative web search with quality evaluation

Performs multi-iteration web searches with quality assessment and refinement.
Extracted from monolithic processor.py
"""

import json
import re
from typing import Dict, List

from .base import BaseProcessor
from ..models import ProcessingContext
from ..prompts import PromptTemplates
from ..error_handler import enhanced_error_handler


class SearchProcessor(BaseProcessor):
    """ç¶²è·¯æœç´¢è™•ç†å™¨ - æ”¯æ´è¿­ä»£æœç´¢èˆ‡è³ªé‡è©•ä¼°"""

    @enhanced_error_handler(max_retries=2, retryable_categories=["NETWORK", "LLM"])
    async def process(self, context: ProcessingContext) -> str:
        self.logger.progress("web-search", "start")
        context.set_current_step("web-search")

        # è¨˜éŒ„å·¥å…·æ±ºç­–
        await self._log_tool_decision(
            "web_search",
            "ç”¨æˆ¶æŸ¥è©¢éœ€è¦ç¶²è·¯æœç´¢ä¾†ç²å–æœ€æ–°è³‡è¨Š",
            0.95
        )

        # è¿­ä»£æœç´¢æ©Ÿåˆ¶
        MAX_ITERATIONS = 2
        all_search_results = []
        iteration = 0

        while iteration < MAX_ITERATIONS:
            iteration += 1
            self.logger.info(f"ğŸ”„ Search Iteration {iteration}/{MAX_ITERATIONS}", "search", "iteration")

            # Step 1: ç”Ÿæˆ SERP æŸ¥è©¢
            self.logger.progress("query-generation", "start")

            if iteration == 1:
                # ç¬¬ä¸€æ¬¡ï¼šåŸºæ–¼åŸå§‹æŸ¥è©¢ç”Ÿæˆ
                search_queries = await self._generate_serp_queries(context.request.query)
            else:
                # å¾ŒçºŒè¿­ä»£ï¼šåŸºæ–¼è³ªé‡è©•ä¼°æ”¹é€²æŸ¥è©¢
                search_queries = await self._refine_search_queries(
                    context.request.query,
                    all_search_results
                )

            if not search_queries:
                break

            self.logger.info(
                f"ğŸ“ Generated {len(search_queries)} search queries",
                "search",
                "queries_generated",
                queries=search_queries
            )
            self.logger.progress("query-generation", "end", {"queries": len(search_queries)})

            # Step 2: åŸ·è¡Œæœç´¢
            self.logger.progress("searching", "start")
            iteration_results = []
            for i, query_obj in enumerate(search_queries, 1):
                self.logger.info(
                    f"ğŸŒ Searching {i}/{len(search_queries)}: {query_obj.get('query', '')[:100]}",
                    "search",
                    "performing_search"
                )
                results = await self._perform_search(query_obj.get('query', ''))
                iteration_results.append({
                    'query': query_obj.get('query'),
                    'goal': query_obj.get('researchGoal'),
                    'results': results,
                    'iteration': iteration
                })
            self.logger.progress("searching", "end", {"total_results": len(iteration_results)})

            all_search_results.extend(iteration_results)

            # Step 3: è©•ä¼°æœç´¢è³ªé‡
            is_sufficient = await self._evaluate_search_quality(all_search_results, context.request.query)

            if is_sufficient:
                self.logger.info("âœ… Search quality is sufficient", "search", "quality_ok")
                break

            self.logger.info("ğŸ“Š Search needs refinement, continuing...", "search", "refine")

        # Step 4: åˆæˆæœ€çµ‚çµæœ
        combined_context = "\n\n".join([
            f"Query: {r['query']}\nGoal: {r['goal']}\nResults: {r['results']}"
            for r in all_search_results
        ])

        self.logger.info(
            f"ğŸ”„ Synthesizing {len(all_search_results)} search results...",
            "search",
            "synthesis"
        )

        prompt = PromptTemplates.get_search_result_prompt(
            query=context.request.query,
            research_goal="æä¾›å…¨é¢ã€æº–ç¢ºçš„ç­”æ¡ˆ",
            context=combined_context
        )

        citation_rules = PromptTemplates.get_citation_rules()
        full_prompt = f"{prompt}\n\n{citation_rules}"

        response = await self._call_llm(full_prompt, context)

        self.logger.message(response)
        context.mark_step_complete("web-search")
        self.logger.progress("web-search", "end")

        return response

    async def _generate_serp_queries(self, user_query: str) -> List[Dict[str, str]]:
        """ç”Ÿæˆå„ªåŒ–çš„ SERP æŸ¥è©¢ - ä½¿ç”¨å°ˆæ¥­ prompt"""
        # å…ˆç”Ÿæˆç°¡å–®çš„ç ”ç©¶è¨ˆåŠƒ
        plan = f"ç ”ç©¶ä¸»é¡Œ: {user_query}"

        # å®šç¾© schema
        output_schema = {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "query": {"type": "string"},
                    "researchGoal": {"type": "string"}
                }
            }
        }

        # ä½¿ç”¨å°ˆæ¥­çš„ SERP æŸ¥è©¢æç¤ºè©
        prompt = PromptTemplates.get_serp_queries_prompt(plan, output_schema)

        response = await self._call_llm(prompt, None)

        # è§£æ JSON å›æ‡‰
        try:
            # å¾å›æ‡‰ä¸­æå– JSON
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response)
            if json_match:
                queries = json.loads(json_match.group(1))
            else:
                # å¦‚æœæ²’æœ‰ markdown åŒ…è£ï¼Œç›´æ¥è§£æ
                queries = json.loads(response)
            return queries[:3]  # é™åˆ¶æœ€å¤š 3 å€‹æŸ¥è©¢
        except:
            # å¦‚æœè§£æå¤±æ•—ï¼Œè¿”å›é è¨­æŸ¥è©¢
            return [{"query": user_query, "researchGoal": "ç²å–ç›¸é—œè³‡è¨Š"}]

    async def _perform_search(self, query: str) -> str:
        """åŸ·è¡Œç¶²è·¯æœç´¢ - ä½¿ç”¨çœŸå¯¦æœç´¢æœå‹™æˆ– LLM fallback"""
        # è¨˜éŒ„æœç´¢æŸ¥è©¢
        search_service = self.services.get("search")
        provider = getattr(search_service, 'primary_provider', 'none') if search_service else 'none'
        self.logger.info(
            f"ğŸ” Web Query: {query}",
            "search",
            "query",
            query=query,
            provider=provider
        )

        # Use real search service if available
        raw_results = ""
        if search_service:
            try:
                results = await search_service.search(query, max_results=5)
                if results:
                    raw_results = "\n\n".join(
                        f"[{r.title}]\n{r.snippet}\nURL: {r.url}"
                        for r in results
                    )
                    self.logger.info(
                        f"ğŸ” Search returned {len(results)} results",
                        "search", "results"
                    )
            except Exception as e:
                self.logger.warning(f"Search service error, falling back to LLM: {e}", "search", "fallback")

        # Fallback: no search results â†’ LLM answers with disclaimer
        if not raw_results:
            self.logger.warning("No search results available â€” LLM will answer from training data", "search", "no_results")
            raw_results = (
                f"[Web search unavailable â€” no real-time results for '{query}'. "
                f"The following answer is based on the AI model's training data only.]"
            )

        if self.llm_client:
            result_prompt = PromptTemplates.get_query_result_prompt(
                query=query,
                research_goal="æä¾›æº–ç¢ºã€æœ€æ–°çš„è³‡è¨Š"
            )
            full_prompt = f"{result_prompt}\n\næœç´¢çµæœï¼š{raw_results}"
            processed_results = await self._call_llm(full_prompt, None)
            return processed_results

        return raw_results

    async def _evaluate_search_quality(self, results: List[Dict], original_query: str) -> bool:
        """è©•ä¼°æœç´¢çµæœè³ªé‡æ˜¯å¦å……åˆ†"""
        if not results:
            return False

        # ç°¡å–®çš„è³ªé‡æª¢æŸ¥
        total_content = sum(len(r.get('results', '')) for r in results)
        unique_queries = len(set(r['query'] for r in results))

        # åŸºæ–¼å…§å®¹é‡å’ŒæŸ¥è©¢å¤šæ¨£æ€§è©•ä¼°
        if total_content < 500 or unique_queries < 2:
            return False

        # ä½¿ç”¨ LLM è©•ä¼°ç›¸é—œæ€§
        evaluation_prompt = f"""Evaluate if the search results are sufficient for answering the query.

Original Query: {original_query}

Search Results Summary:
- Total results: {len(results)}
- Total content: {total_content} characters
- Unique queries: {unique_queries}

First few results:
{results[0].get('results', '')[:500] if results else 'No results'}

Answer with YES if sufficient, NO if more search is needed.
Consider: coverage, relevance, quality.

Answer (YES/NO):"""

        response = await self._call_llm(evaluation_prompt, None)
        return "YES" in response.upper()[:10]

    async def _refine_search_queries(self, original_query: str, previous_results: List[Dict]) -> List[Dict[str, str]]:
        """åŸºæ–¼å‰æ¬¡çµæœæ”¹é€²æœç´¢æŸ¥è©¢"""
        # æº–å‚™å·²æœ‰çµæœæ‘˜è¦
        results_summary = "\n".join([
            f"Query: {r['query']}\nFound: {r['results'][:200]}..."
            for r in previous_results[:3]
        ])

        refine_prompt = f"""Based on the original query and previous search results, generate improved search queries to fill knowledge gaps.

Original Query: {original_query}

Previous Search Results:
{results_summary}

Identify what's missing and generate 1-2 new search queries that would provide additional valuable information.

Output JSON array format:
[{{"query": "specific search query", "researchGoal": "what to find"}}]

Generate queries:"""

        response = await self._call_llm(refine_prompt, None)

        try:
            # å˜—è©¦è§£æ JSON
            json_match = re.search(r'\[.*?\]', response, re.DOTALL)
            if json_match:
                queries = json.loads(json_match.group(0))
                return queries[:2]  # é™åˆ¶æœ€å¤š2å€‹æ–°æŸ¥è©¢
        except:
            pass

        return []
