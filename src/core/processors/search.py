"""
Search Processor - Iterative web search with quality evaluation

Performs multi-iteration web searches with quality assessment and refinement.
Extracted from monolithic processor.py
"""

import json
import re
from typing import Dict, List, Tuple

from .base import BaseProcessor
from ..models_v2 import ProcessingContext
from ..prompts import PromptTemplates
from ..error_handler import enhanced_error_handler


class SearchProcessor(BaseProcessor):
    """ç¶²è·¯æœç´¢è™•ç†å™¨ - æ”¯æ´è¿­ä»£æœç´¢èˆ‡è³ªé‡è©•ä¼°"""

    @enhanced_error_handler(max_retries=2, retryable_categories=["NETWORK", "LLM"])
    async def process(self, context: ProcessingContext) -> str:
        self.logger.progress("web-search", "start")
        context.set_current_step("web-search")

        # è¨˜éŒ„å·¥å…·æ±ºç­–
        await self._log_tool_decision(
            "web_search",
            "ç”¨æˆ¶æŸ¥è©¢éœ€è¦ç¶²è·¯æœç´¢ä¾†ç²å–æœ€æ–°è³‡è¨Š",
            0.95
        )

        # è¿­ä»£æœç´¢æ©Ÿåˆ¶
        MAX_ITERATIONS = 2
        all_search_results = []
        all_sources = []  # raw SearchResult objects
        iteration = 0

        while iteration < MAX_ITERATIONS:
            iteration += 1
            self.logger.info(f"ğŸ”„ Search Iteration {iteration}/{MAX_ITERATIONS}", "search", "iteration")

            # Step 1: ç”Ÿæˆ SERP æŸ¥è©¢
            self.logger.progress("query-generation", "start")

            if iteration == 1:
                # ç¬¬ä¸€æ¬¡ï¼šåŸºæ–¼åŸå§‹æŸ¥è©¢ç”Ÿæˆ
                search_queries = await self._generate_serp_queries(context.request.query)
            else:
                # å¾ŒçºŒè¿­ä»£ï¼šåŸºæ–¼è³ªé‡è©•ä¼°æ”¹é€²æŸ¥è©¢
                search_queries = await self._refine_search_queries(
                    context.request.query,
                    all_search_results
                )

            if not search_queries:
                break

            self.logger.info(
                f"ğŸ“ Generated {len(search_queries)} search queries",
                "search",
                "queries_generated",
                queries=search_queries
            )
            self.logger.progress("query-generation", "end", {"queries": len(search_queries)})

            # Step 2: åŸ·è¡Œæœç´¢
            self.logger.progress("searching", "start")
            iteration_results = []
            for i, query_obj in enumerate(search_queries, 1):
                self.logger.info(
                    f"ğŸŒ Searching {i}/{len(search_queries)}: {query_obj.get('query', '')[:100]}",
                    "search",
                    "performing_search"
                )
                processed_text, sources = await self._perform_search(query_obj.get('query', ''))
                all_sources.extend(sources)
                iteration_results.append({
                    'query': query_obj.get('query'),
                    'goal': query_obj.get('researchGoal'),
                    'results': processed_text,
                    'iteration': iteration
                })
            self.logger.progress("searching", "end", {"total_results": len(iteration_results)})

            all_search_results.extend(iteration_results)

            # Step 3: è©•ä¼°æœç´¢è³ªé‡
            is_sufficient = await self._evaluate_search_quality(all_search_results, context.request.query)

            if is_sufficient:
                self.logger.info("âœ… Search quality is sufficient", "search", "quality_ok")
                break

            self.logger.info("ğŸ“Š Search needs refinement, continuing...", "search", "refine")

        # Build deduplicated reference list
        references = self._build_references(all_sources)

        # Step 4: åˆæˆæœ€çµ‚çµæœ
        combined_context = "\n\n".join([
            f"Query: {r['query']}\nGoal: {r['goal']}\nResults: {r['results']}"
            for r in all_search_results
        ])

        self.logger.info(
            f"ğŸ”„ Synthesizing {len(all_search_results)} search results with {len(references)} references...",
            "search",
            "synthesis"
        )

        prompt = PromptTemplates.get_search_result_prompt(
            query=context.request.query,
            research_goal="æä¾›å…¨é¢ã€æº–ç¢ºçš„ç­”æ¡ˆ",
            context=combined_context
        )

        # Provide reference mapping so LLM can cite correctly
        ref_mapping = "\n".join(
            f"[{ref['id']}] {ref['title']} - {ref['url']}"
            for ref in references
        )
        citation_rules = PromptTemplates.get_citation_rules()
        full_prompt = f"{prompt}\n\nAvailable References:\n{ref_mapping}\n\n{citation_rules}"

        response = await self._call_llm(full_prompt, context)

        # Append reference list
        if references:
            response = self._append_references(response, references)

        self.logger.message(response)
        context.mark_step_complete("web-search")
        self.logger.progress("web-search", "end")

        return response

    def _build_references(self, sources: list) -> List[Dict]:
        """Build deduplicated reference list from raw SearchResult objects."""
        seen_urls = set()
        references = []
        ref_id = 1

        for source in sources:
            url = source.url if hasattr(source, 'url') else source.get('url', '')
            if not url or url in seen_urls:
                continue
            seen_urls.add(url)

            title = source.title if hasattr(source, 'title') else source.get('title', 'Untitled')
            references.append({
                'id': ref_id,
                'title': title,
                'url': url,
            })
            ref_id += 1

        return references

    def _append_references(self, response: str, references: List[Dict]) -> str:
        """Append formatted reference list to the response."""
        # Scan which references were actually cited
        cited_ids = set()
        for match in re.finditer(r'\[(\d+)\]', response):
            cited_ids.add(int(match.group(1)))

        # Separate cited vs uncited
        cited = [r for r in references if r['id'] in cited_ids]
        uncited = [r for r in references if r['id'] not in cited_ids]

        parts = [response, "\n\n---\n"]

        if cited:
            parts.append("\n**References:**\n")
            for ref in cited:
                parts.append(f"[{ref['id']}] [{ref['title']}]({ref['url']})\n")

        if uncited:
            parts.append("\n**Related Sources:**\n")
            for ref in uncited:
                parts.append(f"- [{ref['title']}]({ref['url']})\n")

        return "".join(parts)

    async def _generate_serp_queries(self, user_query: str) -> List[Dict[str, str]]:
        """ç”Ÿæˆå„ªåŒ–çš„ SERP æŸ¥è©¢ - ä½¿ç”¨å°ˆæ¥­ prompt"""
        # å…ˆç”Ÿæˆç°¡å–®çš„ç ”ç©¶è¨ˆåŠƒ
        plan = f"ç ”ç©¶ä¸»é¡Œ: {user_query}"

        # å®šç¾© schema
        output_schema = {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "query": {"type": "string"},
                    "researchGoal": {"type": "string"}
                }
            }
        }

        # ä½¿ç”¨å°ˆæ¥­çš„ SERP æŸ¥è©¢æç¤ºè©
        prompt = PromptTemplates.get_serp_queries_prompt(plan, output_schema)

        response = await self._call_llm(prompt, None)

        # è§£æ JSON å›æ‡‰
        try:
            # å¾å›æ‡‰ä¸­æå– JSON
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response)
            if json_match:
                queries = json.loads(json_match.group(1))
            else:
                # å¦‚æœæ²’æœ‰ markdown åŒ…è£ï¼Œç›´æ¥è§£æ
                queries = json.loads(response)
            return queries[:3]  # é™åˆ¶æœ€å¤š 3 å€‹æŸ¥è©¢
        except:
            # å¦‚æœè§£æå¤±æ•—ï¼Œè¿”å›é è¨­æŸ¥è©¢
            return [{"query": user_query, "researchGoal": "ç²å–ç›¸é—œè³‡è¨Š"}]

    async def _perform_search(self, query: str) -> Tuple[str, list]:
        """åŸ·è¡Œç¶²è·¯æœç´¢ - å›å‚³ (processed_text, raw_sources)"""
        search_service = self.services.get("search")
        provider = getattr(search_service, 'primary_provider', 'none') if search_service else 'none'
        self.logger.info(
            f"ğŸ” Web Query: {query}",
            "search",
            "query",
            query=query,
            provider=provider
        )

        # Use real search service if available
        raw_results = ""
        sources = []
        if search_service:
            try:
                results = await search_service.search(query, max_results=5)
                if results:
                    sources = list(results)
                    raw_results = "\n\n".join(
                        f"[{r.title}]\n{r.snippet}\nURL: {r.url}"
                        for r in results
                    )
                    self.logger.info(
                        f"ğŸ” Search returned {len(results)} results",
                        "search", "results"
                    )
            except Exception as e:
                self.logger.warning(f"Search service error, falling back to LLM: {e}", "search", "fallback")

        # Fallback: no search results
        if not raw_results:
            self.logger.warning("No search results available â€” LLM will answer from training data", "search", "no_results")
            raw_results = (
                f"[Web search unavailable â€” no real-time results for '{query}'. "
                f"The following answer is based on the AI model's training data only.]"
            )

        if self.llm_client:
            result_prompt = PromptTemplates.get_query_result_prompt(
                query=query,
                research_goal="æä¾›æº–ç¢ºã€æœ€æ–°çš„è³‡è¨Š"
            )
            full_prompt = f"{result_prompt}\n\næœç´¢çµæœï¼š{raw_results}"
            processed_results = await self._call_llm(full_prompt, None)
            return processed_results, sources

        return raw_results, sources

    async def _evaluate_search_quality(self, results: List[Dict], original_query: str) -> bool:
        """è©•ä¼°æœç´¢çµæœè³ªé‡æ˜¯å¦å……åˆ†"""
        if not results:
            return False

        # ç°¡å–®çš„è³ªé‡æª¢æŸ¥
        total_content = sum(len(r.get('results', '')) for r in results)
        unique_queries = len(set(r['query'] for r in results))

        # åŸºæ–¼å…§å®¹é‡å’ŒæŸ¥è©¢å¤šæ¨£æ€§è©•ä¼°
        if total_content < 500 or unique_queries < 2:
            return False

        # ä½¿ç”¨ LLM è©•ä¼°ç›¸é—œæ€§
        evaluation_prompt = f"""Evaluate if the search results are sufficient for answering the query.

Original Query: {original_query}

Search Results Summary:
- Total results: {len(results)}
- Total content: {total_content} characters
- Unique queries: {unique_queries}

First few results:
{results[0].get('results', '')[:500] if results else 'No results'}

Answer with YES if sufficient, NO if more search is needed.
Consider: coverage, relevance, quality.

Answer (YES/NO):"""

        response = await self._call_llm(evaluation_prompt, None)
        return "YES" in response.upper()[:10]

    async def _refine_search_queries(self, original_query: str, previous_results: List[Dict]) -> List[Dict[str, str]]:
        """åŸºæ–¼å‰æ¬¡çµæœæ”¹é€²æœç´¢æŸ¥è©¢"""
        # æº–å‚™å·²æœ‰çµæœæ‘˜è¦
        results_summary = "\n".join([
            f"Query: {r['query']}\nFound: {r['results'][:200]}..."
            for r in previous_results[:3]
        ])

        refine_prompt = f"""Based on the original query and previous search results, generate improved search queries to fill knowledge gaps.

Original Query: {original_query}

Previous Search Results:
{results_summary}

Identify what's missing and generate 1-2 new search queries that would provide additional valuable information.

Output JSON array format:
[{{"query": "specific search query", "researchGoal": "what to find"}}]

Generate queries:"""

        response = await self._call_llm(refine_prompt, None)

        try:
            # å˜—è©¦è§£æ JSON
            json_match = re.search(r'\[.*?\]', response, re.DOTALL)
            if json_match:
                queries = json.loads(json_match.group(0))
                return queries[:2]  # é™åˆ¶æœ€å¤š2å€‹æ–°æŸ¥è©¢
        except:
            pass

        return []
