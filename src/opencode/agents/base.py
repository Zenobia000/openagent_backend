"""
Agent åŸºé¡

å®šç¾©æ‰€æœ‰ Agent çš„é€šç”¨è¡Œç‚ºå’Œä»‹é¢
"""

import os
import json
import logging
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
import uuid

logger = logging.getLogger(__name__)


class AgentType(str, Enum):
    """Agent é¡å‹"""
    DISPATCHER = "dispatcher"     # ç¸½æ©Ÿ - åˆ†æéœ€æ±‚ã€åˆ†é…ä»»å‹™
    RESEARCHER = "researcher"     # ç ”ç©¶è€… - æœé›†å’Œåˆ†æè³‡æ–™
    WRITER = "writer"             # å¯«ä½œè€… - æ’°å¯«å…§å®¹
    CODER = "coder"               # ç·¨ç¢¼è€… - ç·¨å¯«å’Œåˆ†æç¨‹å¼
    ANALYST = "analyst"           # åˆ†æå¸« - æ•¸æ“šåˆ†æ
    REVIEWER = "reviewer"         # å¯©æ ¸è€… - å¯©æ ¸å“è³ª


@dataclass
class AgentTask:
    """Agent ä»»å‹™"""
    id: str = field(default_factory=lambda: str(uuid.uuid4())[:8])
    type: str = ""
    description: str = ""
    parameters: Dict[str, Any] = field(default_factory=dict)
    context: Dict[str, Any] = field(default_factory=dict)  # ä¸Šä¸‹æ–‡ï¼ˆä¾†è‡ªå‰ä¸€æ­¥ï¼‰
    created_at: str = field(default_factory=lambda: datetime.utcnow().isoformat())


@dataclass
class AgentResult:
    """Agent åŸ·è¡Œçµæœ"""
    task_id: str
    agent_type: str
    success: bool
    output: Any
    tool_calls: List[Dict] = field(default_factory=list)  # èª¿ç”¨çš„å·¥å…·è¨˜éŒ„
    thinking: str = ""  # æ€è€ƒéç¨‹
    error: Optional[str] = None
    execution_time: float = 0
    usage: Dict[str, Any] = field(default_factory=dict)  # Token ä½¿ç”¨é‡
    created_at: str = field(default_factory=lambda: datetime.utcnow().isoformat())


class BaseAgent(ABC):
    """
    Agent åŸºé¡
    
    æ‰€æœ‰å°ˆæ¥­ Agent ç¹¼æ‰¿æ­¤é¡ï¼Œå¯¦ç¾ process_task æ–¹æ³•
    """
    
    def __init__(self, agent_type: AgentType, name: str = None):
        self.id = f"{agent_type.value}_{uuid.uuid4().hex[:8]}"
        self.type = agent_type
        self.name = name or f"{agent_type.value.title()} Agent"
        self.model = os.getenv("LLM_MODEL", "gpt-4o")
        self._llm_client = None
        self._tool_registry = None
        self._available_tools: List[str] = []
        self._memory: List[Dict] = []  # å°è©±è¨˜æ†¶
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ– Agent"""
        try:
            # åˆå§‹åŒ– LLM å®¢æˆ¶ç«¯
            from openai import AsyncOpenAI
            api_key = os.getenv("OPENAI_API_KEY")
            if api_key:
                self._llm_client = AsyncOpenAI(api_key=api_key)
            
            # ç²å–å·¥å…·è¨»å†Šä¸­å¿ƒ
            from opencode.tools import get_tool_registry, get_tools_for_agent
            self._tool_registry = get_tool_registry()
            self._available_tools = get_tools_for_agent(self.type.value)
            
            logger.info(f"ğŸ¤– Agent initialized: {self.name} with {len(self._available_tools)} tools")
            return True
        except Exception as e:
            logger.error(f"Agent init error: {e}")
            return False
    
    @property
    @abstractmethod
    def system_prompt(self) -> str:
        """è¿”å› Agent çš„ç³»çµ±æç¤º"""
        pass
    
    @abstractmethod
    async def process_task(self, task: AgentTask) -> AgentResult:
        """
        è™•ç†ä»»å‹™
        
        Args:
            task: è¦è™•ç†çš„ä»»å‹™
            
        Returns:
            åŸ·è¡Œçµæœ
        """
        pass
    
    async def call_tool(self, tool_name: str, **kwargs) -> Dict[str, Any]:
        """
        èª¿ç”¨å·¥å…·
        
        Args:
            tool_name: å·¥å…·åç¨±
            **kwargs: å·¥å…·åƒæ•¸
            
        Returns:
            å·¥å…·åŸ·è¡Œçµæœ
        """
        if tool_name not in self._available_tools:
            return {"error": f"Tool {tool_name} not available for this agent"}
        
        if not self._tool_registry:
            return {"error": "Tool registry not initialized"}
        
        result = await self._tool_registry.execute(tool_name, **kwargs)
        logger.info(f"ğŸ”§ {self.name} called tool: {tool_name}")
        return result
    
    async def think(self, prompt: str, use_tools: bool = True) -> Dict[str, Any]:
        """
        ä½¿ç”¨ LLM æ€è€ƒ
        
        Args:
            prompt: æç¤º
            use_tools: æ˜¯å¦å•Ÿç”¨å·¥å…·èª¿ç”¨
            
        Returns:
            æ€è€ƒçµæœï¼ŒåŒ…å«å›ç­”ã€å·¥å…·èª¿ç”¨å’Œ token ä½¿ç”¨é‡
        """
        if not self._llm_client:
            logger.error(f"âŒ {self.name}: LLM client not initialized")
            return {"error": "LLM client not initialized", "answer": "", "usage": {}}
        
        messages = [
            {"role": "system", "content": self.system_prompt},
            *self._memory[-10:],  # æœ€è¿‘ 10 æ¢è¨˜æ†¶
            {"role": "user", "content": prompt}
        ]
        
        logger.info(f"ğŸ¤– {self.name} thinking... (use_tools={use_tools})")
        logger.debug(f"ğŸ“ Prompt: {prompt[:200]}...")
        
        # Token ä½¿ç”¨é‡è¿½è¹¤
        total_usage = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0
        }
        
        try:
            # æº–å‚™å·¥å…·å®šç¾©
            tools = None
            if use_tools and self._available_tools and self._tool_registry:
                tools = []
                for name in self._available_tools:
                    tool = self._tool_registry.get(name)
                    if tool:
                        tool_def = tool.to_openai_function()
                        tools.append(tool_def)
                        logger.debug(f"ğŸ”§ Tool loaded: {name}")
                
                if tools:
                    logger.info(f"ğŸ”§ {self.name} has {len(tools)} tools available")
            
            # èª¿ç”¨ LLM
            logger.info(f"ğŸ“¡ Calling LLM ({self.model})...")
            response = await self._llm_client.chat.completions.create(
                model=self.model,
                messages=messages,
                tools=tools if tools else None,
                tool_choice="auto" if tools else None,
                temperature=0.7
            )
            
            # è¨˜éŒ„ token ä½¿ç”¨é‡
            if response.usage:
                total_usage["prompt_tokens"] += response.usage.prompt_tokens
                total_usage["completion_tokens"] += response.usage.completion_tokens
                total_usage["total_tokens"] += response.usage.total_tokens
            
            message = response.choices[0].message
            tool_calls_made = []
            
            logger.info(f"âœ… LLM responded (has_tool_calls={bool(message.tool_calls)})")
            
            # è™•ç†å·¥å…·èª¿ç”¨
            if message.tool_calls:
                logger.info(f"ğŸ”§ Processing {len(message.tool_calls)} tool calls...")
                for tool_call in message.tool_calls:
                    tool_name = tool_call.function.name
                    tool_args = json.loads(tool_call.function.arguments)
                    
                    logger.info(f"ğŸ”§ Calling tool: {tool_name}")
                    logger.debug(f"   Args: {tool_args}")
                    
                    # åŸ·è¡Œå·¥å…·
                    tool_result = await self.call_tool(tool_name, **tool_args)
                    
                    logger.info(f"âœ… Tool {tool_name} completed")
                    
                    tool_calls_made.append({
                        "tool": tool_name,
                        "arguments": tool_args,
                        "result": tool_result
                    })
                
                # å°‡å·¥å…·çµæœåŠ å…¥ä¸Šä¸‹æ–‡ï¼Œå†æ¬¡èª¿ç”¨ LLM
                tool_results_prompt = "\n\n".join([
                    f"å·¥å…· {tc['tool']} çµæœï¼š\n{json.dumps(tc['result'], ensure_ascii=False, indent=2)}"
                    for tc in tool_calls_made
                ])
                
                messages.append({"role": "assistant", "content": message.content or ""})
                messages.append({"role": "user", "content": f"æ ¹æ“šå·¥å…·åŸ·è¡Œçµæœï¼Œè«‹ç¸½çµå›ç­”ï¼š\n\n{tool_results_prompt}"})
                
                logger.info(f"ğŸ“¡ Calling LLM for final answer...")
                final_response = await self._llm_client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=0.7
                )
                answer = final_response.choices[0].message.content
                
                # ç´¯åŠ ç¬¬äºŒæ¬¡èª¿ç”¨çš„ token
                if final_response.usage:
                    total_usage["prompt_tokens"] += final_response.usage.prompt_tokens
                    total_usage["completion_tokens"] += final_response.usage.completion_tokens
                    total_usage["total_tokens"] += final_response.usage.total_tokens
            else:
                answer = message.content
            
            # æ›´æ–°è¨˜æ†¶
            self._memory.append({"role": "user", "content": prompt})
            self._memory.append({"role": "assistant", "content": answer})
            
            # è¨ˆç®—æˆæœ¬ä¼°ç®— (GPT-4o åƒ¹æ ¼: $2.50/1M input, $10/1M output)
            estimated_cost = (
                total_usage["prompt_tokens"] * 0.0000025 +
                total_usage["completion_tokens"] * 0.00001
            )
            total_usage["estimated_cost_usd"] = round(estimated_cost, 6)
            
            logger.info(f"âœ… {self.name} completed thinking")
            logger.info(f"ğŸ“Š Token usage: {total_usage['total_tokens']} (${total_usage['estimated_cost_usd']:.4f})")
            logger.debug(f"ğŸ“ Answer: {answer[:200] if answer else 'None'}...")
            
            return {
                "answer": answer,
                "tool_calls": tool_calls_made,
                "usage": total_usage
            }
            
        except Exception as e:
            logger.error(f"âŒ {self.name} think error: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return {"error": str(e), "answer": "", "usage": total_usage}
    
    def clear_memory(self):
        """æ¸…ç©ºè¨˜æ†¶"""
        self._memory = []
    
    def to_dict(self) -> Dict[str, Any]:
        """è½‰æ›ç‚ºå­—å…¸"""
        return {
            "id": self.id,
            "type": self.type.value,
            "name": self.name,
            "model": self.model,
            "available_tools": self._available_tools
        }
