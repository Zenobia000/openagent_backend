"""
ç¸½æ©Ÿ Agentï¼ˆDispatcherï¼‰

è² è²¬ï¼š
1. åˆ†æžç”¨æˆ¶éœ€æ±‚
2. æ™ºèƒ½åˆ¤æ–·ï¼šç°¡å–®æŸ¥è©¢ç›´æŽ¥ RAGï¼Œè¤‡é›œä»»å‹™å•Ÿå‹•å¤š Agent
3. æ‹†è§£ç‚ºå­ä»»å‹™
4. åˆ†é…çµ¦å°ˆæ¥­ Agent
"""

import json
import logging
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
import time

from .base import BaseAgent, AgentType, AgentTask, AgentResult

logger = logging.getLogger(__name__)


@dataclass
class TaskPlan:
    """ä»»å‹™è¨ˆåŠƒ"""
    original_request: str
    analysis: str
    is_simple_query: bool = False  # æ˜¯å¦ç‚ºç°¡å–®æŸ¥è©¢ï¼ˆç›´æŽ¥ RAGï¼‰
    subtasks: List[Dict[str, Any]] = field(default_factory=list)
    usage: Dict[str, Any] = field(default_factory=dict)  # Token ä½¿ç”¨é‡
    # subtask format: {"id": "1", "agent": "researcher", "task": "...", "depends_on": []}


class DispatcherAgent(BaseAgent):
    """
    ç¸½æ©Ÿ Agent
    
    æ™ºèƒ½åˆ¤æ–·å•é¡Œé¡žåž‹ï¼š
    - ç°¡å–®æŸ¥è©¢ï¼šç›´æŽ¥ä½¿ç”¨ RAG æœå°‹çŸ¥è­˜åº«
    - è¤‡é›œä»»å‹™ï¼šå¤š Agent å”ä½œ
    """
    
    def __init__(self):
        super().__init__(AgentType.DISPATCHER, "Dispatcher")
        self.model = "gpt-4o"  # ç¸½æ©Ÿä½¿ç”¨è¼ƒå¼·çš„æ¨¡åž‹
    
    @property
    def system_prompt(self) -> str:
        return """ä½ æ˜¯ä¸€å€‹æ™ºèƒ½ä»»å‹™èª¿åº¦ç³»çµ±çš„ç¸½æ©Ÿ Agentã€‚

ä½ çš„æ ¸å¿ƒè·è²¬æ˜¯ï¼š**åˆ¤æ–·å•é¡Œé¡žåž‹ï¼Œé¸æ“‡æœ€é«˜æ•ˆçš„è™•ç†æ–¹å¼**

## å£èªžåŒ–ç†è§£ï¼ˆéžå¸¸é‡è¦ï¼ï¼‰

ç”¨æˆ¶ç¶“å¸¸ä½¿ç”¨å£èªžåŒ–çš„è¡¨é”ï¼Œä½ éœ€è¦ç†è§£å…¶çœŸæ­£æ„åœ–ï¼š

**ã€Œé€™ç¯‡ã€ã€Œé€™å€‹ã€ã€Œé€™ä»½ã€** â†’ æŒ‡ç”¨æˆ¶é¸ä¸­çš„æ–‡ä»¶ï¼ˆåœ¨ context.selected_docs ä¸­ï¼‰
**ã€Œæ•´ç†ã€ã€Œç¸½çµã€ã€Œæ‘˜è¦ã€** â†’ å°æ–‡ä»¶å…§å®¹é€²è¡Œæ­¸ç´æ•´ç†
**ã€Œè¬›ä»€éº¼ã€ã€Œåœ¨èªªä»€éº¼ã€ã€Œä¸»è¦å…§å®¹ã€** â†’ è©¢å•æ–‡ä»¶çš„ä¸»é¡Œå’Œé‡é»ž
**ã€Œæœ‰æ²’æœ‰æåˆ°ã€ã€Œè¬›åˆ°...å—Žã€** â†’ æœå°‹ç‰¹å®šå…§å®¹
**ã€Œå¹«æˆ‘çœ‹çœ‹ã€ã€Œå¹«æˆ‘æŸ¥ã€** â†’ æœå°‹ä¸¦åˆ†æž

## ðŸŒ ç¶²è·¯æœå°‹åˆ¤æ–·ï¼ˆé‡è¦ï¼ï¼‰

**ç•¶ç”¨æˆ¶æ˜Žç¢ºè¦æ±‚ç¶²è·¯è³‡è¨Šæ™‚ï¼Œå¿…é ˆä½¿ç”¨ web_search è€Œä¸æ˜¯ rag_searchï¼š**

è§¸ç™¼é—œéµè©žï¼š
- ã€Œç¶²è·¯ä¸Šã€ã€Œç·šä¸Šã€ã€Œäº’è¯ç¶²ã€ã€Œç¶²ä¸Šã€
- ã€Œæœ€æ–°çš„ã€ã€Œæœ€è¿‘çš„ã€ã€Œä»Šå¤©çš„ã€ã€Œå³æ™‚ã€
- ã€Œæ–°èžã€ã€Œå‹•æ…‹ã€ã€Œè¶¨å‹¢ã€
- ã€Œæœå°‹ç¶²è·¯ã€ã€Œä¸Šç¶²æŸ¥ã€ã€ŒGoogleã€

ç¯„ä¾‹ï¼š
- "çµ¦æˆ‘ç¶²è·¯ä¸Šé—œæ–¼ XX çš„è³‡è¨Š" â†’ ä½¿ç”¨ web_search
- "é€™ç¯‡è«–æ–‡åœ¨ç¶²è·¯ä¸Šçš„è©•åƒ¹å¦‚ä½•" â†’ ä½¿ç”¨ web_search
- "æœ€æ–°çš„ AI ç™¼å±•è¶¨å‹¢" â†’ ä½¿ç”¨ web_search
- "å¹«æˆ‘æŸ¥æŸ¥é€™ç¯‡è«–æ–‡è¬›ä»€éº¼" â†’ ä½¿ç”¨ rag_searchï¼ˆæŸ¥æœ¬åœ°æ–‡ä»¶ï¼‰

## å•é¡Œåˆ†é¡ž

**ç°¡å–®æŸ¥è©¢**ï¼ˆåªéœ€è¦æœå°‹çŸ¥è­˜åº«ï¼Œis_simple_query=trueï¼‰ï¼š
- è©¢å•æ–‡ä»¶å…§å®¹ï¼š"é€™ç¯‡æ–‡ç« åœ¨è¬›ä»€éº¼"ã€"æœ‰æ²’æœ‰æåˆ° XX"
- äº‹å¯¦æ€§å•é¡Œï¼š"XX çš„å®šç¾©æ˜¯ä»€éº¼"ã€"é€™å€‹æ•¸æ“šæ˜¯å¤šå°‘"
- ç°¡å–®æ‘˜è¦ï¼š"å¹«æˆ‘ç¸½çµé€™ä»½æ–‡ä»¶"ã€"æ•´ç†é€™ç¯‡è«–æ–‡"

**ç¶²è·¯æœå°‹**ï¼ˆéœ€è¦æœå°‹ç¶²è·¯ï¼Œis_simple_query=trueï¼Œä½†æŒ‡å®š use_web_search=trueï¼‰ï¼š
- ç¶²è·¯è³‡è¨Šï¼š"ç¶²è·¯ä¸Šé—œæ–¼ XX çš„è³‡è¨Š"ã€"ç·šä¸Šè©•åƒ¹"
- æœ€æ–°è³‡è¨Šï¼š"æœ€æ–°çš„ XX"ã€"ä»Šå¤©çš„æ–°èž"

**è¤‡é›œä»»å‹™**ï¼ˆéœ€è¦å¤šæ­¥é©Ÿå”ä½œï¼Œis_simple_query=falseï¼‰ï¼š
- ç ”ç©¶ + å¯«ä½œï¼š"ç ”ç©¶ XX è¶¨å‹¢ä¸¦å¯«ä¸€ä»½å ±å‘Š"
- åˆ†æž + å»ºè­°ï¼š"åˆ†æžæ•¸æ“šä¸¦çµ¦å‡ºæ”¹é€²å»ºè­°"
- **ç¨‹å¼ + è¨ˆç®—**ï¼š"è¨ˆç®— XX"ã€"ç”¨ Python è¨ˆç®—"ã€"ç•«å‡ºåœ–è¡¨"ã€"ç¹ªè£½è¶¨å‹¢åœ–"
- ç¨‹å¼ + æ¸¬è©¦ï¼š"å¯«ä¸€å€‹æ¼”ç®—æ³•ä¸¦æ¸¬è©¦"
- å¤šä¾†æºæ•´åˆï¼š"æ¯”è¼ƒ A å’Œ B çš„å„ªç¼ºé»ž"

**ç‰¹åˆ¥æ³¨æ„**ï¼šç•¶ç”¨æˆ¶è¦æ±‚ã€Œè¨ˆç®—ã€ã€ã€Œç”¨ Pythonã€ã€ã€Œç•«åœ–ã€ã€ã€Œç¹ªè£½ã€æ™‚ï¼Œå¿…é ˆåˆ†é…çµ¦ **coder** agentï¼

## å¯ç”¨çš„å°ˆæ¥­ Agent

- **researcher**: ç ”ç©¶è€… - æœå°‹çŸ¥è­˜åº«(RAG)ã€**ç¶²è·¯æœå°‹(web_search)**ã€æ•´ç†ä¿¡æ¯
- **writer**: å¯«ä½œè€… - æ’°å¯«æ–‡ç« ã€å ±å‘Šã€æ–‡æª”
- **coder**: ç·¨ç¢¼è€… - ç·¨å¯«ç¨‹å¼ç¢¼ã€åŸ·è¡Œæ¸¬è©¦
- **analyst**: åˆ†æžå¸« - æ•¸æ“šåˆ†æžã€çµ±è¨ˆè¨ˆç®—
- **reviewer**: å¯©æ ¸è€… - å¯©æ ¸å“è³ªã€æ”¹é€²å»ºè­°

## è¼¸å‡ºæ ¼å¼ï¼ˆJSONï¼‰

ç°¡å–®æŸ¥è©¢ï¼ˆæœ¬åœ° RAGï¼‰ï¼š
{
  "analysis": "ç”¨æˆ¶æƒ³è¦äº†è§£é¸ä¸­æ–‡ä»¶çš„å…§å®¹",
  "is_simple_query": true,
  "subtasks": [
    {
      "id": "1",
      "agent": "researcher",
      "task": "æœå°‹ä¸¦æ•´ç†æ–‡ä»¶å…§å®¹",
      "description": "æœå°‹çŸ¥è­˜åº«ä¸­çš„ç›¸é—œå…§å®¹ä¸¦é€²è¡Œæ•´ç†æ‘˜è¦",
      "use_web_search": false,
      "depends_on": []
    }
  ]
}

ç¶²è·¯æœå°‹ï¼š
{
  "analysis": "ç”¨æˆ¶æƒ³è¦ç²å–ç¶²è·¯ä¸Šçš„ç›¸é—œè³‡è¨Š",
  "is_simple_query": true,
  "subtasks": [
    {
      "id": "1",
      "agent": "researcher",
      "task": "æœå°‹ç¶²è·¯è³‡è¨Š",
      "description": "ä½¿ç”¨ web_search æœå°‹ç¶²è·¯ä¸Šçš„ç›¸é—œè³‡è¨Šä¸¦æ•´ç†",
      "use_web_search": true,
      "search_query": "CLIP è«–æ–‡ è©•åƒ¹ å½±éŸ¿",
      "depends_on": []
    }
  ]
}

## é‡è¦åŽŸå‰‡

1. **ç†è§£å£èªž**ï¼šç”¨æˆ¶èªªã€Œé€™ç¯‡ã€å°±æ˜¯æŒ‡ selected_docs ä¸­çš„æ–‡ä»¶
2. **å€åˆ†æœå°‹ä¾†æº**ï¼šã€Œç¶²è·¯ä¸Šã€â†’ web_searchï¼Œã€Œæ–‡ä»¶ä¸­ã€â†’ rag_search
3. **æ•ˆçŽ‡å„ªå…ˆ**ï¼šèƒ½ç”¨ RAG ç›´æŽ¥è§£æ±ºçš„ï¼Œè¨­ç½® is_simple_query=true
4. **ä¸è¦è¿½å•**ï¼šå¦‚æžœç”¨æˆ¶å·²é¸ä¸­æ–‡ä»¶ä¸¦èªªã€Œæ•´ç†é€™ç¯‡ã€ï¼Œç›´æŽ¥åŸ·è¡Œï¼Œä¸è¦è¦æ±‚æ›´å¤šç´°ç¯€
5. **ä»»å‹™ç²¾ç°¡**ï¼šä¸è¦éŽåº¦æ‹†åˆ†ï¼Œ2-4 å€‹æ­¥é©Ÿæœ€ä½³
"""
    
    async def analyze_request(self, user_request: str, context: Dict = None) -> TaskPlan:
        """
        åˆ†æžç”¨æˆ¶è«‹æ±‚ï¼Œç”Ÿæˆä»»å‹™è¨ˆåŠƒ
        
        Args:
            user_request: ç”¨æˆ¶è«‹æ±‚
            context: ä¸Šä¸‹æ–‡ï¼ˆå¦‚é¸ä¸­çš„æ–‡ä»¶ã€çŸ¥è­˜åº«ç­‰ï¼‰
            
        Returns:
            ä»»å‹™è¨ˆåŠƒ
        """
        prompt = f"""ç”¨æˆ¶è«‹æ±‚ï¼š{user_request}

{"ä¸Šä¸‹æ–‡ï¼š" + json.dumps(context, ensure_ascii=False) if context else ""}

è«‹åˆ†æžé€™å€‹è«‹æ±‚ï¼š
1. åˆ¤æ–·æ˜¯ã€Œç°¡å–®æŸ¥è©¢ã€é‚„æ˜¯ã€Œè¤‡é›œä»»å‹™ã€
2. å¦‚æžœæ˜¯ç°¡å–®æŸ¥è©¢ï¼Œè¨­ç½® is_simple_query=trueï¼Œåªåˆ†é…çµ¦ researcher
3. å¦‚æžœæ˜¯è¤‡é›œä»»å‹™ï¼Œæ‹†è§£ç‚ºå¤šå€‹å­ä»»å‹™

è¼¸å‡º JSON æ ¼å¼ã€‚"""

        result = await self.think(prompt, use_tools=False)
        usage = result.get("usage", {})
        
        try:
            # è§£æž JSON
            answer = result.get("answer", "")
            # æå– JSON
            if "```json" in answer:
                json_str = answer.split("```json")[1].split("```")[0]
            elif "```" in answer:
                json_str = answer.split("```")[1].split("```")[0]
            else:
                json_str = answer
            
            plan_data = json.loads(json_str.strip())
            
            return TaskPlan(
                original_request=user_request,
                analysis=plan_data.get("analysis", ""),
                is_simple_query=plan_data.get("is_simple_query", False),
                subtasks=plan_data.get("subtasks", []),
                usage=usage
            )
        except Exception as e:
            logger.error(f"Failed to parse task plan: {e}")
            # é è¨­ç‚ºç°¡å–®æŸ¥è©¢
            return TaskPlan(
                original_request=user_request,
                analysis="é è¨­ç‚ºçŸ¥è­˜æŸ¥è©¢",
                is_simple_query=True,
                subtasks=[{
                    "id": "1",
                    "agent": "researcher",
                    "task": "æœå°‹çŸ¥è­˜åº«",
                    "description": user_request,
                    "depends_on": []
                }],
                usage=usage
            )
    
    async def process_task(self, task: AgentTask) -> AgentResult:
        """è™•ç†ä»»å‹™ï¼ˆç”Ÿæˆä»»å‹™è¨ˆåŠƒï¼‰"""
        start_time = time.time()
        
        user_request = task.parameters.get("request", "")
        context = task.context
        
        plan = await self.analyze_request(user_request, context)
        
        return AgentResult(
            task_id=task.id,
            agent_type=self.type.value,
            success=len(plan.subtasks) > 0,
            output={
                "analysis": plan.analysis,
                "is_simple_query": plan.is_simple_query,
                "subtasks": plan.subtasks,
                "total_steps": len(plan.subtasks)
            },
            thinking=plan.analysis,
            execution_time=time.time() - start_time,
            usage=plan.usage
        )
