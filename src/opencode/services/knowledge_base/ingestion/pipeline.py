"""
Ingestion Pipeline - æ–‡ä»¶è™•ç†ç®¡ç·š
å¾ rag-project é·ç§»ä¸¦å¢å¼·
"""

import logging
import os
from typing import List, Dict, Any, Optional
from pathlib import Path

# ä½¿ç”¨çµ±ä¸€çš„è·¯å¾‘å·¥å…·è¼‰å…¥ç’°å¢ƒè®Šæ•¸
from opencode.core.utils import load_env, get_project_root
load_env()

logger = logging.getLogger(__name__)


class DocumentChunk:
    """æ–‡ä»¶å€å¡Š"""
    
    def __init__(
        self,
        text: str,
        metadata: Dict[str, Any] = None
    ):
        self.text = text
        self.metadata = metadata or {}
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "text": self.text,
            "metadata": self.metadata
        }


class PDFParser:
    """PDF è§£æå™¨"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.chunk_size = self.config.get("chunk_size", 1000)
        self.chunk_overlap = self.config.get("chunk_overlap", 200)
    
    def parse(self, file_path: str) -> List[Dict[str, Any]]:
        """
        è§£æ PDF æª”æ¡ˆ
        
        Args:
            file_path: PDF æª”æ¡ˆè·¯å¾‘
            
        Returns:
            æ–‡ä»¶å€å¡Šåˆ—è¡¨
        """
        file_name = os.path.basename(file_path)
        logger.info(f"ğŸ“„ ====== é–‹å§‹è§£æ PDF ======")
        logger.info(f"ğŸ“„ æª”æ¡ˆ: {file_name}")
        logger.info(f"ğŸ“„ è·¯å¾‘: {file_path}")
        
        # ç›´æ¥ä½¿ç”¨ fallback (pymupdf/PyPDF2)ï¼Œå› ç‚ºæ›´ç©©å®š
        documents = self._parse_with_fallback(file_path, file_name)
        
        if documents:
            return documents
        
        # å¦‚æœ fallback å¤±æ•—ï¼Œå˜—è©¦ docling
        try:
            logger.info("ğŸ“„ å˜—è©¦ä½¿ç”¨ Docling...")
            return self._parse_with_docling(file_path, file_name)
        except Exception as e:
            logger.error(f"âŒ Docling ä¹Ÿå¤±æ•—äº†: {e}")
            return []
    
    def _parse_with_docling(
        self, 
        file_path: str, 
        file_name: str
    ) -> List[Dict[str, Any]]:
        """ä½¿ç”¨ Docling è§£æ"""
        from docling.document_converter import DocumentConverter
        
        converter = DocumentConverter()
        result = converter.convert(file_path)
        
        documents = []
        
        # æŒ‰é é¢è™•ç†
        for page_idx, page in enumerate(result.document.pages, 1):
            page_text = page.text if hasattr(page, 'text') else str(page)
            
            if not page_text.strip():
                continue
            
            # åˆ†å¡Š
            chunks = self._chunk_text(page_text)
            
            for chunk_idx, chunk in enumerate(chunks):
                documents.append({
                    "text": chunk,
                    "metadata": {
                        "file_name": file_name,
                        "page_label": str(page_idx),
                        "chunk_index": chunk_idx
                    }
                })
        
        logger.info(f"âœ… Parsed {len(documents)} chunks from {file_name}")
        return documents
    
    def _parse_with_fallback(
        self, 
        file_path: str, 
        file_name: str
    ) -> List[Dict[str, Any]]:
        """ä½¿ç”¨ pymupdf (fitz) è§£æ PDF"""
        documents = []
        
        # å„ªå…ˆä½¿ç”¨ pymupdf
        try:
            import fitz  # pymupdf
            
            logger.info(f"ğŸ“– ä½¿ç”¨ PyMuPDF è§£æ: {file_path}")
            
            doc = fitz.open(file_path)
            total_pages = len(doc)
            logger.info(f"ğŸ“– PDF å…± {total_pages} é ")
            
            for page_idx in range(total_pages):
                page = doc[page_idx]
                page_text = page.get_text()
                
                logger.info(f"ğŸ“– ç¬¬ {page_idx + 1} é : {len(page_text)} å­—ç¬¦")
                
                if not page_text.strip():
                    logger.warning(f"âš ï¸ ç¬¬ {page_idx + 1} é æ²’æœ‰æ–‡å­—")
                    continue
                
                # åˆ†å¡Š
                chunks = self._chunk_text(page_text)
                logger.info(f"ğŸ“– ç¬¬ {page_idx + 1} é åˆ†æˆ {len(chunks)} å€‹å¡Š")
                
                for chunk_idx, chunk in enumerate(chunks):
                    if chunk.strip():  # ç¢ºä¿ä¸æ˜¯ç©ºç™½
                        documents.append({
                            "text": chunk,
                            "metadata": {
                                "file_name": file_name,
                                "page_label": str(page_idx + 1),
                                "chunk_index": chunk_idx
                            }
                        })
            
            doc.close()
            logger.info(f"âœ… PyMuPDF è§£æå®Œæˆ: {len(documents)} å€‹æ–‡å­—å¡Š")
            return documents
            
        except ImportError:
            logger.warning("âš ï¸ PyMuPDF ä¸å¯ç”¨ï¼Œå˜—è©¦ PyPDF2")
        except Exception as e:
            logger.error(f"âŒ PyMuPDF è§£æå¤±æ•—: {e}")
        
        # å‚™ç”¨: PyPDF2
        try:
            import PyPDF2
            
            logger.info(f"ğŸ“– ä½¿ç”¨ PyPDF2 è§£æ: {file_path}")
            
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                total_pages = len(reader.pages)
                logger.info(f"ğŸ“– PDF å…± {total_pages} é ")
                
                for page_idx, page in enumerate(reader.pages):
                    page_text = page.extract_text()
                    
                    logger.info(f"ğŸ“– ç¬¬ {page_idx + 1} é : {len(page_text) if page_text else 0} å­—ç¬¦")
                    
                    if not page_text or not page_text.strip():
                        logger.warning(f"âš ï¸ ç¬¬ {page_idx + 1} é æ²’æœ‰æ–‡å­—")
                        continue
                    
                    chunks = self._chunk_text(page_text)
                    logger.info(f"ğŸ“– ç¬¬ {page_idx + 1} é åˆ†æˆ {len(chunks)} å€‹å¡Š")
                    
                    for chunk_idx, chunk in enumerate(chunks):
                        if chunk.strip():
                            documents.append({
                                "text": chunk,
                                "metadata": {
                                    "file_name": file_name,
                                    "page_label": str(page_idx + 1),
                                    "chunk_index": chunk_idx
                                }
                            })
            
            logger.info(f"âœ… PyPDF2 è§£æå®Œæˆ: {len(documents)} å€‹æ–‡å­—å¡Š")
            return documents
            
        except ImportError:
            logger.error("âŒ PyPDF2 ä¹Ÿä¸å¯ç”¨")
        except Exception as e:
            logger.error(f"âŒ PyPDF2 è§£æå¤±æ•—: {e}")
        
        return []
    
    def _chunk_text(self, text: str) -> List[str]:
        """å°‡æ–‡å­—åˆ†å¡Š"""
        if len(text) <= self.chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.chunk_size
            
            # å˜—è©¦åœ¨å¥å­é‚Šç•Œåˆ†å‰²
            if end < len(text):
                # å°‹æ‰¾æœ€è¿‘çš„å¥è™Ÿæˆ–æ›è¡Œ
                for sep in ['\n\n', '\n', 'ã€‚', '.', 'ï¼', '!', 'ï¼Ÿ', '?']:
                    last_sep = text.rfind(sep, start, end)
                    if last_sep > start:
                        end = last_sep + 1
                        break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            # ä¸‹ä¸€å¡Šå¾ overlap ä½ç½®é–‹å§‹
            start = end - self.chunk_overlap
            if start < 0:
                start = 0
        
        return chunks


class Indexer:
    """å‘é‡ç´¢å¼•å™¨"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.qdrant_host = self.config.get("qdrant_host", "localhost")
        self.qdrant_port = self.config.get("qdrant_port", 6333)
        self.collection_name = self.config.get("collection", "rag_knowledge_base")
        self.embedding_model = self.config.get("embedding_model", "text-embedding-3-small")
        
        self.qdrant_client = None
        self.openai_client = None
    
    def _init_clients(self):
        """åˆå§‹åŒ–å®¢æˆ¶ç«¯"""
        if self.qdrant_client is None:
            from qdrant_client import QdrantClient
            from openai import OpenAI
            from opencode.core.utils import get_env_path
            
            # å¼·åˆ¶é‡æ–°è¼‰å…¥ .envï¼ˆèƒŒæ™¯ä»»å‹™å¯èƒ½éœ€è¦ï¼‰
            load_env()
            
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                env_path = get_env_path()
                logger.error(f"âŒ OPENAI_API_KEY æœªè¨­ç½®ï¼.env è·¯å¾‘: {env_path}, å­˜åœ¨: {env_path.exists()}")
                raise ValueError(f"OPENAI_API_KEY æœªè¨­ç½®ã€‚è«‹ç¢ºèª {env_path} æª”æ¡ˆå­˜åœ¨ä¸”åŒ…å« OPENAI_API_KEY=sk-xxx")
            
            self.qdrant_client = QdrantClient(
                host=self.qdrant_host,
                port=self.qdrant_port
            )
            self.openai_client = OpenAI(api_key=api_key)
            
            self._ensure_collection()
    
    def _ensure_collection(self):
        """ç¢ºä¿ collection å­˜åœ¨"""
        from qdrant_client.models import VectorParams, Distance
        
        try:
            self.qdrant_client.get_collection(self.collection_name)
        except:
            logger.info(f"ğŸ”§ Creating collection: {self.collection_name}")
            self.qdrant_client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=1536,
                    distance=Distance.COSINE
                )
            )
    
    def get_embedding(self, text: str) -> List[float]:
        """å–å¾—æ–‡å­—å‘é‡"""
        text = text.replace("\n", " ")
        response = self.openai_client.embeddings.create(
            input=[text],
            model=self.embedding_model
        )
        return response.data[0].embedding
    
    def index_documents(self, documents: List[Dict[str, Any]]) -> None:
        """
        å°‡æ–‡ä»¶ç´¢å¼•åˆ° Qdrant
        
        Args:
            documents: æ–‡ä»¶åˆ—è¡¨
        """
        if not documents:
            logger.warning("âš ï¸ æ²’æœ‰æ–‡ä»¶éœ€è¦ç´¢å¼•")
            return
        
        self._init_clients()
        
        from qdrant_client.models import PointStruct
        import uuid
        
        logger.info(f"ğŸ’¾ ====== é–‹å§‹ç´¢å¼• ======")
        logger.info(f"ğŸ’¾ æ–‡ä»¶æ•¸é‡: {len(documents)}")
        
        # è¨˜éŒ„ç¬¬ä¸€å€‹æ–‡ä»¶çš„çµæ§‹
        if documents:
            sample = documents[0]
            logger.info(f"ğŸ’¾ æ–‡ä»¶çµæ§‹ç¯„ä¾‹: keys={list(sample.keys())}")
            logger.info(f"ğŸ’¾ metadata: {sample.get('metadata', {})}")
        
        points = []
        for i, doc in enumerate(documents):
            text = doc.get("text", "")
            if not text.strip():
                continue
            
            try:
                vector = self.get_embedding(text)
                
                metadata = doc.get("metadata", {})
                payload = {
                    "text": text,
                    **metadata
                }
                
                # è¨˜éŒ„å‰3å€‹çš„ payload
                if i < 3:
                    logger.info(f"ğŸ’¾ [{i+1}] payload_keys={list(payload.keys())}, file_name={payload.get('file_name', 'MISSING')}")
                
                points.append(PointStruct(
                    id=str(uuid.uuid4()),
                    vector=vector,
                    payload=payload
                ))
                
            except Exception as e:
                logger.error(f"âŒ Embedding failed for doc {i}: {e}")
        
        if points:
            self.qdrant_client.upsert(
                collection_name=self.collection_name,
                points=points
            )
            logger.info(f"âœ… æˆåŠŸç´¢å¼• {len(points)} å€‹æ–‡ä»¶åˆ° Qdrant")
        else:
            logger.warning("âš ï¸ æ²’æœ‰æˆåŠŸç”Ÿæˆä»»ä½• embedding")


def run_ingestion(file_path: str, config: Optional[Dict[str, Any]] = None) -> None:
    """
    åŸ·è¡Œæ–‡ä»¶è™•ç†ç®¡ç·š
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        config: é…ç½®
    """
    config = config or {}
    
    logger.info(f"ğŸš€ Starting ingestion: {file_path}")
    
    try:
        # è§£æ
        parser = PDFParser(config.get("parser", {}))
        documents = parser.parse(file_path)
        
        if not documents:
            logger.warning("âš ï¸ No documents parsed")
            return
        
        # ç´¢å¼•
        indexer = Indexer(config.get("indexer", {}))
        indexer.index_documents(documents)
        
        logger.info("âœ… Ingestion complete")
        
    except Exception as e:
        logger.error(f"âŒ Ingestion failed: {e}")
        raise


async def process_pdf_to_qdrant(
    file_path: str, 
    original_filename: str = None,
    config: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    è™•ç† PDF ä¸¦ç´¢å¼•åˆ° Qdrant (ç•°æ­¥ç‰ˆæœ¬)
    
    Args:
        file_path: PDF æª”æ¡ˆè·¯å¾‘
        original_filename: åŸå§‹æª”å
        config: é…ç½®
        
    Returns:
        è™•ç†çµæœ
    """
    import asyncio
    
    config = config or {}
    
    if original_filename is None:
        original_filename = os.path.basename(file_path)
    
    logger.info(f"ğŸš€ Processing: {original_filename}")
    
    try:
        # è§£æ PDF
        parser = PDFParser(config.get("parser", {}))
        documents = parser.parse(file_path)
        
        if not documents:
            logger.warning("âš ï¸ No content extracted from PDF")
            return {"chunks": 0, "error": "No content extracted"}
        
        # æ›´æ–°æª”ååˆ° metadata
        for doc in documents:
            doc["metadata"]["file_name"] = original_filename
        
        # ç´¢å¼•åˆ° Qdrant (åœ¨åŸ·è¡Œç·’ä¸­åŸ·è¡ŒåŒæ­¥æ“ä½œ)
        indexer = Indexer(config.get("indexer", {}))
        
        # ä½¿ç”¨ run_in_executor é¿å…é˜»å¡
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(None, indexer.index_documents, documents)
        
        logger.info(f"âœ… Processed {len(documents)} chunks from {original_filename}")
        
        return {
            "success": True,
            "filename": original_filename,
            "chunks": len(documents),
            "pages": len(set(d["metadata"].get("page_label") for d in documents))
        }
        
    except Exception as e:
        logger.error(f"âŒ Processing failed: {e}")
        raise
