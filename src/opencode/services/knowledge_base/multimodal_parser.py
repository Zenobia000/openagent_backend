"""
å¤šæ¨¡æ…‹æ–‡ä»¶è§£æå™¨

æ”¯æ´ï¼š
- PDFï¼ˆå«åœ–ç‰‡ OCRã€è¡¨æ ¼ï¼‰
- Word (.docx)
- Excel (.xlsx, .csv)
- Markdown (.md)
- ç´”æ–‡å­— (.txt)
- JSON (.json)
- ç¨‹å¼ç¢¼ (.py, .js, .ts, .java, .cpp ç­‰)

åœ–ç‰‡è™•ç†ï¼šä½¿ç”¨ Docling OCR æå–åœ–ç‰‡ä¸­çš„æ–‡å­—
"""

import os
import io
import json
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path

logger = logging.getLogger(__name__)

# æ”¯æ´çš„æ–‡ä»¶æ ¼å¼
SUPPORTED_FORMATS = {
    # æ–‡æª”é¡
    '.pdf': 'pdf',
    '.docx': 'word',
    '.doc': 'word',
    '.xlsx': 'excel',
    '.xls': 'excel',
    '.csv': 'csv',
    '.tsv': 'csv',
    
    # æ–‡å­—é¡
    '.txt': 'text',
    '.md': 'markdown',
    '.markdown': 'markdown',
    '.rst': 'text',
    
    # æ•¸æ“šé¡
    '.json': 'json',
    '.jsonl': 'json',
    '.yaml': 'yaml',
    '.yml': 'yaml',
    '.xml': 'xml',
    
    # ç¨‹å¼ç¢¼é¡
    '.py': 'code',
    '.js': 'code',
    '.ts': 'code',
    '.jsx': 'code',
    '.tsx': 'code',
    '.java': 'code',
    '.cpp': 'code',
    '.c': 'code',
    '.h': 'code',
    '.hpp': 'code',
    '.cs': 'code',
    '.go': 'code',
    '.rs': 'code',
    '.rb': 'code',
    '.php': 'code',
    '.swift': 'code',
    '.kt': 'code',
    '.scala': 'code',
    '.r': 'code',
    '.sql': 'code',
    '.sh': 'code',
    '.bash': 'code',
    '.ps1': 'code',
    '.html': 'code',
    '.css': 'code',
    '.scss': 'code',
    '.less': 'code',
}


class MultimodalParser:
    """
    å¤šæ¨¡æ…‹æ–‡ä»¶è§£æå™¨
    
    ç‰¹è‰²ï¼š
    1. è‡ªå‹•è­˜åˆ¥æ–‡ä»¶é¡å‹
    2. PyMuPDF + Tesseract OCR æå–åœ–ç‰‡æ–‡å­—
    3. è¡¨æ ¼çµæ§‹åŒ–æå–
    4. æ”¯æ´ 15+ ç¨®æ–‡ä»¶æ ¼å¼
    """
    
    def __init__(
        self,
        chunk_size: int = 512,
        chunk_overlap: int = 50,
        enable_ocr: bool = True
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.enable_ocr = enable_ocr
        
        # æª¢æŸ¥ OCR å·¥å…·
        self._ocr_available = False
        self._init_ocr()
    
    def _init_ocr(self):
        """åˆå§‹åŒ– OCR å·¥å…·"""
        # æ–¹æ³•1ï¼šå˜—è©¦ä½¿ç”¨ pytesseract
        try:
            import pytesseract
            from PIL import Image
            # æ¸¬è©¦ tesseract æ˜¯å¦å¯ç”¨
            pytesseract.get_tesseract_version()
            self._ocr_available = True
            self._ocr_method = "pytesseract"
            logger.info("âœ… [MultimodalParser] Tesseract OCR åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ [MultimodalParser] Tesseract OCR ä¸å¯ç”¨: {e}")
            
            # æ–¹æ³•2ï¼šå˜—è©¦ä½¿ç”¨ easyocr
            try:
                import easyocr
                self._ocr_reader = easyocr.Reader(['ch_tra', 'en'], gpu=False)
                self._ocr_available = True
                self._ocr_method = "easyocr"
                logger.info("âœ… [MultimodalParser] EasyOCR åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e2:
                logger.warning(f"âš ï¸ [MultimodalParser] EasyOCR ä¹Ÿä¸å¯ç”¨: {e2}")
                logger.info("ğŸ’¡ æç¤ºï¼šå®‰è£ OCR å·¥å…·ä¾†æå–åœ–ç‰‡æ–‡å­—")
                logger.info("   pip install pytesseract pillow")
                logger.info("   æˆ– pip install easyocr")
    
    def _ocr_image(self, image_bytes: bytes) -> str:
        """å°åœ–ç‰‡é€²è¡Œ OCR"""
        if not self._ocr_available:
            return ""
        
        try:
            if self._ocr_method == "pytesseract":
                import pytesseract
                from PIL import Image
                import io
                
                image = Image.open(io.BytesIO(image_bytes))
                # ä½¿ç”¨ä¸­æ–‡+è‹±æ–‡è­˜åˆ¥
                text = pytesseract.image_to_string(image, lang='chi_tra+eng')
                return text.strip()
                
            elif self._ocr_method == "easyocr":
                import numpy as np
                from PIL import Image
                import io
                
                image = Image.open(io.BytesIO(image_bytes))
                image_np = np.array(image)
                results = self._ocr_reader.readtext(image_np)
                text = " ".join([r[1] for r in results])
                return text.strip()
                
        except Exception as e:
            logger.warning(f"âš ï¸ [OCR] è­˜åˆ¥å¤±æ•—: {e}")
            return ""
        
        return ""
    
    def parse(self, file_path: str) -> List[Dict[str, Any]]:
        """
        è§£ææ–‡ä»¶ï¼ˆè‡ªå‹•è­˜åˆ¥é¡å‹ï¼‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾‘
            
        Returns:
            è§£æå¾Œçš„ chunks åˆ—è¡¨
        """
        if not os.path.exists(file_path):
            logger.error(f"âŒ [Parser] æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return []
        
        ext = Path(file_path).suffix.lower()
        file_type = SUPPORTED_FORMATS.get(ext)
        
        if not file_type:
            logger.warning(f"âš ï¸ [Parser] ä¸æ”¯æ´çš„æ–‡ä»¶æ ¼å¼: {ext}")
            file_type = 'text'
        
        logger.info(f"ğŸ“„ [Parser] è§£ææ–‡ä»¶: {file_path} (é¡å‹: {file_type})")
        
        # æ ¹æ“šé¡å‹èª¿ç”¨å°æ‡‰çš„è§£æå™¨
        parsers = {
            'pdf': self._parse_pdf,
            'word': self._parse_word,
            'excel': self._parse_excel,
            'csv': self._parse_csv,
            'text': self._parse_text,
            'markdown': self._parse_markdown,
            'json': self._parse_json,
            'yaml': self._parse_yaml,
            'xml': self._parse_xml,
            'code': self._parse_code,
        }
        
        parser_func = parsers.get(file_type, self._parse_text)
        return parser_func(file_path)
    
    def _parse_pdf(self, file_path: str) -> List[Dict[str, Any]]:
        """
        è§£æ PDF - ä½¿ç”¨ PyMuPDFï¼Œæ”¯æ´åœ–ç‰‡ OCR
        """
        try:
            import fitz  # PyMuPDF
        except ImportError:
            logger.error("âŒ [Parser] PyMuPDF æœªå®‰è£: pip install pymupdf")
            return []
        
        parsed_data = []
        file_name = os.path.basename(file_path)
        
        try:
            doc = fitz.open(file_path)
            total_pages = len(doc)
            logger.info(f"ğŸ“– [Parser] PDF å…± {total_pages} é ")
            
            for page_num in range(total_pages):
                page = doc[page_num]
                # é ç¢¼å¾ 1 é–‹å§‹
                page_label = str(page_num + 1)
                
                logger.info(f"ğŸ“„ [Parser] è§£æç¬¬ {page_label}/{total_pages} é ...")
                
                # 1. æå–æ–‡å­—å…§å®¹
                text = page.get_text("text").strip()
                if text and len(text) > 30:
                    chunks = self._split_text(text)
                    for chunk_idx, chunk in enumerate(chunks):
                        if len(chunk) > 30:
                            parsed_data.append({
                                "text": chunk,
                                "metadata": {
                                    "file_name": file_name,
                                    "page_label": page_label,
                                    "chunk_index": chunk_idx,
                                    "content_type": "text",
                                    "source": "pymupdf"
                                }
                            })
                    logger.info(f"   æ–‡å­—: {len(chunks)} å€‹å€å¡Š")
                
                # 2. æå–åœ–ç‰‡ä¸¦é€²è¡Œ OCR
                if self.enable_ocr:
                    images = page.get_images(full=True)
                    image_count = 0
                    
                    for img_idx, img in enumerate(images):
                        try:
                            xref = img[0]
                            base_image = doc.extract_image(xref)
                            image_bytes = base_image["image"]
                            
                            # è·³éå¤ªå°çš„åœ–ç‰‡ï¼ˆå¯èƒ½æ˜¯åœ–æ¨™ï¼‰
                            if len(image_bytes) < 3000:  # å°æ–¼ 3KB
                                continue
                            
                            # é€²è¡Œ OCR
                            ocr_text = self._ocr_image(image_bytes)
                            
                            if ocr_text and len(ocr_text) > 10:
                                parsed_data.append({
                                    "text": f"[åœ–ç‰‡å…§å®¹ - ç¬¬ {page_label} é  åœ– {img_idx + 1}]\n{ocr_text}",
                                    "metadata": {
                                        "file_name": file_name,
                                        "page_label": page_label,
                                        "content_type": "image_ocr",
                                        "image_index": img_idx,
                                        "source": "ocr"
                                    }
                                })
                                image_count += 1
                                
                        except Exception as e:
                            logger.debug(f"åœ–ç‰‡ {img_idx} è™•ç†å¤±æ•—: {e}")
                    
                    if image_count > 0:
                        logger.info(f"   åœ–ç‰‡ OCR: {image_count} å¼µ")
                
                # 3. æå–è¡¨æ ¼
                try:
                    tables = page.find_tables()
                    if tables and len(tables.tables) > 0:
                        for table_idx, table in enumerate(tables.tables):
                            table_data = table.extract()
                            if table_data:
                                table_text = self._format_table(table_data)
                                if table_text and len(table_text) > 30:
                                    parsed_data.append({
                                        "text": f"[è¡¨æ ¼ - ç¬¬ {page_label} é  è¡¨ {table_idx + 1}]\n{table_text}",
                                        "metadata": {
                                            "file_name": file_name,
                                            "page_label": page_label,
                                            "content_type": "table",
                                            "table_index": table_idx,
                                            "source": "pymupdf_table"
                                        }
                                    })
                        if len(tables.tables) > 0:
                            logger.info(f"   è¡¨æ ¼: {len(tables.tables)} å€‹")
                except Exception as e:
                    logger.debug(f"è¡¨æ ¼æå–å¤±æ•—: {e}")
            
            doc.close()
            
            # çµ±è¨ˆ
            content_types = {}
            for item in parsed_data:
                ct = item.get("metadata", {}).get("content_type", "text")
                content_types[ct] = content_types.get(ct, 0) + 1
            
            summary = ", ".join([f"{k}: {v}" for k, v in content_types.items()])
            logger.info(f"âœ… [Parser] PDF è§£æå®Œæˆ: {len(parsed_data)} å€‹å€å¡Š ({summary})")
            
        except Exception as e:
            logger.error(f"âŒ [Parser] PDF è§£æå¤±æ•—: {e}")
            import traceback
            logger.error(traceback.format_exc())
        
        return parsed_data
    
    def _format_table(self, table_data: List[List]) -> str:
        """å°‡è¡¨æ ¼æ•¸æ“šæ ¼å¼åŒ–ç‚ºæ–‡å­—"""
        if not table_data:
            return ""
        
        lines = []
        for row in table_data:
            row_text = " | ".join([str(cell) if cell else "" for cell in row])
            if row_text.strip():
                lines.append(row_text)
        
        return "\n".join(lines)
    
    def _parse_word(self, file_path: str) -> List[Dict[str, Any]]:
        """è§£æ Word æ–‡æª”"""
        try:
            from docx import Document
        except ImportError:
            logger.error("âŒ [Parser] python-docx æœªå®‰è£: pip install python-docx")
            return []
        
        parsed_data = []
        file_name = os.path.basename(file_path)
        
        try:
            doc = Document(file_path)
            full_text = []
            
            for para in doc.paragraphs:
                if para.text.strip():
                    full_text.append(para.text.strip())
            
            # è™•ç†è¡¨æ ¼
            for table in doc.tables:
                table_text = []
                for row in table.rows:
                    row_text = " | ".join([cell.text.strip() for cell in row.cells])
                    if row_text.strip():
                        table_text.append(row_text)
                if table_text:
                    full_text.append("[è¡¨æ ¼]\n" + "\n".join(table_text))
            
            combined_text = "\n\n".join(full_text)
            chunks = self._split_text(combined_text)
            
            for chunk_idx, chunk in enumerate(chunks):
                if len(chunk) > 30:
                    parsed_data.append({
                        "text": chunk,
                        "metadata": {
                            "file_name": file_name,
                            "page_label": "1",
                            "chunk_index": chunk_idx,
                            "content_type": "text",
                            "source": "python-docx"
                        }
                    })
            
            logger.info(f"âœ… [Parser] Word è§£æå®Œæˆï¼Œå…± {len(parsed_data)} å€‹ chunks")
            
        except Exception as e:
            logger.error(f"âŒ [Parser] Word è§£æå¤±æ•—: {e}")
        
        return parsed_data
    
    def _parse_excel(self, file_path: str) -> List[Dict[str, Any]]:
        """è§£æ Excel æ–‡ä»¶"""
        try:
            import pandas as pd
        except ImportError:
            logger.error("âŒ [Parser] pandas æœªå®‰è£: pip install pandas openpyxl")
            return []
        
        parsed_data = []
        file_name = os.path.basename(file_path)
        
        try:
            xlsx = pd.ExcelFile(file_path)
            
            for sheet_name in xlsx.sheet_names:
                df = pd.read_excel(xlsx, sheet_name=sheet_name)
                
                sheet_text = f"[å·¥ä½œè¡¨: {sheet_name}]\n"
                sheet_text += f"æ¬„ä½: {', '.join(df.columns.astype(str))}\n"
                sheet_text += f"è³‡æ–™ç­†æ•¸: {len(df)}\n\n"
                
                if len(df) > 0:
                    sample = df.head(10).to_string()
                    sheet_text += f"æ•¸æ“šç¯„ä¾‹:\n{sample}"
                
                try:
                    desc = df.describe().to_string()
                    sheet_text += f"\n\nçµ±è¨ˆæ‘˜è¦:\n{desc}"
                except:
                    pass
                
                chunks = self._split_text(sheet_text)
                for chunk_idx, chunk in enumerate(chunks):
                    if len(chunk) > 30:
                        parsed_data.append({
                            "text": chunk,
                            "metadata": {
                                "file_name": file_name,
                                "page_label": sheet_name,
                                "chunk_index": chunk_idx,
                                "content_type": "spreadsheet",
                                "source": "pandas"
                            }
                        })
            
            logger.info(f"âœ… [Parser] Excel è§£æå®Œæˆï¼Œå…± {len(parsed_data)} å€‹ chunks")
            
        except Exception as e:
            logger.error(f"âŒ [Parser] Excel è§£æå¤±æ•—: {e}")
        
        return parsed_data
    
    def _parse_csv(self, file_path: str) -> List[Dict[str, Any]]:
        """è§£æ CSV æ–‡ä»¶"""
        try:
            import pandas as pd
        except ImportError:
            logger.error("âŒ [Parser] pandas æœªå®‰è£")
            return []
        
        parsed_data = []
        file_name = os.path.basename(file_path)
        
        try:
            for encoding in ['utf-8', 'utf-8-sig', 'gbk', 'big5', 'latin1']:
                try:
                    df = pd.read_csv(file_path, encoding=encoding)
                    break
                except:
                    continue
            else:
                logger.error(f"âŒ [Parser] ç„¡æ³•è®€å– CSV æ–‡ä»¶")
                return []
            
            csv_text = f"[CSV æ–‡ä»¶: {file_name}]\n"
            csv_text += f"æ¬„ä½: {', '.join(df.columns.astype(str))}\n"
            csv_text += f"è³‡æ–™ç­†æ•¸: {len(df)}\n\n"
            
            if len(df) > 0:
                sample = df.head(20).to_string()
                csv_text += f"æ•¸æ“šç¯„ä¾‹:\n{sample}"
            
            chunks = self._split_text(csv_text)
            for chunk_idx, chunk in enumerate(chunks):
                if len(chunk) > 30:
                    parsed_data.append({
                        "text": chunk,
                        "metadata": {
                            "file_name": file_name,
                            "page_label": "1",
                            "chunk_index": chunk_idx,
                            "content_type": "csv",
                            "source": "pandas"
                        }
                    })
            
            logger.info(f"âœ… [Parser] CSV è§£æå®Œæˆï¼Œå…± {len(parsed_data)} å€‹ chunks")
            
        except Exception as e:
            logger.error(f"âŒ [Parser] CSV è§£æå¤±æ•—: {e}")
        
        return parsed_data
    
    def _parse_text(self, file_path: str) -> List[Dict[str, Any]]:
        """è§£æç´”æ–‡å­—æ–‡ä»¶"""
        parsed_data = []
        file_name = os.path.basename(file_path)
        
        try:
            content = None
            for encoding in ['utf-8', 'utf-8-sig', 'gbk', 'big5', 'latin1']:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        content = f.read()
                    break
                except:
                    continue
            
            if not content:
                logger.error(f"âŒ [Parser] ç„¡æ³•è®€å–æ–‡ä»¶")
                return []
            
            chunks = self._split_text(content)
            for chunk_idx, chunk in enumerate(chunks):
                if len(chunk) > 30:
                    parsed_data.append({
                        "text": chunk,
                        "metadata": {
                            "file_name": file_name,
                            "page_label": "1",
                            "chunk_index": chunk_idx,
                            "content_type": "text",
                            "source": "text"
                        }
                    })
            
            logger.info(f"âœ… [Parser] æ–‡å­—æ–‡ä»¶è§£æå®Œæˆï¼Œå…± {len(parsed_data)} å€‹ chunks")
            
        except Exception as e:
            logger.error(f"âŒ [Parser] æ–‡å­—æ–‡ä»¶è§£æå¤±æ•—: {e}")
        
        return parsed_data
    
    def _parse_markdown(self, file_path: str) -> List[Dict[str, Any]]:
        """è§£æ Markdown æ–‡ä»¶"""
        return self._parse_text(file_path)
    
    def _parse_json(self, file_path: str) -> List[Dict[str, Any]]:
        """è§£æ JSON æ–‡ä»¶"""
        parsed_data = []
        file_name = os.path.basename(file_path)
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            json_text = json.dumps(data, ensure_ascii=False, indent=2)
            
            chunks = self._split_text(json_text)
            for chunk_idx, chunk in enumerate(chunks):
                if len(chunk) > 30:
                    parsed_data.append({
                        "text": chunk,
                        "metadata": {
                            "file_name": file_name,
                            "page_label": "1",
                            "chunk_index": chunk_idx,
                            "content_type": "json",
                            "source": "json"
                        }
                    })
            
            logger.info(f"âœ… [Parser] JSON è§£æå®Œæˆï¼Œå…± {len(parsed_data)} å€‹ chunks")
            
        except Exception as e:
            logger.error(f"âŒ [Parser] JSON è§£æå¤±æ•—: {e}")
        
        return parsed_data
    
    def _parse_yaml(self, file_path: str) -> List[Dict[str, Any]]:
        """è§£æ YAML æ–‡ä»¶"""
        try:
            import yaml
        except ImportError:
            logger.warning("âš ï¸ [Parser] PyYAML æœªå®‰è£ï¼Œä½œç‚ºæ–‡å­—è™•ç†")
            return self._parse_text(file_path)
        
        parsed_data = []
        file_name = os.path.basename(file_path)
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
            
            yaml_text = yaml.dump(data, allow_unicode=True, default_flow_style=False)
            
            chunks = self._split_text(yaml_text)
            for chunk_idx, chunk in enumerate(chunks):
                if len(chunk) > 30:
                    parsed_data.append({
                        "text": chunk,
                        "metadata": {
                            "file_name": file_name,
                            "page_label": "1",
                            "chunk_index": chunk_idx,
                            "content_type": "yaml",
                            "source": "yaml"
                        }
                    })
            
            logger.info(f"âœ… [Parser] YAML è§£æå®Œæˆï¼Œå…± {len(parsed_data)} å€‹ chunks")
            
        except Exception as e:
            logger.error(f"âŒ [Parser] YAML è§£æå¤±æ•—: {e}")
        
        return parsed_data
    
    def _parse_xml(self, file_path: str) -> List[Dict[str, Any]]:
        """è§£æ XML æ–‡ä»¶"""
        return self._parse_text(file_path)
    
    def _parse_code(self, file_path: str) -> List[Dict[str, Any]]:
        """è§£æç¨‹å¼ç¢¼æ–‡ä»¶"""
        parsed_data = []
        file_name = os.path.basename(file_path)
        ext = Path(file_path).suffix.lower()
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            code_text = f"[ç¨‹å¼ç¢¼æ–‡ä»¶: {file_name} ({ext})]\n\n{content}"
            
            chunks = self._split_text(code_text)
            for chunk_idx, chunk in enumerate(chunks):
                if len(chunk) > 30:
                    parsed_data.append({
                        "text": chunk,
                        "metadata": {
                            "file_name": file_name,
                            "page_label": "1",
                            "chunk_index": chunk_idx,
                            "content_type": "code",
                            "language": ext.lstrip('.'),
                            "source": "code"
                        }
                    })
            
            logger.info(f"âœ… [Parser] ç¨‹å¼ç¢¼è§£æå®Œæˆï¼Œå…± {len(parsed_data)} å€‹ chunks")
            
        except Exception as e:
            logger.error(f"âŒ [Parser] ç¨‹å¼ç¢¼è§£æå¤±æ•—: {e}")
        
        return parsed_data
    
    def _split_text(self, text: str) -> List[str]:
        """å°‡æ–‡å­—åˆ‡åˆ†æˆé©ç•¶å¤§å°çš„ chunks"""
        if len(text) <= self.chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.chunk_size
            
            if end < len(text):
                for sep in ['\n\n', '\n', 'ã€‚', '.', 'ï¼', '!', 'ï¼Ÿ', '?', 'ï¼›', ';', 'ï¼Œ', ',']:
                    last_sep = text.rfind(sep, start, end)
                    if last_sep > start + self.chunk_size // 2:
                        end = last_sep + 1
                        break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - self.chunk_overlap if end < len(text) else end
        
        return chunks


# å…¨åŸŸå¯¦ä¾‹
_parser_instance = None


def get_multimodal_parser() -> MultimodalParser:
    """å–å¾—å…¨åŸŸè§£æå™¨å¯¦ä¾‹"""
    global _parser_instance
    if _parser_instance is None:
        _parser_instance = MultimodalParser()
    return _parser_instance
