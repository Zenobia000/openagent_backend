"""
Web Search Service - ç¶²è·¯æœå°‹æœå‹™ (å¢žå¼·ç‰ˆ)
æ”¯æ´å¤šç¨®æœå°‹å¼•æ“Ž: Tavily, SerpAPI, Bing, DuckDuckGo
å¢žåŠ : å¤šå¼•æ“Žä¸¦è¡Œã€ç¶²é å…§å®¹æŠ“å–ã€é‡è©¦æ©Ÿåˆ¶
"""

import os
import re
import logging
import asyncio
import aiohttp
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict, field
from datetime import datetime
from urllib.parse import quote_plus, urlparse
from bs4 import BeautifulSoup

from opencode.core.utils import load_env
load_env()

logger = logging.getLogger(__name__)


@dataclass
class SearchResult:
    """æœå°‹çµæžœ"""
    title: str
    url: str
    snippet: str
    source: str = ""
    published_date: Optional[str] = None
    content: str = ""  # å®Œæ•´ç¶²é å…§å®¹ï¼ˆfetch å¾Œå¡«å…¥ï¼‰
    fetched: bool = False
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


class WebSearchService:
    """
    ç¶²è·¯æœå°‹æœå‹™ (å¢žå¼·ç‰ˆ)
    
    æ–°å¢žåŠŸèƒ½ï¼š
    1. å¤šå¼•æ“Žä¸¦è¡Œæœå°‹ï¼ˆæé«˜æˆåŠŸçŽ‡ï¼‰
    2. ç¶²é å…§å®¹æŠ“å–ï¼ˆä¸åªæ˜¯æ‘˜è¦ï¼‰
    3. æœå°‹å¤±æ•—æ™‚è‡ªå‹•é‡è©¦
    4. æ™ºèƒ½é—œéµè©žæ“´å±•
    """
    
    def __init__(self):
        self._initialized = False
        self.provider = None
        self.tavily_key = os.getenv("TAVILY_API_KEY")
        self.serpapi_key = os.getenv("SERPAPI_KEY")
        self.serper_key = os.getenv("SERPER_API_KEY")
        self._session: Optional[aiohttp.ClientSession] = None
        
    async def initialize(self) -> None:
        """åˆå§‹åŒ–æœå‹™"""
        if self._initialized:
            return
            
        # å„ªå…ˆé †åº: Tavily > Serper > SerpAPI > Bing/DDG
        if self.tavily_key:
            self.provider = "tavily"
            logger.info("âœ… WebSearch ä½¿ç”¨ Tavily API (æŽ¨è–¦)")
        elif self.serper_key:
            self.provider = "serper"
            logger.info("âœ… WebSearch ä½¿ç”¨ Serper (Google)")
        elif self.serpapi_key:
            self.provider = "serpapi"
            logger.info("âœ… WebSearch ä½¿ç”¨ SerpAPI")
        else:
            self.provider = "multi"  # å¤šå¼•æ“Žæ¨¡å¼
            logger.info("âš ï¸ WebSearch ä½¿ç”¨å…è²»å¤šå¼•æ“Žæ¨¡å¼ (Bing + DuckDuckGo)")
            
        self._initialized = True
    
    async def _get_session(self) -> aiohttp.ClientSession:
        """ç²å– HTTP session"""
        if self._session is None or self._session.closed:
            timeout = aiohttp.ClientTimeout(total=30)
            self._session = aiohttp.ClientSession(timeout=timeout)
        return self._session
    
    async def close(self):
        """é—œé–‰ session"""
        if self._session and not self._session.closed:
            await self._session.close()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # æœå°‹æ–¹æ³•
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def search(
        self,
        query: str,
        max_results: int = 5,
        search_type: str = "general"
    ) -> List[SearchResult]:
        """
        åŸ·è¡Œç¶²è·¯æœå°‹
        
        Args:
            query: æœå°‹é—œéµå­—
            max_results: æœ€å¤§çµæžœæ•¸
            search_type: æœå°‹é¡žåž‹
            
        Returns:
            æœå°‹çµæžœåˆ—è¡¨
        """
        await self.initialize()
        
        try:
            if self.provider == "tavily":
                results = await self._search_tavily(query, max_results, search_type)
            elif self.provider == "serper":
                results = await self._search_serper(query, max_results)
            elif self.provider == "serpapi":
                results = await self._search_serpapi(query, max_results)
            else:
                # å¤šå¼•æ“Žä¸¦è¡Œæ¨¡å¼
                results = await self._search_multi_engine(query, max_results)
            
            if results:
                return results
                
        except Exception as e:
            logger.error(f"âŒ æœå°‹å¤±æ•—: {e}")
        
        # Fallback: å¤šå¼•æ“Žæ¨¡å¼
        if self.provider != "multi":
            logger.info("ðŸ”„ Fallback to multi-engine search")
            return await self._search_multi_engine(query, max_results)
        
        return []
    
    async def _search_multi_engine(
        self,
        query: str,
        max_results: int = 5
    ) -> List[SearchResult]:
        """å¤šå¼•æ“Žä¸¦è¡Œæœå°‹"""
        logger.info(f"ðŸ” å¤šå¼•æ“Žæœå°‹: {query}")
        
        # ä¸¦è¡ŒåŸ·è¡Œå¤šå€‹æœå°‹å¼•æ“Ž
        tasks = [
            self._search_bing(query, max_results),
            self._search_duckduckgo_html(query, max_results),
        ]
        
        all_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # åˆä½µçµæžœï¼ŒåŽ»é‡
        seen_urls = set()
        merged = []
        
        for results in all_results:
            if isinstance(results, Exception):
                logger.warning(f"âš ï¸ å¼•æ“Žå¤±æ•—: {results}")
                continue
            if not results:
                continue
            for r in results:
                if r.url and r.url not in seen_urls:
                    seen_urls.add(r.url)
                    merged.append(r)
        
        logger.info(f"âœ… å¤šå¼•æ“Žæœå°‹å®Œæˆï¼Œå…± {len(merged)} å€‹ä¸é‡è¤‡çµæžœ")
        return merged[:max_results]
    
    async def _search_bing(
        self,
        query: str,
        max_results: int = 5
    ) -> List[SearchResult]:
        """Bing æœå°‹ï¼ˆçˆ¬å– HTMLï¼‰"""
        try:
            session = await self._get_session()
            url = f"https://www.bing.com/search?q={quote_plus(query)}&count={max_results}"
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml",
                "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.8"
            }
            
            async with session.get(url, headers=headers) as resp:
                if resp.status == 200:
                    html = await resp.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    results = []
                    
                    for item in soup.select('.b_algo')[:max_results]:
                        title_elem = item.select_one('h2 a')
                        snippet_elem = item.select_one('.b_caption p')
                        
                        if title_elem and title_elem.get('href'):
                            results.append(SearchResult(
                                title=title_elem.get_text(strip=True),
                                url=title_elem.get('href', ''),
                                snippet=snippet_elem.get_text(strip=True) if snippet_elem else "",
                                source="Bing"
                            ))
                    
                    logger.info(f"âœ… Bing æ‰¾åˆ° {len(results)} å€‹çµæžœ")
                    return results
                else:
                    logger.warning(f"âš ï¸ Bing è¿”å›žç‹€æ…‹ç¢¼: {resp.status}")
        except Exception as e:
            logger.error(f"âŒ Bing æœå°‹å¤±æ•—: {e}")
        return []
    
    async def _search_duckduckgo_html(
        self,
        query: str,
        max_results: int = 5
    ) -> List[SearchResult]:
        """DuckDuckGo HTML æœå°‹ï¼ˆç›´æŽ¥çˆ¬å–ï¼‰"""
        try:
            session = await self._get_session()
            url = f"https://html.duckduckgo.com/html/?q={quote_plus(query)}"
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            }
            
            async with session.get(url, headers=headers) as resp:
                if resp.status == 200:
                    html = await resp.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    results = []
                    
                    for result in soup.select('.result')[:max_results]:
                        title_elem = result.select_one('.result__title a')
                        snippet_elem = result.select_one('.result__snippet')
                        
                        if title_elem:
                            # æå–çœŸå¯¦ URL
                            href = title_elem.get('href', '')
                            if 'uddg=' in href:
                                import urllib.parse
                                parsed = urllib.parse.parse_qs(urllib.parse.urlparse(href).query)
                                real_url = parsed.get('uddg', [href])[0]
                            else:
                                real_url = href
                            
                            if real_url and real_url.startswith('http'):
                                results.append(SearchResult(
                                    title=title_elem.get_text(strip=True),
                                    url=real_url,
                                    snippet=snippet_elem.get_text(strip=True) if snippet_elem else "",
                                    source="DuckDuckGo"
                                ))
                    
                    logger.info(f"âœ… DuckDuckGo æ‰¾åˆ° {len(results)} å€‹çµæžœ")
                    return results
        except Exception as e:
            logger.error(f"âŒ DuckDuckGo æœå°‹å¤±æ•—: {e}")
        return []
    
    async def _search_tavily(
        self,
        query: str,
        max_results: int = 5,
        search_type: str = "general"
    ) -> List[SearchResult]:
        """ä½¿ç”¨ Tavily API (AI å„ªåŒ–æœå°‹)"""
        try:
            session = await self._get_session()
            url = "https://api.tavily.com/search"
            payload = {
                "api_key": self.tavily_key,
                "query": query,
                "max_results": max_results,
                "search_depth": "advanced" if search_type == "academic" else "basic",
                "include_answer": True,
                "include_raw_content": False
            }
            
            async with session.post(url, json=payload) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    results = []
                    
                    for r in data.get("results", []):
                        results.append(SearchResult(
                            title=r.get("title", ""),
                            url=r.get("url", ""),
                            snippet=r.get("content", ""),
                            source="Tavily",
                            published_date=r.get("published_date")
                        ))
                    
                    logger.info(f"âœ… Tavily æ‰¾åˆ° {len(results)} å€‹çµæžœ")
                    return results
                else:
                    error = await resp.text()
                    logger.error(f"âŒ Tavily API éŒ¯èª¤: {error}")
        except Exception as e:
            logger.error(f"âŒ Tavily æœå°‹å¤±æ•—: {e}")
        return []
    
    async def _search_serper(
        self,
        query: str,
        max_results: int = 5
    ) -> List[SearchResult]:
        """ä½¿ç”¨ Serper.dev (Google æœå°‹)"""
        try:
            session = await self._get_session()
            async with session.post(
                "https://google.serper.dev/search",
                headers={"X-API-KEY": self.serper_key},
                json={"q": query, "num": max_results}
            ) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    results = []
                    for r in data.get("organic", []):
                        results.append(SearchResult(
                            title=r.get("title", ""),
                            url=r.get("link", ""),
                            snippet=r.get("snippet", ""),
                            source="Google"
                        ))
                    logger.info(f"âœ… Serper æ‰¾åˆ° {len(results)} å€‹çµæžœ")
                    return results
        except Exception as e:
            logger.error(f"âŒ Serper æœå°‹å¤±æ•—: {e}")
        return []
    
    async def _search_serpapi(
        self,
        query: str,
        max_results: int = 5
    ) -> List[SearchResult]:
        """ä½¿ç”¨ SerpAPI (Google æœå°‹)"""
        try:
            session = await self._get_session()
            url = "https://serpapi.com/search"
            params = {
                "api_key": self.serpapi_key,
                "q": query,
                "num": max_results,
                "engine": "google"
            }
            
            async with session.get(url, params=params) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    results = []
                    
                    for r in data.get("organic_results", []):
                        results.append(SearchResult(
                            title=r.get("title", ""),
                            url=r.get("link", ""),
                            snippet=r.get("snippet", ""),
                            source="Google"
                        ))
                    
                    logger.info(f"âœ… SerpAPI æ‰¾åˆ° {len(results)} å€‹çµæžœ")
                    return results
        except Exception as e:
            logger.error(f"âŒ SerpAPI æœå°‹å¤±æ•—: {e}")
        return []
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ç¶²é å…§å®¹æŠ“å–
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def fetch_url(self, url: str, timeout: int = 15) -> Optional[str]:
        """æŠ“å–ç¶²é å…§å®¹ä¸¦æå–ä¸»è¦æ–‡å­—"""
        try:
            session = await self._get_session()
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "Accept": "text/html,application/xhtml+xml",
                "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.8"
            }
            
            async with session.get(url, headers=headers, timeout=timeout) as resp:
                if resp.status == 200:
                    html = await resp.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                    for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'iframe', 'noscript', 'form']):
                        tag.decompose()
                    
                    # å˜—è©¦æ‰¾ä¸»è¦å…§å®¹å€å¡Š
                    main_content = (
                        soup.find('article') or 
                        soup.find('main') or 
                        soup.find(class_=re.compile(r'content|article|post|entry|text')) or
                        soup.find('body')
                    )
                    
                    if main_content:
                        text = main_content.get_text(separator='\n', strip=True)
                        # æ¸…ç†å¤šé¤˜ç©ºè¡Œ
                        text = re.sub(r'\n{3,}', '\n\n', text)
                        # é™åˆ¶é•·åº¦
                        if len(text) > 5000:
                            text = text[:5000] + "...[å…§å®¹æˆªæ–·]"
                        
                        logger.info(f"âœ… æŠ“å–æˆåŠŸ: {urlparse(url).netloc} ({len(text)} å­—)")
                        return text
                    
        except asyncio.TimeoutError:
            logger.warning(f"â±ï¸ æŠ“å–è¶…æ™‚: {url}")
        except Exception as e:
            logger.error(f"âŒ æŠ“å–å¤±æ•— {url}: {e}")
        
        return None
    
    async def fetch_multiple(self, urls: List[str], max_concurrent: int = 3) -> Dict[str, str]:
        """ä¸¦è¡ŒæŠ“å–å¤šå€‹ç¶²é """
        logger.info(f"ðŸ“¥ é–‹å§‹æŠ“å– {len(urls)} å€‹ç¶²é ...")
        
        # ä½¿ç”¨ semaphore é™åˆ¶ä¸¦è¡Œæ•¸
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def fetch_with_semaphore(url):
            async with semaphore:
                return url, await self.fetch_url(url)
        
        tasks = [fetch_with_semaphore(url) for url in urls]
        results = await asyncio.gather(*tasks)
        
        content_map = {}
        success_count = 0
        for url, content in results:
            if content:
                content_map[url] = content
                success_count += 1
        
        logger.info(f"âœ… æˆåŠŸæŠ“å– {success_count}/{len(urls)} å€‹ç¶²é ")
        return content_map
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # æ·±åº¦æœå°‹ï¼ˆæœå°‹ + æŠ“å–å…§å®¹ï¼‰
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def search_and_fetch(
        self,
        query: str,
        max_results: int = 5,
        fetch_top_n: int = 3
    ) -> List[SearchResult]:
        """
        æœå°‹ä¸¦æŠ“å–ç¶²é å…§å®¹
        
        Args:
            query: æœå°‹é—œéµå­—
            max_results: æœå°‹çµæžœæ•¸
            fetch_top_n: æŠ“å–å‰ N å€‹ç¶²é çš„å®Œæ•´å…§å®¹
            
        Returns:
            åŒ…å«å®Œæ•´å…§å®¹çš„æœå°‹çµæžœ
        """
        results = await self.search(query, max_results)
        
        if not results:
            return []
        
        # æŠ“å–å‰ N å€‹ç¶²é çš„å…§å®¹
        urls_to_fetch = [r.url for r in results[:fetch_top_n]]
        content_map = await self.fetch_multiple(urls_to_fetch)
        
        # æ›´æ–°çµæžœ
        for r in results:
            if r.url in content_map:
                r.content = content_map[r.url]
                r.fetched = True
        
        return results
    
    async def search_with_retry(
        self,
        query: str,
        max_results: int = 5,
        max_retries: int = 2
    ) -> List[SearchResult]:
        """
        å¸¶é‡è©¦æ©Ÿåˆ¶çš„æœå°‹
        
        å¦‚æžœæœå°‹å¤±æ•—ï¼Œæœƒå˜—è©¦ï¼š
        1. ç°¡åŒ–é—œéµè©ž
        2. ä½¿ç”¨è‹±æ–‡é—œéµè©ž
        """
        # ç¬¬ä¸€æ¬¡å˜—è©¦
        results = await self.search(query, max_results)
        if results:
            return results
        
        # é‡è©¦ç­–ç•¥
        retry_queries = [
            # ç°¡åŒ–ï¼šåªå–å‰å¹¾å€‹è©ž
            ' '.join(query.split()[:3]),
            # åŠ ä¸Š "æ˜¯ä»€éº¼"
            f"{query.split()[0]} æ˜¯ä»€éº¼",
        ]
        
        for i, retry_query in enumerate(retry_queries[:max_retries]):
            logger.info(f"ðŸ”„ é‡è©¦æœå°‹ ({i+1}/{max_retries}): {retry_query}")
            results = await self.search(retry_query, max_results)
            if results:
                return results
        
        return []
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # LLM æ‘˜è¦
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def search_and_summarize(
        self,
        query: str,
        max_results: int = 5,
        fetch_content: bool = True
    ) -> Dict[str, Any]:
        """
        æœå°‹ä¸¦ç”¨ LLM æ‘˜è¦çµæžœ
        
        Returns:
            {
                "query": str,
                "summary": str,
                "results": List[dict],
                "sources": List[str]
            }
        """
        if fetch_content:
            results = await self.search_and_fetch(query, max_results, fetch_top_n=3)
        else:
            results = await self.search_with_retry(query, max_results)
        
        if not results:
            return {
                "query": query,
                "summary": "æœªæ‰¾åˆ°ç›¸é—œæœå°‹çµæžœã€‚å»ºè­°å˜—è©¦ä¸åŒçš„é—œéµè©žã€‚",
                "results": [],
                "sources": []
            }
        
        # å»ºæ§‹æœå°‹çµæžœæ–‡å­—
        context_parts = []
        for i, r in enumerate(results):
            part = f"[{i+1}] {r.title}\n{r.snippet}"
            if r.content:
                # å¦‚æžœæœ‰å®Œæ•´å…§å®¹ï¼ŒåŠ å…¥æ‘˜è¦
                part += f"\n\nè©³ç´°å…§å®¹:\n{r.content[:1500]}"
            part += f"\nä¾†æº: {r.url}"
            context_parts.append(part)
        
        context = "\n\n---\n\n".join(context_parts)
        
        # ä½¿ç”¨ LLM æ‘˜è¦
        try:
            from openai import AsyncOpenAI
            api_key = os.getenv("OPENAI_API_KEY")
            
            if api_key:
                client = AsyncOpenAI(api_key=api_key)
                response = await client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {
                            "role": "system",
                            "content": """ä½ æ˜¯ä¸€å€‹æœå°‹æ‘˜è¦åŠ©æ‰‹ã€‚è«‹æ ¹æ“šæœå°‹çµæžœï¼Œç”¨ç¹é«”ä¸­æ–‡è©³ç´°å›žç­”ç”¨æˆ¶çš„å•é¡Œã€‚

è¦æ±‚ï¼š
1. ç¶œåˆæ‰€æœ‰ä¾†æºçš„è³‡è¨Š
2. ä½¿ç”¨ [1], [2] ç­‰æ¨™è¨˜å¼•ç”¨ä¾†æº
3. çµæ§‹åŒ–å‘ˆç¾è³‡è¨Š
4. æœ€å¾Œåˆ—å‡ºåƒè€ƒä¾†æº"""
                        },
                        {
                            "role": "user",
                            "content": f"å•é¡Œ: {query}\n\næœå°‹çµæžœ:\n{context}\n\nè«‹æ ¹æ“šä»¥ä¸Šæœå°‹çµæžœè©³ç´°å›žç­”å•é¡Œï¼Œä¸¦æ¨™è¨»ä¾†æºã€‚"
                        }
                    ],
                    temperature=0.3,
                    max_tokens=2000
                )
                summary = response.choices[0].message.content
            else:
                summary = f"æ‰¾åˆ° {len(results)} å€‹ç›¸é—œçµæžœã€‚"
                
        except Exception as e:
            logger.error(f"âŒ LLM æ‘˜è¦å¤±æ•—: {e}")
            summary = f"æ‰¾åˆ° {len(results)} å€‹ç›¸é—œçµæžœã€‚"
        
        return {
            "query": query,
            "summary": summary,
            "results": [r.to_dict() for r in results],
            "sources": [{"title": r.title, "url": r.url} for r in results]
        }
    
    async def execute(self, action: str, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """MCP çµ±ä¸€åŸ·è¡Œä»‹é¢"""
        await self.initialize()
        
        if action == "search":
            query = parameters.get("query", "")
            max_results = parameters.get("max_results", 5)
            results = await self.search_with_retry(query, max_results)
            return {
                "success": len(results) > 0,
                "results": [r.to_dict() for r in results],
                "count": len(results)
            }
        
        elif action == "search_and_fetch":
            query = parameters.get("query", "")
            max_results = parameters.get("max_results", 5)
            fetch_top_n = parameters.get("fetch_top_n", 3)
            results = await self.search_and_fetch(query, max_results, fetch_top_n)
            return {
                "success": len(results) > 0,
                "results": [r.to_dict() for r in results],
                "count": len(results),
                "fetched_count": sum(1 for r in results if r.fetched)
            }
            
        elif action == "fetch":
            url = parameters.get("url", "")
            content = await self.fetch_url(url)
            return {
                "success": content is not None,
                "url": url,
                "content": content or "",
                "length": len(content) if content else 0
            }
            
        elif action == "search_summarize":
            query = parameters.get("query", "")
            max_results = parameters.get("max_results", 5)
            fetch_content = parameters.get("fetch_content", True)
            return await self.search_and_summarize(query, max_results, fetch_content)
            
        else:
            return {"success": False, "error": f"Unknown action: {action}"}


# å…¨åŸŸå¯¦ä¾‹
_web_search_service = None

def get_web_search_service() -> WebSearchService:
    global _web_search_service
    if _web_search_service is None:
        _web_search_service = WebSearchService()
    return _web_search_service
