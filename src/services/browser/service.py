"""
Deep Research Service

å¯¦ç¾åŠŸèƒ½ï¼š
1. Playwright è‡ªå‹•åŒ–ç€è¦½å™¨ï¼ˆæ”¯æ´ JavaScript æ¸²æŸ“ï¼‰
2. å¤šè¼ªæœå°‹ + å…§å®¹åˆ†æ
3. æˆªåœ–åŠŸèƒ½
4. SSE å³æ™‚é€²åº¦å›å ±
5. LLM æ™ºèƒ½åˆ†æå’Œå ±å‘Šç”Ÿæˆ
"""

import os
import re
import base64
import asyncio
import logging
import json
from typing import Dict, Any, Optional, List, AsyncGenerator
from dataclasses import dataclass, asdict, field
from datetime import datetime
from urllib.parse import urlparse, quote_plus
import aiohttp
from bs4 import BeautifulSoup

from core.utils import load_env
from core.prompts import PromptTemplates
load_env()

logger = logging.getLogger(__name__)

# Playwright æ˜¯å¯é¸ä¾è³´
PLAYWRIGHT_AVAILABLE = False
try:
    from playwright.async_api import async_playwright, Browser, Page, Playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    logger.warning("âš ï¸ Playwright æœªå®‰è£")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# è³‡æ–™çµæ§‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class SearchResult:
    """æœå°‹çµæœ"""
    title: str
    url: str
    snippet: str
    source: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class BrowseResult:
    """ç€è¦½çµæœ"""
    url: str
    title: str
    content: str
    screenshot: str = ""  # base64 encoded JPEG
    success: bool = True
    error: str = ""
    load_time: float = 0
    
    def to_dict(self) -> Dict[str, Any]:
        # ä¸åŒ…å«å®Œæ•´ screenshotï¼ˆå¤ªå¤§ï¼‰
        d = asdict(self)
        if d.get('screenshot'):
            d['screenshot'] = f"data:image/jpeg;base64,{d['screenshot'][:100]}..." if len(d['screenshot']) > 100 else d['screenshot']
            d['has_screenshot'] = True
        else:
            d['has_screenshot'] = False
        return d


@dataclass
class ResearchStep:
    """ç ”ç©¶æ­¥é©Ÿï¼ˆç”¨æ–¼ SSEï¼‰"""
    step_type: str  # thinking, searching, browsing, reading, analyzing, complete, error
    status: str     # running, completed, failed
    message: str
    data: Dict[str, Any] = field(default_factory=dict)
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç€è¦½å™¨æœå‹™
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class BrowserService:
    """
    Playwright ç€è¦½å™¨æœå‹™
    
    å¦‚æœ Playwright ä¸å¯ç”¨ï¼Œè‡ªå‹•é™ç´šåˆ° aiohttp
    """
    
    def __init__(self):
        self._playwright = None
        self._browser: Optional[Browser] = None
        self._initialized = False
        self.use_playwright = PLAYWRIGHT_AVAILABLE
        
        # HTTP session for fallback
        self._http_session: Optional[aiohttp.ClientSession] = None
        
        # è¨­å®š
        self.timeout = 30000  # 30 ç§’
        self.viewport = {"width": 1280, "height": 720}
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–"""
        if self._initialized:
            return True
        
        if self.use_playwright:
            try:
                self._playwright = await async_playwright().start()
                self._browser = await self._playwright.chromium.launch(
                    headless=True,
                    args=['--no-sandbox', '--disable-setuid-sandbox', '--disable-gpu']
                )
                logger.info("âœ… Playwright ç€è¦½å™¨å•Ÿå‹•æˆåŠŸ")
            except Exception as e:
                logger.warning(f"âš ï¸ Playwright å•Ÿå‹•å¤±æ•—: {e}ï¼Œé™ç´šåˆ° HTTP æ¨¡å¼")
                self.use_playwright = False
        
        # åˆå§‹åŒ– HTTP session
        timeout = aiohttp.ClientTimeout(total=30)
        self._http_session = aiohttp.ClientSession(timeout=timeout)
        
        self._initialized = True
        return True
    
    async def close(self):
        """é—œé–‰"""
        if self._browser:
            await self._browser.close()
        if self._playwright:
            await self._playwright.stop()
        if self._http_session and not self._http_session.closed:
            await self._http_session.close()
        self._initialized = False
    
    async def browse(
        self,
        url: str,
        take_screenshot: bool = True
    ) -> BrowseResult:
        """ç€è¦½ç¶²é """
        await self.initialize()
        
        if self.use_playwright and self._browser:
            return await self._browse_playwright(url, take_screenshot)
        else:
            return await self._browse_http(url)
    
    async def _browse_playwright(self, url: str, take_screenshot: bool) -> BrowseResult:
        """ä½¿ç”¨ Playwright ç€è¦½"""
        page = None
        try:
            import time
            start = time.time()
            
            page = await self._browser.new_page(viewport=self.viewport)
            page.set_default_timeout(self.timeout)
            
            logger.info(f"ğŸŒ [Playwright] æ­£åœ¨ç€è¦½: {url}")
            response = await page.goto(url, wait_until="domcontentloaded")
            
            if not response or response.status >= 400:
                return BrowseResult(
                    url=url, title="", content="",
                    success=False,
                    error=f"HTTP {response.status if response else 'error'}"
                )
            
            # ç­‰å¾…å…§å®¹è¼‰å…¥
            await asyncio.sleep(1)
            
            title = await page.title()
            
            # æˆªåœ–
            screenshot_b64 = ""
            if take_screenshot:
                try:
                    screenshot_bytes = await page.screenshot(type="jpeg", quality=60)
                    screenshot_b64 = base64.b64encode(screenshot_bytes).decode()
                except:
                    pass
            
            # æå–å…§å®¹
            content = await page.evaluate("""
                () => {
                    // ç§»é™¤å¹²æ“¾å…ƒç´ 
                    ['script', 'style', 'nav', 'footer', 'header', 'aside', 'iframe', '.ad', '.advertisement'].forEach(sel => {
                        document.querySelectorAll(sel).forEach(el => el.remove());
                    });
                    
                    const main = document.querySelector('article') || document.querySelector('main') || document.body;
                    return main ? main.innerText : '';
                }
            """)
            
            # æ¸…ç†å…§å®¹
            content = re.sub(r'\n{3,}', '\n\n', content).strip()
            if len(content) > 6000:
                content = content[:6000] + "\n...[æˆªæ–·]"
            
            return BrowseResult(
                url=url,
                title=title,
                content=content,
                screenshot=screenshot_b64,
                success=True,
                load_time=time.time() - start
            )
            
        except asyncio.TimeoutError:
            logger.warning(f"â±ï¸ Playwright è¶…æ™‚: {url}")
            return BrowseResult(url=url, title="", content="", success=False, error="è¶…æ™‚")
        except Exception as e:
            logger.error(f"âŒ Playwright ç€è¦½å¤±æ•—: {e}")
            return BrowseResult(url=url, title="", content="", success=False, error=str(e))
        finally:
            if page:
                await page.close()
    
    async def _browse_http(self, url: str) -> BrowseResult:
        """ä½¿ç”¨ HTTP ç€è¦½ï¼ˆé™ç´šæ¨¡å¼ï¼‰- å„ªåŒ–ç‰ˆ"""
        try:
            import time
            import ssl
            start = time.time()
            
            # æª¢æŸ¥æ˜¯å¦æ˜¯å·²çŸ¥é›£ä»¥è¨ªå•çš„ç¶²ç«™
            skip_domains = [
                'zhihu.com', 'zhuanlan.zhihu.com',  # çŸ¥ä¹éœ€è¦ç™»éŒ„
                'weixin.qq.com', 'mp.weixin.qq.com',  # å¾®ä¿¡å…¬çœ¾è™Ÿ
                'jianshu.com',  # ç°¡æ›¸
                'bilibili.com',  # Bç«™
            ]
            
            domain = urlparse(url).netloc.lower()
            for skip in skip_domains:
                if skip in domain:
                    logger.warning(f"âš ï¸ è·³éå—é™ç¶²ç«™: {domain}")
                    return BrowseResult(
                        url=url, title="", content="",
                        success=False, error=f"å—é™ç¶²ç«™: {domain}"
                    )
            
            # æ›´å®Œæ•´çš„ headersï¼Œæ¨¡æ“¬çœŸå¯¦ç€è¦½å™¨
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
                "Accept-Encoding": "gzip, deflate",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "Cache-Control": "max-age=0",
            }
            
            # å‰µå»ºè¼ƒçŸ­è¶…æ™‚çš„ session
            timeout = aiohttp.ClientTimeout(total=15, connect=8)
            
            # å‰µå»º SSL context ä»¥å¿½ç•¥è­‰æ›¸éŒ¯èª¤
            ssl_context = ssl.create_default_context()
            ssl_context.check_hostname = False
            ssl_context.verify_mode = ssl.CERT_NONE
            
            connector = aiohttp.TCPConnector(ssl=ssl_context)
            
            logger.info(f"ğŸŒ [HTTP] æ­£åœ¨ç€è¦½: {url}")
            
            async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:
                async with session.get(url, headers=headers, allow_redirects=True) as resp:
                    if resp.status >= 400:
                        return BrowseResult(
                            url=url, title="", content="",
                            success=False, error=f"HTTP {resp.status}"
                        )
                    
                    # æª¢æŸ¥å…§å®¹é¡å‹
                    content_type = resp.headers.get('Content-Type', '')
                    if 'text/html' not in content_type and 'text/plain' not in content_type:
                        return BrowseResult(
                            url=url, title="", content="",
                            success=False, error=f"é HTML å…§å®¹: {content_type}"
                        )
                    
                    html = await resp.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # ç§»é™¤å¹²æ“¾å…ƒç´ 
                    for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'iframe', 'noscript']):
                        tag.decompose()
                    
                    title = soup.title.string if soup.title else ""
                    
                    # æå–å…§å®¹
                    main = soup.find('article') or soup.find('main') or soup.find(class_='content') or soup.body
                    content = main.get_text(separator='\n', strip=True) if main else ""
                    
                    content = re.sub(r'\n{3,}', '\n\n', content)
                    if len(content) > 6000:
                        content = content[:6000] + "\n...[æˆªæ–·]"
                    
                    return BrowseResult(
                        url=url,
                        title=title,
                        content=content,
                        screenshot="",  # HTTP æ¨¡å¼ç„¡æˆªåœ–
                        success=True,
                        load_time=time.time() - start
                    )
        
        except asyncio.TimeoutError:
            logger.warning(f"â±ï¸ HTTP è¶…æ™‚: {url}")
            return BrowseResult(url=url, title="", content="", success=False, error="é€£ç·šè¶…æ™‚")
        except aiohttp.ClientError as e:
            logger.warning(f"âš ï¸ HTTP é€£ç·šéŒ¯èª¤: {url} - {e}")
            return BrowseResult(url=url, title="", content="", success=False, error="é€£ç·šéŒ¯èª¤")
        except Exception as e:
            logger.error(f"âŒ HTTP ç€è¦½å¤±æ•—: {e}")
            return BrowseResult(url=url, title="", content="", success=False, error=str(e))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æœå°‹æœå‹™
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SearchService:
    """å¤šå¼•æ“æœå°‹æœå‹™"""
    
    def __init__(self):
        self._session: Optional[aiohttp.ClientSession] = None
        self.tavily_key = os.getenv("TAVILY_API_KEY")
    
    async def _get_session(self) -> aiohttp.ClientSession:
        if self._session is None or self._session.closed:
            self._session = aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30))
        return self._session
    
    async def close(self):
        if self._session and not self._session.closed:
            await self._session.close()
    
    async def search(self, query: str, max_results: int = 10) -> List[SearchResult]:
        """åŸ·è¡Œæœå°‹"""
        # å„ªå…ˆä½¿ç”¨ Tavily
        if self.tavily_key:
            results = await self._search_tavily(query, max_results)
            if results:
                return results
        
        # Fallback: Bing + DuckDuckGo
        results = await self._search_multi(query, max_results)
        return results
    
    async def _search_tavily(self, query: str, max_results: int) -> List[SearchResult]:
        """Tavily AI æœå°‹"""
        try:
            session = await self._get_session()
            async with session.post(
                "https://api.tavily.com/search",
                json={
                    "api_key": self.tavily_key,
                    "query": query,
                    "search_depth": "basic",
                    "max_results": max_results
                }
            ) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    return [
                        SearchResult(
                            title=r.get("title", ""),
                            url=r.get("url", ""),
                            snippet=r.get("content", ""),
                            source="Tavily"
                        )
                        for r in data.get("results", [])
                    ]
        except Exception as e:
            logger.error(f"Tavily æœå°‹å¤±æ•—: {e}")
        return []
    
    async def _search_multi(self, query: str, max_results: int) -> List[SearchResult]:
        """å¤šå¼•æ“æœå°‹"""
        tasks = [
            self._search_bing(query, max_results),
            self._search_duckduckgo(query, max_results)
        ]
        all_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # åˆä½µå»é‡
        seen = set()
        merged = []
        for results in all_results:
            if isinstance(results, Exception):
                continue
            for r in results:
                if r.url not in seen:
                    seen.add(r.url)
                    merged.append(r)
        
        return merged[:max_results]
    
    async def _search_bing(self, query: str, max_results: int) -> List[SearchResult]:
        """Bing æœå°‹"""
        try:
            session = await self._get_session()
            url = f"https://www.bing.com/search?q={quote_plus(query)}&count={max_results}"
            headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
            
            async with session.get(url, headers=headers) as resp:
                if resp.status == 200:
                    html = await resp.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    results = []
                    
                    for item in soup.select('.b_algo')[:max_results]:
                        title_el = item.select_one('h2 a')
                        snippet_el = item.select_one('.b_caption p')
                        if title_el and title_el.get('href'):
                            results.append(SearchResult(
                                title=title_el.get_text(strip=True),
                                url=title_el.get('href'),
                                snippet=snippet_el.get_text(strip=True) if snippet_el else "",
                                source="Bing"
                            ))
                    return results
        except Exception as e:
            logger.error(f"Bing æœå°‹å¤±æ•—: {e}")
        return []
    
    async def _search_duckduckgo(self, query: str, max_results: int) -> List[SearchResult]:
        """DuckDuckGo æœå°‹"""
        try:
            session = await self._get_session()
            url = f"https://html.duckduckgo.com/html/?q={quote_plus(query)}"
            headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
            
            async with session.get(url, headers=headers) as resp:
                if resp.status == 200:
                    html = await resp.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    results = []
                    
                    for item in soup.select('.result')[:max_results]:
                        title_el = item.select_one('.result__title a')
                        snippet_el = item.select_one('.result__snippet')
                        if title_el:
                            href = title_el.get('href', '')
                            # è§£æ DuckDuckGo é‡å®šå‘ URL
                            if 'uddg=' in href:
                                import urllib.parse
                                parsed = urllib.parse.parse_qs(urllib.parse.urlparse(href).query)
                                href = parsed.get('uddg', [href])[0]
                            
                            if href.startswith('http'):
                                results.append(SearchResult(
                                    title=title_el.get_text(strip=True),
                                    url=href,
                                    snippet=snippet_el.get_text(strip=True) if snippet_el else "",
                                    source="DuckDuckGo"
                                ))
                    return results
        except Exception as e:
            logger.error(f"DuckDuckGo æœå°‹å¤±æ•—: {e}")
        return []


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ·±åº¦ç ”ç©¶ Agent
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class DeepResearchAgent:
    """
    Manus é¢¨æ ¼æ·±åº¦ç ”ç©¶ Agent
    
    å·¥ä½œæµç¨‹ï¼š
    1. åˆ†æå•é¡Œ â†’ ç”Ÿæˆæœå°‹ç­–ç•¥
    2. å¤šè¼ªæœå°‹ + ç€è¦½
    3. æ™ºèƒ½åˆ†æ â†’ æ±ºå®šæ˜¯å¦éœ€è¦æ›´å¤šè³‡æ–™
    4. ç”Ÿæˆçµæ§‹åŒ–å ±å‘Š
    """
    
    def __init__(self):
        self.browser = BrowserService()
        self.search = SearchService()
        self._llm_client = None
        
        # é…ç½®
        self.max_rounds = 3  # æœ€å¤šæœå°‹è¼ªæ•¸
        self.browse_per_round = 3  # æ¯è¼ªç€è¦½ç¶²é æ•¸
    
    async def initialize(self):
        """åˆå§‹åŒ–"""
        await self.browser.initialize()
        
        # åˆå§‹åŒ– LLM
        try:
            from openai import AsyncOpenAI
            api_key = os.getenv("OPENAI_API_KEY")
            if api_key:
                self._llm_client = AsyncOpenAI(api_key=api_key)
        except Exception as e:
            logger.warning(f"LLM åˆå§‹åŒ–å¤±æ•—: {e}")
    
    async def close(self):
        """é—œé–‰"""
        await self.browser.close()
        await self.search.close()
    
    async def research(
        self,
        query: str,
        depth: str = "standard",  # quick, standard, deep
        selected_docs: Optional[List[str]] = None  # ç”¨æˆ¶é¸æ“‡çš„æ–‡ä»¶
    ) -> AsyncGenerator[ResearchStep, None]:
        """
        åŸ·è¡Œæ·±åº¦ç ”ç©¶ï¼ˆSSE ä¸²æµï¼‰
        
        Args:
            query: ç ”ç©¶ä¸»é¡Œ
            depth: ç ”ç©¶æ·±åº¦
            selected_docs: ç”¨æˆ¶é¸æ“‡çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆå¯é¸ï¼‰
            
        Yields:
            ResearchStep ç”¨æ–¼å‰ç«¯é¡¯ç¤º
        """
        await self.initialize()
        
        # é…ç½®
        config = {
            "quick": {"rounds": 1, "browse": 2, "queries": 2},
            "standard": {"rounds": 2, "browse": 3, "queries": 4},
            "deep": {"rounds": 3, "browse": 4, "queries": 6}
        }.get(depth, {"rounds": 2, "browse": 3, "queries": 4})
        
        all_content = []  # æ”¶é›†æ‰€æœ‰å…§å®¹
        all_sources = []  # æ”¶é›†æ‰€æœ‰ä¾†æº
        browsed_urls = set()
        doc_sources = []  # ç”¨æˆ¶æ–‡ä»¶ä¾†æº
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Step 0: å¦‚æœæœ‰é¸æ“‡æ–‡ä»¶ï¼Œå…ˆæª¢æŸ¥ç›¸é—œæ€§ä¸¦æœå°‹æ–‡ä»¶
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if selected_docs and len(selected_docs) > 0:
            yield ResearchStep(
                step_type="thinking",
                status="running",
                message=f"æ­£åœ¨åˆ†æ {len(selected_docs)} å€‹ç”¨æˆ¶æ–‡ä»¶çš„ç›¸é—œæ€§...",
                data={"documents": selected_docs}
            )
            
            # æª¢æŸ¥æ–‡ä»¶èˆ‡ä¸»é¡Œçš„ç›¸é—œæ€§
            relevance_check = await self._check_document_relevance(query, selected_docs)
            
            if not relevance_check["is_relevant"]:
                yield ResearchStep(
                    step_type="thinking",
                    status="completed",
                    message=f"âš ï¸ {relevance_check['message']}",
                    data={
                        "warning": True,
                        "suggestion": relevance_check.get("suggestion", ""),
                        "documents": selected_docs
                    }
                )
                # å¦‚æœå®Œå…¨ä¸ç›¸é—œï¼Œè©¢å•æ˜¯å¦ç¹¼çºŒï¼ˆé€™è£¡æˆ‘å€‘æœƒç¹¼çºŒä½†æ¨™è¨˜è­¦å‘Šï¼‰
            else:
                yield ResearchStep(
                    step_type="thinking",
                    status="completed",
                    message=f"âœ… æ–‡ä»¶èˆ‡ä¸»é¡Œç›¸é—œï¼Œå°‡æ•´åˆåˆ†æ",
                    data={"documents": selected_docs}
                )
            
            # æœå°‹ç”¨æˆ¶æ–‡ä»¶
            yield ResearchStep(
                step_type="searching",
                status="running",
                message="æ­£åœ¨æœå°‹ç”¨æˆ¶æ–‡ä»¶...",
                data={"type": "rag", "documents": selected_docs}
            )
            
            doc_results = await self._search_user_documents(query, selected_docs)
            
            if doc_results:
                for doc in doc_results:
                    all_content.append({
                        "title": f"[ç”¨æˆ¶æ–‡ä»¶] {doc['source']}",
                        "url": f"file://{doc['source']}",
                        "content": doc['content'],
                        "type": "document"
                    })
                    doc_sources.append({
                        "title": doc['source'],
                        "url": f"file://{doc['source']}",
                        "page": doc.get('page', ''),
                        "type": "document"
                    })
                
                yield ResearchStep(
                    step_type="reading",
                    status="completed",
                    message=f"å¾ç”¨æˆ¶æ–‡ä»¶ä¸­æ‰¾åˆ° {len(doc_results)} å€‹ç›¸é—œç‰‡æ®µ",
                    data={"count": len(doc_results), "sources": [d['source'] for d in doc_results]}
                )
            else:
                yield ResearchStep(
                    step_type="searching",
                    status="completed",
                    message="ç”¨æˆ¶æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ç›´æ¥ç›¸é—œå…§å®¹ï¼Œå°‡ä»¥ç¶²è·¯æœå°‹ç‚ºä¸»",
                    data={}
                )
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Step 1: æ€è€ƒ - åˆ†æå•é¡Œ
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        yield ResearchStep(
            step_type="thinking",
            status="running",
            message="æ­£åœ¨åˆ†æç ”ç©¶ä¸»é¡Œ...",
            data={"query": query}
        )
        
        search_queries = await self._generate_search_queries(query, config["queries"])
        
        yield ResearchStep(
            step_type="thinking",
            status="completed",
            message=f"å·²è¦åŠƒ {len(search_queries)} å€‹æœå°‹æ–¹å‘",
            data={"queries": search_queries}
        )
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Step 2: å¤šè¼ªæœå°‹ + ç€è¦½
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        for round_num in range(config["rounds"]):
            if round_num >= len(search_queries):
                break
            
            current_query = search_queries[round_num]
            
            # æœå°‹
            yield ResearchStep(
                step_type="searching",
                status="running",
                message=f"ç¶²è·¯æœå°‹: {current_query}",
                data={"query": current_query, "round": round_num + 1}
            )
            
            results = await self.search.search(current_query, max_results=8)
            
            if not results:
                yield ResearchStep(
                    step_type="searching",
                    status="failed",
                    message="æœå°‹ç„¡çµæœ",
                    data={"query": current_query}
                )
                continue
            
            yield ResearchStep(
                step_type="searching",
                status="completed",
                message=f"æ‰¾åˆ° {len(results)} å€‹çµæœ",
                data={
                    "query": current_query,
                    "count": len(results),
                    "results": [{"title": r.title, "url": r.url} for r in results[:5]]
                }
            )
            
            # é¸æ“‡è¦ç€è¦½çš„ URL
            urls_to_browse = []
            for r in results:
                if r.url not in browsed_urls and len(urls_to_browse) < config["browse"]:
                    urls_to_browse.append(r)
                    browsed_urls.add(r.url)
            
            # ç€è¦½ç¶²é 
            for i, result in enumerate(urls_to_browse):
                domain = urlparse(result.url).netloc
                
                yield ResearchStep(
                    step_type="browsing",
                    status="running",
                    message=f"æ­£åœ¨ç€è¦½: {domain}",
                    data={"url": result.url, "title": result.title, "index": i + 1, "total": len(urls_to_browse)}
                )
                
                browse_result = await self.browser.browse(result.url, take_screenshot=True)
                
                if browse_result.success and browse_result.content:
                    all_content.append({
                        "title": browse_result.title or result.title,
                        "url": result.url,
                        "content": browse_result.content
                    })
                    all_sources.append({
                        "title": browse_result.title or result.title,
                        "url": result.url
                    })
                    
                    yield ResearchStep(
                        step_type="reading",
                        status="completed",
                        message=f"âœ… å·²è®€å–: {browse_result.title[:50] if browse_result.title else domain}",
                        data={
                            "url": result.url,
                            "title": browse_result.title,
                            "content_length": len(browse_result.content),
                            "screenshot": browse_result.screenshot,  # base64 æˆªåœ–
                            "load_time": browse_result.load_time
                        }
                    )
                else:
                    yield ResearchStep(
                        step_type="browsing",
                        status="failed",
                        message=f"âŒ ç„¡æ³•è®€å–: {domain}",
                        data={"url": result.url, "error": browse_result.error}
                    )
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Step 3: åˆ†æä¸¦ç”Ÿæˆå ±å‘Š
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # åˆä½µæ–‡ä»¶ä¾†æºåˆ°æ‰€æœ‰ä¾†æº
        if doc_sources:
            # ç‚ºæ–‡ä»¶ä¾†æºæ·»åŠ ç´¢å¼•
            for i, ds in enumerate(doc_sources):
                ds["index"] = len(all_sources) + i + 1
            all_sources.extend(doc_sources)
        
        # ç‚ºæ‰€æœ‰ä¾†æºæ·»åŠ ç´¢å¼•ï¼ˆå¦‚æœé‚„æ²’æœ‰ï¼‰
        for i, src in enumerate(all_sources):
            if "index" not in src:
                src["index"] = i + 1
        
        if not all_content:
            yield ResearchStep(
                step_type="error",
                status="failed",
                message="æœªèƒ½æ”¶é›†åˆ°è¶³å¤ çš„ç ”ç©¶è³‡æ–™",
                data={}
            )
            return
        
        yield ResearchStep(
            step_type="analyzing",
            status="running",
            message=f"æ­£åœ¨åˆ†æ {len(all_content)} å€‹ä¾†æºï¼Œç”Ÿæˆå ±å‘Š...",
            data={"source_count": len(all_content), "doc_count": len(doc_sources)}
        )
        
        report = await self._generate_report(query, all_content)
        
        yield ResearchStep(
            step_type="complete",
            status="completed",
            message="ç ”ç©¶å®Œæˆï¼",
            data={
                "report": report,
                "sources": all_sources,
                "stats": {
                    "total_sources": len(all_sources),
                    "pages_browsed": len(browsed_urls),
                    "documents_used": len(doc_sources) if doc_sources else 0
                }
            }
        )
    
    async def _generate_search_queries(self, query: str, num_queries: int) -> List[str]:
        """ä½¿ç”¨ LLM ç”Ÿæˆæœå°‹æŸ¥è©¢"""
        if not self._llm_client:
            return [
                query,
                f"{query} ä»‹ç´¹",
                f"{query} æ•™å­¸",
                f"{query} æ‡‰ç”¨",
                f"{query} æœ€æ–°ç™¼å±•",
                f"{query} å„ªç¼ºé»"
            ][:num_queries]
        
        try:
            response = await self._llm_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": f"""ä½ æ˜¯ç ”ç©¶åŠ©æ‰‹ã€‚æ ¹æ“šç”¨æˆ¶çš„ç ”ç©¶ä¸»é¡Œï¼Œç”Ÿæˆ {num_queries} å€‹ä¸åŒè§’åº¦çš„æœå°‹é—œéµè©ã€‚

æ¯è¡Œä¸€å€‹é—œéµè©ï¼Œä¸è¦ç·¨è™Ÿï¼Œä¸è¦è§£é‡‹ã€‚
é—œéµè©æ‡‰è©²ï¼š
- æ¶µè“‹ä¸åŒé¢å‘ï¼ˆå®šç¾©ã€åŸç†ã€æ‡‰ç”¨ã€æ¯”è¼ƒç­‰ï¼‰
- ä½¿ç”¨é©åˆæœå°‹å¼•æ“çš„ç”¨è©
- ä¸­è‹±æ–‡æ··åˆä½¿ç”¨æ•ˆæœæ›´å¥½"""
                    },
                    {"role": "user", "content": f"ç ”ç©¶ä¸»é¡Œï¼š{query}"}
                ],
                temperature=0.7,
                max_tokens=300
            )
            
            queries = response.choices[0].message.content.strip().split('\n')
            queries = [q.strip() for q in queries if q.strip() and len(q.strip()) > 2]
            return queries[:num_queries]
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæœå°‹æŸ¥è©¢å¤±æ•—: {e}")
            return [query]
    
    async def _generate_report(self, query: str, contents: List[Dict]) -> str:
        """ä½¿ç”¨ LLM ç”Ÿæˆå ±å‘Š - ä½¿ç”¨å°ˆæ¥­ prompts"""
        if not self._llm_client:
            report = f"# {query} ç ”ç©¶å ±å‘Š\n\n"
            for i, c in enumerate(contents, 1):
                source_type = "ğŸ“„ ç”¨æˆ¶æ–‡ä»¶" if c.get('type') == 'document' else "ğŸŒ ç¶²è·¯ä¾†æº"
                report += f"## [{i}] {source_type}: {c['title']}\n\n"
                report += c['content'][:800] + "\n\n"
            return report

        try:
            # å…ˆç”Ÿæˆç ”ç©¶è¨ˆåŠƒ
            plan_prompt = PromptTemplates.get_report_plan_prompt(query)
            plan_response = await self._llm_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": plan_prompt}],
                temperature=0.3,
                max_tokens=500
            )
            report_plan = plan_response.choices[0].message.content

            # æ•´ç†ç ”ç©¶ç™¼ç¾ (learnings)
            learnings = []
            for i, c in enumerate(contents, 1):
                source_type = "ç”¨æˆ¶æ–‡ä»¶" if c.get('type') == 'document' else "ç¶²è·¯ä¾†æº"
                learnings.append(f"- [{i}] ({source_type}) {c['title']}: {c['content'][:400]}...")
            learnings_text = "\n".join(learnings)

            # æ•´ç†ä¾†æº (sources)
            sources_list = []
            for i, c in enumerate(contents, 1):
                source_type = "ç”¨æˆ¶æ–‡ä»¶" if c.get('type') == 'document' else "ç¶²è·¯ä¾†æº"
                sources_list.append(f"- [{i}] [{source_type}] {c['title']} - {c['url']}")
            sources_text = "\n".join(sources_list)

            # çµ±è¨ˆä¾†æºé¡å‹
            doc_count = sum(1 for c in contents if c.get('type') == 'document')
            web_count = len(contents) - doc_count

            # ä½¿ç”¨å°ˆæ¥­çš„æœ€çµ‚å ±å‘Š prompt
            requirement = f"""ç”Ÿæˆè©³ç´°ã€å°ˆæ¥­çš„ç¹é«”ä¸­æ–‡ç ”ç©¶å ±å‘Šã€‚
è³‡æ–™ä¾†æºåŒ…å«ï¼š{web_count} å€‹ç¶²è·¯ä¾†æº" + (f"å’Œ {doc_count} å€‹ç”¨æˆ¶æ–‡ä»¶" if doc_count > 0 else "") + "ã€‚
è«‹ç¶œåˆåˆ†ææ‰€æœ‰è³‡æ–™ï¼Œæä¾›å…¨é¢çš„è¦‹è§£ã€‚"""

            final_prompt = PromptTemplates.get_final_report_prompt(
                plan=report_plan,
                learnings=learnings_text,
                sources=sources_text,
                images="",  # æš«æ™‚ä¸åŒ…å«æˆªåœ–
                requirement=requirement
            )

            # åŠ ä¸Šå¼•ç”¨è¦å‰‡ã€åœ–ç‰‡è¦å‰‡å’Œè¼¸å‡ºæŒ‡å—
            references_prompt = PromptTemplates.get_final_report_references_prompt()
            image_prompt = PromptTemplates.get_final_report_citation_image_prompt()
            output_guidelines = PromptTemplates.get_output_guidelines()
            full_prompt = f"{final_prompt}\n\n{references_prompt}\n\n{image_prompt}\n\n{output_guidelines}"

            response = await self._llm_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "user", "content": full_prompt}

## æ ¼å¼è¦æ±‚
1. ä½¿ç”¨ç¹é«”ä¸­æ–‡
2. ä½¿ç”¨ Markdown æ ¼å¼
3. çµæ§‹ï¼šæ‘˜è¦ â†’ ä¸»è¦å…§å®¹ï¼ˆåˆ†å°ç¯€ï¼‰â†’ çµè«–

## ä¾†æºèªªæ˜
- è³‡æ–™ä¾†æºåˆ†ç‚ºã€Œç¶²è·¯ä¾†æºã€å’Œã€Œç”¨æˆ¶æ–‡ä»¶ã€å…©é¡
- å…©è€…éƒ½åŒç­‰é‡è¦ï¼Œè«‹ç¶œåˆåˆ†æ
- å¦‚æœç”¨æˆ¶æ–‡ä»¶èˆ‡ä¸»é¡Œé«˜åº¦ç›¸é—œï¼Œæ‡‰çµ¦äºˆé©ç•¶æ¬Šé‡

## å¼•ç”¨æ¨™è¨˜ï¼ˆéå¸¸é‡è¦ï¼ï¼‰
- æ¯å€‹äº‹å¯¦ã€æ•¸æ“šã€è§€é»å¾Œé¢éƒ½å¿…é ˆæ¨™è¨»ä¾†æºç·¨è™Ÿï¼Œæ ¼å¼ç‚º [1]ã€[2] ç­‰
- å¼•ç”¨æ¨™è¨˜è¦ç·Šè·Ÿåœ¨ç›¸é—œå¥å­å¾Œé¢ï¼Œåƒ Wikipedia é‚£æ¨£
- ä¾‹å¦‚ï¼šã€Œæ ¹æ“šç ”ç©¶å ±å‘Š[1]ï¼Œè©²æŠ€è¡“å·²è¢«å»£æ³›æ‡‰ç”¨[2][3]ã€‚ã€
- åŒä¸€å¥è©±å¦‚æœå¼•ç”¨å¤šå€‹ä¾†æºï¼Œå¯ä»¥å¯«æˆ [1][2] æˆ– [1,2]

## å¯«ä½œé¢¨æ ¼
- å®¢è§€ã€å°ˆæ¥­ã€å­¸è¡“æ€§
- é‡é»çªå‡º
- é‚è¼¯æ¸…æ™°
- ä¸è¦åœ¨å ±å‘Šæœ«å°¾é‡è¤‡åˆ—å‡ºåƒè€ƒä¾†æºï¼ˆå‰ç«¯æœƒè‡ªå‹•é¡¯ç¤ºï¼‰"""
                    },
                    {
                        "role": "user",
                        "content": f"ç ”ç©¶ä¸»é¡Œï¼š{query}\n\næ”¶é›†åˆ°çš„è³‡æ–™{source_info}ï¼š\n\n{context}\n\nè«‹æ’°å¯«å®Œæ•´çš„ç ”ç©¶å ±å‘Šï¼ˆè¨˜å¾—åœ¨æ¯å€‹äº‹å¯¦å¾Œæ¨™è¨»ä¾†æºç·¨è™Ÿ [1], [2] ç­‰ï¼‰ï¼š"
                    }
                ],
                temperature=0.3,
                max_tokens=4000
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå ±å‘Šå¤±æ•—: {e}")
            return f"# {query}\n\næ”¶é›†åˆ° {len(contents)} å€‹ä¾†æºï¼Œä½†å ±å‘Šç”Ÿæˆå¤±æ•—ï¼š{e}"
    
    async def _check_document_relevance(
        self, 
        query: str, 
        documents: List[str]
    ) -> Dict[str, Any]:
        """
        æª¢æŸ¥ç”¨æˆ¶æ–‡ä»¶èˆ‡ç ”ç©¶ä¸»é¡Œçš„ç›¸é—œæ€§
        
        Returns:
            {
                "is_relevant": bool,
                "message": str,
                "suggestion": str (å¦‚æœä¸ç›¸é—œ)
            }
        """
        if not self._llm_client:
            # æ²’æœ‰ LLMï¼Œå‡è¨­ç›¸é—œ
            return {"is_relevant": True, "message": "æ–‡ä»¶å·²æ·»åŠ "}
        
        try:
            doc_names = ", ".join(documents[:5])  # æœ€å¤šé¡¯ç¤º 5 å€‹
            
            response = await self._llm_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": """ä½ æ˜¯ä¸€å€‹ç ”ç©¶åŠ©æ‰‹ã€‚è«‹åˆ¤æ–·ç”¨æˆ¶é¸æ“‡çš„æ–‡ä»¶æ˜¯å¦èˆ‡ç ”ç©¶ä¸»é¡Œç›¸é—œã€‚

å›è¦†æ ¼å¼ï¼ˆJSONï¼‰ï¼š
{
  "is_relevant": true/false,
  "relevance_score": 0-100,
  "reason": "ç°¡çŸ­èªªæ˜",
  "suggestion": "å¦‚æœä¸ç›¸é—œï¼Œå»ºè­°å¦‚ä½•èª¿æ•´"
}

åˆ¤æ–·æ¨™æº–ï¼š
- æ–‡ä»¶åç¨±åŒ…å«ç›¸é—œé—œéµè© â†’ ç›¸é—œ
- ä¸»é¡Œèˆ‡æ–‡ä»¶å…§å®¹é ˜åŸŸç›¸è¿‘ â†’ ç›¸é—œ
- å®Œå…¨ä¸åŒé ˜åŸŸ â†’ ä¸ç›¸é—œï¼ˆä½†ä»å¯ä½œç‚ºèƒŒæ™¯åƒè€ƒï¼‰"""
                    },
                    {
                        "role": "user",
                        "content": f"ç ”ç©¶ä¸»é¡Œï¼š{query}\n\né¸æ“‡çš„æ–‡ä»¶ï¼š{doc_names}\n\nè«‹åˆ¤æ–·ç›¸é—œæ€§ï¼š"
                    }
                ],
                temperature=0.3,
                max_tokens=200
            )
            
            import json
            try:
                result = json.loads(response.choices[0].message.content.strip())
                return {
                    "is_relevant": result.get("is_relevant", True),
                    "message": result.get("reason", ""),
                    "suggestion": result.get("suggestion", ""),
                    "score": result.get("relevance_score", 50)
                }
            except:
                return {"is_relevant": True, "message": "æ–‡ä»¶å·²æ·»åŠ "}
                
        except Exception as e:
            logger.error(f"æª¢æŸ¥ç›¸é—œæ€§å¤±æ•—: {e}")
            return {"is_relevant": True, "message": "æ–‡ä»¶å·²æ·»åŠ "}
    
    async def _search_user_documents(
        self,
        query: str,
        documents: List[str],
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ RAG æœå°‹ç”¨æˆ¶æ–‡ä»¶
        
        Args:
            query: æœå°‹æŸ¥è©¢
            documents: æ–‡ä»¶åç¨±åˆ—è¡¨
            top_k: è¿”å›çµæœæ•¸é‡
            
        Returns:
            [{
                "content": str,
                "source": str (æ–‡ä»¶å),
                "page": str,
                "score": float
            }]
        """
        try:
            import cohere
            from qdrant_client import QdrantClient
            from qdrant_client.models import Filter, FieldCondition, MatchAny
            
            # ç¢ºä¿ç’°å¢ƒè®Šæ•¸
            from core.utils import load_env
            load_env()
            
            cohere_key = os.getenv("COHERE_API_KEY")
            if not cohere_key:
                logger.error("COHERE_API_KEY æœªè¨­ç½®")
                return []
            
            # åˆå§‹åŒ–å®¢æˆ¶ç«¯
            cohere_client = cohere.Client(cohere_key)
            qdrant_client = QdrantClient(host="localhost", port=6333)
            
            # ç”ŸæˆæŸ¥è©¢å‘é‡
            embed_response = cohere_client.embed(
                texts=[query],
                model="embed-multilingual-v3.0",
                input_type="search_query"
            )
            query_vector = embed_response.embeddings[0]
            
            # å»ºç«‹æ–‡ä»¶éæ¿¾æ¢ä»¶
            search_filter = Filter(
                must=[
                    FieldCondition(
                        key="file_name",
                        match=MatchAny(any=documents)
                    )
                ]
            )
            
            # åŸ·è¡Œæœå°‹
            results = qdrant_client.query_points(
                collection_name="rag_knowledge_base",
                query=query_vector,
                query_filter=search_filter,
                limit=top_k,
                with_payload=True
            )
            
            return [
                {
                    "content": point.payload.get("text", ""),
                    "source": point.payload.get("file_name", ""),
                    "page": point.payload.get("page_label", "1"),
                    "score": point.score
                }
                for point in results.points
                if point.payload.get("text")
            ]
            
        except Exception as e:
            logger.error(f"æœå°‹ç”¨æˆ¶æ–‡ä»¶å¤±æ•—: {e}")
            return []


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# å…¨åŸŸå¯¦ä¾‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_research_agent = None

def get_research_agent() -> DeepResearchAgent:
    """ç²å–å…¨åŸŸ DeepResearchAgent å¯¦ä¾‹"""
    global _research_agent
    if _research_agent is None:
        _research_agent = DeepResearchAgent()
    return _research_agent


# å°å‡º
__all__ = [
    "BrowserService",
    "SearchService", 
    "DeepResearchAgent",
    "SearchResult",
    "BrowseResult",
    "ResearchStep",
    "get_research_agent",
    "PLAYWRIGHT_AVAILABLE"
]
