"""
Deep Research Service - æ·±åº¦ç ”ç©¶æœå‹™

å¯¦ç¾é¡ä¼¼ Manus çš„å¤šè¼ªæœå°‹ã€ç¶²é ç€è¦½ã€å…§å®¹æ•´åˆåŠŸèƒ½

ç‰¹é»ï¼š
1. å¤šé—œéµè©ä¸¦è¡Œæœå°‹
2. è‡ªå‹•æŠ“å–ç¶²é å…§å®¹
3. å¤±æ•—é‡è©¦èˆ‡é—œéµè©æ“´å±•
4. SSE å³æ™‚é€²åº¦å›å ±
5. LLM å…§å®¹æ•´åˆ
"""

import os
import asyncio
import aiohttp
import logging
import re
from typing import List, Dict, Any, Optional, AsyncGenerator
from dataclasses import dataclass, asdict
from datetime import datetime
from urllib.parse import quote_plus, urlparse
from bs4 import BeautifulSoup

from opencode.core.utils import load_env
load_env()

logger = logging.getLogger(__name__)


@dataclass
class SearchResult:
    """æœå°‹çµæœ"""
    title: str
    url: str
    snippet: str
    source: str = ""
    content: str = ""  # å®Œæ•´ç¶²é å…§å®¹ï¼ˆfetch å¾Œå¡«å…¥ï¼‰
    fetched: bool = False


@dataclass
class ResearchStep:
    """ç ”ç©¶æ­¥é©Ÿï¼ˆç”¨æ–¼ SSE å›å ±ï¼‰"""
    step_type: str  # search, fetch, analyze, error
    status: str     # running, completed, failed
    message: str
    data: Dict[str, Any] = None
    
    def to_dict(self):
        return asdict(self)


class DeepResearchService:
    """
    æ·±åº¦ç ”ç©¶æœå‹™
    
    å·¥ä½œæµç¨‹ï¼š
    1. æ“´å±•æœå°‹æŸ¥è©¢ï¼ˆç”Ÿæˆå¤šå€‹ç›¸é—œé—œéµè©ï¼‰
    2. ä¸¦è¡ŒåŸ·è¡Œå¤šå€‹æœå°‹
    3. åˆä½µçµæœï¼Œé¸æ“‡ top URLs
    4. ä¸¦è¡ŒæŠ“å–ç¶²é å…§å®¹
    5. LLM æ•´åˆåˆ†æ
    6. ç”Ÿæˆçµæ§‹åŒ–å ±å‘Š
    """
    
    def __init__(self):
        self.search_providers = []
        self.max_search_results = 10
        self.max_fetch_urls = 5
        self.fetch_timeout = 15
        self._session: Optional[aiohttp.ClientSession] = None
        
        # API Keys
        self.tavily_key = os.getenv("TAVILY_API_KEY")
        self.serpapi_key = os.getenv("SERPAPI_KEY")
        self.serper_key = os.getenv("SERPER_API_KEY")  # å¦ä¸€å€‹æœå°‹ API
        
    async def _get_session(self) -> aiohttp.ClientSession:
        """ç²å– HTTP session"""
        if self._session is None or self._session.closed:
            timeout = aiohttp.ClientTimeout(total=30)
            self._session = aiohttp.ClientSession(timeout=timeout)
        return self._session
    
    async def close(self):
        """é—œé–‰ session"""
        if self._session and not self._session.closed:
            await self._session.close()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # æœå°‹æ–¹æ³•
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def search_tavily(self, query: str, max_results: int = 5) -> List[SearchResult]:
        """Tavily AI æœå°‹ï¼ˆæ¨è–¦ï¼Œæ•ˆæœæœ€å¥½ï¼‰"""
        if not self.tavily_key:
            return []
        
        try:
            session = await self._get_session()
            async with session.post(
                "https://api.tavily.com/search",
                json={
                    "api_key": self.tavily_key,
                    "query": query,
                    "search_depth": "advanced",
                    "include_answer": True,
                    "include_raw_content": False,
                    "max_results": max_results
                }
            ) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    results = []
                    for r in data.get("results", []):
                        results.append(SearchResult(
                            title=r.get("title", ""),
                            url=r.get("url", ""),
                            snippet=r.get("content", ""),
                            source="Tavily"
                        ))
                    logger.info(f"âœ… Tavily æ‰¾åˆ° {len(results)} å€‹çµæœ")
                    return results
        except Exception as e:
            logger.error(f"âŒ Tavily æœå°‹å¤±æ•—: {e}")
        return []
    
    async def search_serper(self, query: str, max_results: int = 5) -> List[SearchResult]:
        """Serper.dev Google æœå°‹"""
        if not self.serper_key:
            return []
        
        try:
            session = await self._get_session()
            async with session.post(
                "https://google.serper.dev/search",
                headers={"X-API-KEY": self.serper_key},
                json={"q": query, "num": max_results}
            ) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    results = []
                    for r in data.get("organic", []):
                        results.append(SearchResult(
                            title=r.get("title", ""),
                            url=r.get("link", ""),
                            snippet=r.get("snippet", ""),
                            source="Serper/Google"
                        ))
                    logger.info(f"âœ… Serper æ‰¾åˆ° {len(results)} å€‹çµæœ")
                    return results
        except Exception as e:
            logger.error(f"âŒ Serper æœå°‹å¤±æ•—: {e}")
        return []
    
    async def search_duckduckgo_html(self, query: str, max_results: int = 5) -> List[SearchResult]:
        """DuckDuckGo HTML çˆ¬å–ï¼ˆå…è²»ä½†è¼ƒæ…¢ï¼‰"""
        try:
            session = await self._get_session()
            url = f"https://html.duckduckgo.com/html/?q={quote_plus(query)}"
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
            
            async with session.get(url, headers=headers) as resp:
                if resp.status == 200:
                    html = await resp.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    results = []
                    
                    for result in soup.select('.result')[:max_results]:
                        title_elem = result.select_one('.result__title a')
                        snippet_elem = result.select_one('.result__snippet')
                        
                        if title_elem:
                            # DuckDuckGo çš„ URL æ˜¯é‡å®šå‘æ ¼å¼ï¼Œéœ€è¦æå–çœŸå¯¦ URL
                            href = title_elem.get('href', '')
                            # å˜—è©¦å¾ href æå–çœŸå¯¦ URL
                            if 'uddg=' in href:
                                import urllib.parse
                                parsed = urllib.parse.parse_qs(urllib.parse.urlparse(href).query)
                                real_url = parsed.get('uddg', [href])[0]
                            else:
                                real_url = href
                            
                            results.append(SearchResult(
                                title=title_elem.get_text(strip=True),
                                url=real_url,
                                snippet=snippet_elem.get_text(strip=True) if snippet_elem else "",
                                source="DuckDuckGo"
                            ))
                    
                    logger.info(f"âœ… DuckDuckGo HTML æ‰¾åˆ° {len(results)} å€‹çµæœ")
                    return results
        except Exception as e:
            logger.error(f"âŒ DuckDuckGo HTML æœå°‹å¤±æ•—: {e}")
        return []
    
    async def search_bing(self, query: str, max_results: int = 5) -> List[SearchResult]:
        """Bing æœå°‹ï¼ˆçˆ¬å–æ–¹å¼ï¼‰"""
        try:
            session = await self._get_session()
            url = f"https://www.bing.com/search?q={quote_plus(query)}&count={max_results}"
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.8"
            }
            
            async with session.get(url, headers=headers) as resp:
                if resp.status == 200:
                    html = await resp.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    results = []
                    
                    for item in soup.select('.b_algo')[:max_results]:
                        title_elem = item.select_one('h2 a')
                        snippet_elem = item.select_one('.b_caption p')
                        
                        if title_elem:
                            results.append(SearchResult(
                                title=title_elem.get_text(strip=True),
                                url=title_elem.get('href', ''),
                                snippet=snippet_elem.get_text(strip=True) if snippet_elem else "",
                                source="Bing"
                            ))
                    
                    logger.info(f"âœ… Bing æ‰¾åˆ° {len(results)} å€‹çµæœ")
                    return results
        except Exception as e:
            logger.error(f"âŒ Bing æœå°‹å¤±æ•—: {e}")
        return []
    
    async def multi_search(self, query: str, max_results: int = 10) -> List[SearchResult]:
        """
        å¤šå¼•æ“ä¸¦è¡Œæœå°‹
        
        å„ªå…ˆç´šï¼šTavily > Serper > Bing > DuckDuckGo
        """
        logger.info(f"ğŸ” é–‹å§‹å¤šå¼•æ“æœå°‹: {query}")
        
        # ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰æœå°‹
        tasks = [
            self.search_tavily(query, max_results),
            self.search_serper(query, max_results),
            self.search_bing(query, max_results),
            self.search_duckduckgo_html(query, max_results),
        ]
        
        all_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # åˆä½µçµæœï¼Œå»é‡
        seen_urls = set()
        merged = []
        
        for results in all_results:
            if isinstance(results, Exception):
                continue
            for r in results:
                if r.url and r.url not in seen_urls:
                    seen_urls.add(r.url)
                    merged.append(r)
        
        logger.info(f"âœ… å¤šå¼•æ“æœå°‹å®Œæˆï¼Œå…± {len(merged)} å€‹ä¸é‡è¤‡çµæœ")
        return merged[:max_results]
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ç¶²é æŠ“å–
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def fetch_url(self, url: str) -> Optional[str]:
        """æŠ“å–ç¶²é å…§å®¹"""
        try:
            session = await self._get_session()
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "Accept": "text/html,application/xhtml+xml",
                "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.8"
            }
            
            async with session.get(url, headers=headers, timeout=self.fetch_timeout) as resp:
                if resp.status == 200:
                    html = await resp.text()
                    
                    # æå–ä¸»è¦å…§å®¹
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                    for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'iframe', 'noscript']):
                        tag.decompose()
                    
                    # å˜—è©¦æ‰¾ä¸»è¦å…§å®¹å€å¡Š
                    main_content = (
                        soup.find('article') or 
                        soup.find('main') or 
                        soup.find(class_=re.compile(r'content|article|post|entry')) or
                        soup.find('body')
                    )
                    
                    if main_content:
                        # æå–æ–‡å­—ï¼Œé™åˆ¶é•·åº¦
                        text = main_content.get_text(separator='\n', strip=True)
                        # æ¸…ç†å¤šé¤˜ç©ºè¡Œ
                        text = re.sub(r'\n{3,}', '\n\n', text)
                        # é™åˆ¶é•·åº¦
                        if len(text) > 5000:
                            text = text[:5000] + "...[å…§å®¹æˆªæ–·]"
                        return text
                    
        except asyncio.TimeoutError:
            logger.warning(f"â±ï¸ æŠ“å–è¶…æ™‚: {url}")
        except Exception as e:
            logger.error(f"âŒ æŠ“å–å¤±æ•— {url}: {e}")
        
        return None
    
    async def fetch_multiple(self, urls: List[str]) -> Dict[str, str]:
        """ä¸¦è¡ŒæŠ“å–å¤šå€‹ç¶²é """
        logger.info(f"ğŸ“¥ é–‹å§‹æŠ“å– {len(urls)} å€‹ç¶²é ...")
        
        tasks = [self.fetch_url(url) for url in urls]
        results = await asyncio.gather(*tasks)
        
        content_map = {}
        success_count = 0
        for url, content in zip(urls, results):
            if content:
                content_map[url] = content
                success_count += 1
        
        logger.info(f"âœ… æˆåŠŸæŠ“å– {success_count}/{len(urls)} å€‹ç¶²é ")
        return content_map
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # æ·±åº¦ç ”ç©¶ä¸»æµç¨‹
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def research(
        self,
        query: str,
        expand_queries: bool = True,
        max_urls: int = 5
    ) -> AsyncGenerator[ResearchStep, None]:
        """
        åŸ·è¡Œæ·±åº¦ç ”ç©¶ï¼ˆSSE ä¸²æµï¼‰
        
        Args:
            query: åŸå§‹æŸ¥è©¢
            expand_queries: æ˜¯å¦æ“´å±•æŸ¥è©¢é—œéµè©
            max_urls: æœ€å¤šæŠ“å–å¹¾å€‹ç¶²é 
            
        Yields:
            ResearchStep ç‰©ä»¶ï¼ˆç”¨æ–¼å‰ç«¯é¡¯ç¤ºé€²åº¦ï¼‰
        """
        all_results = []
        
        # Step 1: æ“´å±•æŸ¥è©¢
        queries = [query]
        if expand_queries:
            # ç”Ÿæˆç›¸é—œæŸ¥è©¢è®Šé«”
            queries.extend([
                f"{query} æœ€æ–°",
                f"{query} æ•™å­¸",
                f"{query} æ‡‰ç”¨",
            ])
        
        yield ResearchStep(
            step_type="search",
            status="running",
            message=f"æ­£åœ¨æœå°‹ {len(queries)} å€‹æŸ¥è©¢...",
            data={"queries": queries}
        )
        
        # Step 2: ä¸¦è¡Œæœå°‹
        for i, q in enumerate(queries):
            yield ResearchStep(
                step_type="search",
                status="running",
                message=f"æœå°‹ä¸­ ({i+1}/{len(queries)}): {q}",
                data={"query": q}
            )
            
            results = await self.multi_search(q, max_results=5)
            all_results.extend(results)
            
            yield ResearchStep(
                step_type="search",
                status="completed",
                message=f"æ‰¾åˆ° {len(results)} å€‹çµæœ",
                data={"query": q, "count": len(results)}
            )
        
        # å»é‡
        seen_urls = set()
        unique_results = []
        for r in all_results:
            if r.url not in seen_urls:
                seen_urls.add(r.url)
                unique_results.append(r)
        
        if not unique_results:
            yield ResearchStep(
                step_type="error",
                status="failed",
                message="æœå°‹æœªæ‰¾åˆ°ä»»ä½•çµæœï¼Œè«‹å˜—è©¦ä¸åŒçš„é—œéµè©"
            )
            return
        
        yield ResearchStep(
            step_type="search",
            status="completed",
            message=f"æœå°‹å®Œæˆï¼Œå…± {len(unique_results)} å€‹ä¸é‡è¤‡çµæœ",
            data={"total": len(unique_results)}
        )
        
        # Step 3: æŠ“å–ç¶²é å…§å®¹
        urls_to_fetch = [r.url for r in unique_results[:max_urls]]
        
        yield ResearchStep(
            step_type="fetch",
            status="running",
            message=f"æ­£åœ¨ç€è¦½ {len(urls_to_fetch)} å€‹ç¶²é ...",
            data={"urls": urls_to_fetch}
        )
        
        for i, url in enumerate(urls_to_fetch):
            domain = urlparse(url).netloc
            yield ResearchStep(
                step_type="fetch",
                status="running",
                message=f"æ­£åœ¨ç€è¦½ ({i+1}/{len(urls_to_fetch)}): {domain}",
                data={"url": url}
            )
            
            content = await self.fetch_url(url)
            
            if content:
                # æ›´æ–°å°æ‡‰çš„ SearchResult
                for r in unique_results:
                    if r.url == url:
                        r.content = content
                        r.fetched = True
                        break
                
                yield ResearchStep(
                    step_type="fetch",
                    status="completed",
                    message=f"âœ… æˆåŠŸæŠ“å–: {domain}",
                    data={"url": url, "length": len(content)}
                )
            else:
                yield ResearchStep(
                    step_type="fetch",
                    status="failed",
                    message=f"âŒ æŠ“å–å¤±æ•—: {domain}",
                    data={"url": url}
                )
        
        # Step 4: æ•´ç†çµæœ
        fetched_results = [r for r in unique_results if r.fetched]
        
        yield ResearchStep(
            step_type="analyze",
            status="running",
            message="æ­£åœ¨æ•´ç†ç ”ç©¶çµæœ...",
            data={"fetched_count": len(fetched_results)}
        )
        
        # è¿”å›æœ€çµ‚çµæœ
        yield ResearchStep(
            step_type="analyze",
            status="completed",
            message="ç ”ç©¶å®Œæˆ",
            data={
                "results": [asdict(r) for r in unique_results],
                "fetched_count": len(fetched_results),
                "total_count": len(unique_results)
            }
        )
    
    async def research_sync(self, query: str, max_urls: int = 5) -> Dict[str, Any]:
        """
        åŒæ­¥ç‰ˆæœ¬çš„æ·±åº¦ç ”ç©¶ï¼ˆä¸ç”¨ SSEï¼‰
        
        Returns:
            {"results": [...], "fetched_count": int, "summary": str}
        """
        final_data = {}
        
        async for step in self.research(query, max_urls=max_urls):
            if step.step_type == "analyze" and step.status == "completed":
                final_data = step.data or {}
        
        return final_data


# å…¨åŸŸå¯¦ä¾‹
_deep_research_service = None

def get_deep_research_service() -> DeepResearchService:
    """ç²å–å…¨åŸŸ DeepResearchService å¯¦ä¾‹"""
    global _deep_research_service
    if _deep_research_service is None:
        _deep_research_service = DeepResearchService()
    return _deep_research_service
