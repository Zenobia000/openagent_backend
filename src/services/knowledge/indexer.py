"""
Indexer - ä½¿ç”¨ Cohere Embedding ç´¢å¼•æ–‡ä»¶åˆ° Qdrant
æ”¯æ´ Cohere å’Œ OpenAI é›™ provider
æ–°å¢ï¼šè‡ªå‹•é‡è©¦ã€é€Ÿç‡é™åˆ¶è™•ç†ã€å›é€€æ©Ÿåˆ¶
"""

import os
import logging
import uuid
import time
from typing import List, Dict, Any, Optional

# ä½¿ç”¨çµ±ä¸€çš„è·¯å¾‘å·¥å…·è¼‰å…¥ç’°å¢ƒè®Šæ•¸
from core.utils import load_env
load_env()

logger = logging.getLogger(__name__)

# å…¨åŸŸå¯¦ä¾‹
_indexer_instance = None


class Indexer:
    """
    æ–‡ä»¶ç´¢å¼•å™¨ - æ”¯æ´ Cohere å’Œ OpenAI embedding
    
    Cohere å„ªå‹¢ï¼š
    1. å¤šèªè¨€æ”¯æ´ (embed-multilingual-v3.0)
    2. å€åˆ† document å’Œ query embedding (input_type)
    3. è¼ƒä½æˆæœ¬
    
    æ–°å¢åŠŸèƒ½ï¼š
    - è‡ªå‹•é‡è©¦ (æœ€å¤š 3 æ¬¡ï¼Œå¸¶æŒ‡æ•¸é€€é¿)
    - é€Ÿç‡é™åˆ¶è™•ç† (429 éŒ¯èª¤æ™‚å»¶é²é‡è©¦)
    - å›é€€æ©Ÿåˆ¶ (Cohere å¤±æ•—æ™‚ä½¿ç”¨ OpenAI)
    - æ‰¹é‡ embedding (æ¸›å°‘ API èª¿ç”¨)
    """
    
    def __init__(
        self,
        collection_name: str = "rag_knowledge_base",
        qdrant_url: str = "http://localhost:6333"
    ):
        self.collection_name = collection_name
        self.qdrant_url = qdrant_url
        
        # API clients
        self.cohere_client = None
        self.openai_client = None
        self.qdrant_client = None
        
        # Embedding è¨­å®š
        self.embed_provider = None
        self.embed_model = None
        self.embed_dim = None
        
        # é‡è©¦è¨­å®š
        self.max_retries = 3
        self.base_delay = 2  # åŸºç¤å»¶é²ç§’æ•¸
        self.batch_size = 20  # æ‰¹é‡è™•ç†å¤§å°
        
        # æ˜¯å¦æœ‰ OpenAI ä½œç‚ºå‚™ç”¨
        self._has_openai_fallback = False
        
        self._initialize()
    
    def _initialize(self):
        """åˆå§‹åŒ– clients å’Œè¨­å®š"""
        # ç¢ºä¿ç’°å¢ƒè®Šæ•¸å·²è¼‰å…¥
        load_env()
        
        cohere_key = os.getenv("COHERE_API_KEY")
        openai_key = os.getenv("OPENAI_API_KEY")
        
        # å„ªå…ˆä½¿ç”¨ Cohere
        if cohere_key:
            try:
                import cohere
                self.cohere_client = cohere.Client(api_key=cohere_key)
                self.embed_provider = "cohere"
                self.embed_model = os.getenv("COHERE_EMBED_MODEL", "embed-multilingual-v3.0")
                self.embed_dim = 1024  # Cohere v3 æ¨¡å‹å›ºå®š 1024 ç¶­
                logger.info(f"âœ… [Indexer] ä½¿ç”¨ Cohere embedding: {self.embed_model}")
            except ImportError:
                logger.warning("âš ï¸ [Indexer] cohere å¥—ä»¶æœªå®‰è£")
            except Exception as e:
                logger.error(f"âŒ [Indexer] Cohere åˆå§‹åŒ–å¤±æ•—: {e}")
        
        # OpenAI ä½œç‚ºå‚™ç”¨æˆ–ä¸»è¦
        if openai_key:
            try:
                from openai import OpenAI
                self.openai_client = OpenAI(api_key=openai_key)
                
                if not self.cohere_client:
                    # æ²’æœ‰ Cohereï¼Œä½¿ç”¨ OpenAI ä½œç‚ºä¸»è¦
                    self.embed_provider = "openai"
                    self.embed_model = "text-embedding-3-small"
                    self.embed_dim = 1536
                    logger.info(f"âœ… [Indexer] ä½¿ç”¨ OpenAI embedding: {self.embed_model}")
                else:
                    # æœ‰ Cohereï¼ŒOpenAI ä½œç‚ºå‚™ç”¨
                    self._has_openai_fallback = True
                    logger.info(f"âœ… [Indexer] OpenAI ä½œç‚ºå‚™ç”¨ embedding provider")
                    
            except ImportError:
                logger.warning("âš ï¸ [Indexer] openai å¥—ä»¶æœªå®‰è£")
            except Exception as e:
                logger.error(f"âŒ [Indexer] OpenAI åˆå§‹åŒ–å¤±æ•—: {e}")
        
        if not self.cohere_client and not self.openai_client:
            logger.error("âŒ [Indexer] æ²’æœ‰å¯ç”¨çš„ embedding providerï¼")
            logger.error("   è«‹è¨­å®š COHERE_API_KEY æˆ– OPENAI_API_KEY")
            raise ValueError("éœ€è¦è¨­å®š COHERE_API_KEY æˆ– OPENAI_API_KEY")
        
        # åˆå§‹åŒ– Qdrant
        self._init_qdrant()
    
    def _init_qdrant(self):
        """åˆå§‹åŒ– Qdrant client å’Œ collection"""
        try:
            from qdrant_client import QdrantClient
            from qdrant_client.models import Distance, VectorParams
            
            self.qdrant_client = QdrantClient(url=self.qdrant_url)
            
            # æª¢æŸ¥ collection æ˜¯å¦å­˜åœ¨
            collections = self.qdrant_client.get_collections().collections
            collection_names = [c.name for c in collections]
            
            if self.collection_name not in collection_names:
                # å‰µå»ºæ–° collection
                logger.info(f"ğŸ“¦ [Indexer] å‰µå»ºæ–° collection: {self.collection_name}")
                logger.info(f"ğŸ“¦ [Indexer] å‘é‡ç¶­åº¦: {self.embed_dim}")
                
                self.qdrant_client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=VectorParams(
                        size=self.embed_dim,
                        distance=Distance.COSINE
                    )
                )
                logger.info(f"âœ… [Indexer] Collection å‰µå»ºæˆåŠŸ")
            else:
                # æª¢æŸ¥ç¶­åº¦æ˜¯å¦åŒ¹é…
                collection_info = self.qdrant_client.get_collection(self.collection_name)
                existing_dim = collection_info.config.params.vectors.size
                
                if existing_dim != self.embed_dim:
                    logger.warning(f"âš ï¸ [Indexer] ç¶­åº¦ä¸åŒ¹é…ï¼")
                    logger.warning(f"   Collection ç¶­åº¦: {existing_dim}")
                    logger.warning(f"   ç•¶å‰ provider ç¶­åº¦: {self.embed_dim}")
                    logger.warning(f"   è«‹é‡ç½® collection æˆ–åˆ‡æ› embedding provider")
                else:
                    logger.info(f"âœ… [Indexer] Collection å·²å­˜åœ¨ï¼Œç¶­åº¦åŒ¹é…: {existing_dim}")
                    
        except Exception as e:
            logger.error(f"âŒ [Indexer] Qdrant åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    def get_embedding(self, text: str, input_type: str = "search_document") -> List[float]:
        """
        å–å¾—æ–‡å­—çš„ embedding å‘é‡ï¼ˆå¸¶é‡è©¦å’Œå›é€€ï¼‰
        
        Args:
            text: è¼¸å…¥æ–‡å­—
            input_type: Cohere å°ˆç”¨
                - "search_document": ç´¢å¼•æ–‡ä»¶æ™‚ä½¿ç”¨
                - "search_query": æŸ¥è©¢æ™‚ä½¿ç”¨
                
        Returns:
            embedding å‘é‡
        """
        if self.cohere_client and self.embed_provider == "cohere":
            try:
                return self._get_cohere_embedding_with_retry(text, input_type)
            except Exception as e:
                if self._has_openai_fallback:
                    logger.warning(f"âš ï¸ [Indexer] Cohere å¤±æ•—ï¼Œå›é€€åˆ° OpenAI: {e}")
                    return self._get_openai_embedding(text)
                raise
        else:
            return self._get_openai_embedding(text)
    
    def get_embeddings_batch(self, texts: List[str], input_type: str = "search_document") -> List[List[float]]:
        """
        æ‰¹é‡å–å¾— embeddingï¼ˆæ¸›å°‘ API èª¿ç”¨æ¬¡æ•¸ï¼‰
        
        Args:
            texts: æ–‡å­—åˆ—è¡¨
            input_type: Cohere å°ˆç”¨
            
        Returns:
            embedding å‘é‡åˆ—è¡¨
        """
        if self.cohere_client and self.embed_provider == "cohere":
            try:
                return self._get_cohere_embeddings_batch_with_retry(texts, input_type)
            except Exception as e:
                if self._has_openai_fallback:
                    logger.warning(f"âš ï¸ [Indexer] Cohere æ‰¹é‡å¤±æ•—ï¼Œå›é€€åˆ° OpenAI: {e}")
                    return self._get_openai_embeddings_batch(texts)
                raise
        else:
            return self._get_openai_embeddings_batch(texts)
    
    def _get_cohere_embedding_with_retry(self, text: str, input_type: str) -> List[float]:
        """ä½¿ç”¨ Cohere å–å¾— embeddingï¼ˆå¸¶é‡è©¦ï¼‰"""
        last_error = None
        
        for attempt in range(self.max_retries):
            try:
                response = self.cohere_client.embed(
                    texts=[text],
                    model=self.embed_model,
                    input_type=input_type
                )
                return response.embeddings[0]
                
            except Exception as e:
                last_error = e
                error_str = str(e)
                
                # æª¢æŸ¥æ˜¯å¦æ˜¯é€Ÿç‡é™åˆ¶éŒ¯èª¤
                if "429" in error_str or "rate" in error_str.lower() or "too many" in error_str.lower():
                    delay = self.base_delay * (2 ** attempt)  # æŒ‡æ•¸é€€é¿
                    logger.warning(f"âš ï¸ [Indexer] é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {delay} ç§’å¾Œé‡è©¦ (å˜—è©¦ {attempt + 1}/{self.max_retries})")
                    time.sleep(delay)
                else:
                    # å…¶ä»–éŒ¯èª¤ç›´æ¥æ‹‹å‡º
                    logger.error(f"âŒ [Indexer] Cohere embedding å¤±æ•—: {e}")
                    raise
        
        # æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—
        logger.error(f"âŒ [Indexer] Cohere embedding é‡è©¦ {self.max_retries} æ¬¡å¾Œä»å¤±æ•—")
        raise last_error
    
    def _get_cohere_embeddings_batch_with_retry(self, texts: List[str], input_type: str) -> List[List[float]]:
        """æ‰¹é‡ Cohere embeddingï¼ˆå¸¶é‡è©¦ï¼‰"""
        last_error = None
        
        for attempt in range(self.max_retries):
            try:
                response = self.cohere_client.embed(
                    texts=texts,
                    model=self.embed_model,
                    input_type=input_type
                )
                return response.embeddings
                
            except Exception as e:
                last_error = e
                error_str = str(e)
                
                if "429" in error_str or "rate" in error_str.lower() or "too many" in error_str.lower():
                    delay = self.base_delay * (2 ** attempt)
                    logger.warning(f"âš ï¸ [Indexer] é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {delay} ç§’å¾Œé‡è©¦ (å˜—è©¦ {attempt + 1}/{self.max_retries})")
                    time.sleep(delay)
                else:
                    logger.error(f"âŒ [Indexer] Cohere æ‰¹é‡ embedding å¤±æ•—: {e}")
                    raise
        
        logger.error(f"âŒ [Indexer] Cohere æ‰¹é‡ embedding é‡è©¦ {self.max_retries} æ¬¡å¾Œä»å¤±æ•—")
        raise last_error
    
    def _get_openai_embedding(self, text: str) -> List[float]:
        """ä½¿ç”¨ OpenAI å–å¾— embedding"""
        try:
            response = self.openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"âŒ [Indexer] OpenAI embedding å¤±æ•—: {e}")
            raise
    
    def _get_openai_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡ OpenAI embedding"""
        try:
            response = self.openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=texts
            )
            # æŒ‰åŸå§‹é †åºè¿”å›
            return [item.embedding for item in sorted(response.data, key=lambda x: x.index)]
        except Exception as e:
            logger.error(f"âŒ [Indexer] OpenAI æ‰¹é‡ embedding å¤±æ•—: {e}")
            raise
    
    def index_documents(self, documents: List[Dict[str, Any]]) -> int:
        """
        ç´¢å¼•æ–‡ä»¶åˆ° Qdrantï¼ˆä½¿ç”¨æ‰¹é‡è™•ç†ï¼‰
        
        Args:
            documents: æ–‡ä»¶åˆ—è¡¨ï¼Œæ¯å€‹åŒ…å« text å’Œ metadata
            
        Returns:
            æˆåŠŸç´¢å¼•çš„æ•¸é‡
        """
        if not documents:
            logger.warning("âš ï¸ [Indexer] æ²’æœ‰æ–‡ä»¶éœ€è¦ç´¢å¼•")
            return 0
        
        from qdrant_client.models import PointStruct
        
        logger.info(f"ğŸ’¾ [Indexer] ====== é–‹å§‹ç´¢å¼• ======")
        logger.info(f"ğŸ’¾ [Indexer] æ–‡ä»¶æ•¸é‡: {len(documents)}")
        logger.info(f"ğŸ’¾ [Indexer] Provider: {self.embed_provider}")
        logger.info(f"ğŸ’¾ [Indexer] æ‰¹é‡å¤§å°: {self.batch_size}")
        if self._has_openai_fallback:
            logger.info(f"ğŸ’¾ [Indexer] å‚™ç”¨ Provider: OpenAI")
        
        points = []
        success_count = 0
        
        # æ‰¹é‡è™•ç†
        for batch_start in range(0, len(documents), self.batch_size):
            batch_end = min(batch_start + self.batch_size, len(documents))
            batch = documents[batch_start:batch_end]
            
            # æº–å‚™æ‰¹é‡æ–‡æœ¬
            texts = []
            valid_docs = []
            
            for doc in batch:
                text = doc.get("text", "")
                if text.strip():
                    texts.append(text)
                    valid_docs.append(doc)
            
            if not texts:
                continue
            
            try:
                # æ‰¹é‡ç²å– embedding
                logger.info(f"ğŸ’¾ [Indexer] è™•ç†æ‰¹æ¬¡ {batch_start + 1}-{batch_end}/{len(documents)}...")
                embeddings = self.get_embeddings_batch(texts, input_type="search_document")
                
                # å‰µå»º points
                for i, (doc, vector) in enumerate(zip(valid_docs, embeddings)):
                    metadata = doc.get("metadata", {})
                    payload = {
                        "text": doc["text"],
                        **metadata
                    }
                    
                    points.append(PointStruct(
                        id=str(uuid.uuid4()),
                        vector=vector,
                        payload=payload
                    ))
                    success_count += 1
                
                # æ¯æ‰¹æ¬¡å¾ŒçŸ­æš«å»¶é²ï¼Œé¿å…é€Ÿç‡é™åˆ¶
                if batch_end < len(documents):
                    time.sleep(0.5)
                    
            except Exception as e:
                logger.error(f"âŒ [Indexer] æ‰¹æ¬¡ {batch_start + 1}-{batch_end} å¤±æ•—: {e}")
                
                # å¦‚æœæ‰¹é‡å¤±æ•—ï¼Œå˜—è©¦é€å€‹è™•ç†
                logger.info(f"ğŸ”„ [Indexer] å˜—è©¦é€å€‹è™•ç†...")
                for doc in valid_docs:
                    try:
                        text = doc.get("text", "")
                        vector = self.get_embedding(text, input_type="search_document")
                        
                        metadata = doc.get("metadata", {})
                        payload = {
                            "text": text,
                            **metadata
                        }
                        
                        points.append(PointStruct(
                            id=str(uuid.uuid4()),
                            vector=vector,
                            payload=payload
                        ))
                        success_count += 1
                        
                        # é€å€‹è™•ç†æ™‚å»¶é²æ›´é•·
                        time.sleep(1)
                        
                    except Exception as e2:
                        logger.error(f"âŒ [Indexer] å–®å€‹æ–‡ä»¶ä¹Ÿå¤±æ•—: {e2}")
        
        # æ‰¹æ¬¡å¯«å…¥ Qdrant
        if points:
            try:
                self.qdrant_client.upsert(
                    collection_name=self.collection_name,
                    points=points
                )
                logger.info(f"âœ… [Indexer] æˆåŠŸç´¢å¼• {success_count} å€‹æ–‡ä»¶")
            except Exception as e:
                logger.error(f"âŒ [Indexer] Qdrant å¯«å…¥å¤±æ•—: {e}")
                raise
        
        return success_count
    
    def delete_by_filename(self, file_name: str) -> int:
        """
        åˆªé™¤æŒ‡å®šæª”æ¡ˆçš„æ‰€æœ‰å‘é‡
        
        Args:
            file_name: æª”æ¡ˆåç¨±
            
        Returns:
            åˆªé™¤çš„å‘é‡æ•¸é‡
        """
        from qdrant_client.models import Filter, FieldCondition, MatchValue
        
        try:
            # å…ˆè¨ˆç®—è¦åˆªé™¤å¤šå°‘
            count_result = self.qdrant_client.count(
                collection_name=self.collection_name,
                count_filter=Filter(
                    must=[FieldCondition(key="file_name", match=MatchValue(value=file_name))]
                )
            )
            count = count_result.count
            
            if count > 0:
                # åŸ·è¡Œåˆªé™¤
                self.qdrant_client.delete(
                    collection_name=self.collection_name,
                    points_selector=Filter(
                        must=[FieldCondition(key="file_name", match=MatchValue(value=file_name))]
                    )
                )
                logger.info(f"ğŸ—‘ï¸ [Indexer] å·²åˆªé™¤ {count} å€‹å‘é‡ (file: {file_name})")
            
            return count
            
        except Exception as e:
            logger.error(f"âŒ [Indexer] åˆªé™¤å¤±æ•—: {e}")
            return 0
    
    def get_stats(self) -> Dict[str, Any]:
        """å–å¾—ç´¢å¼•çµ±è¨ˆ"""
        try:
            collection_info = self.qdrant_client.get_collection(self.collection_name)
            return {
                "collection": self.collection_name,
                "points_count": collection_info.points_count,
                "status": str(collection_info.status),
                "embed_provider": self.embed_provider,
                "embed_model": self.embed_model,
                "embed_dim": self.embed_dim,
                "has_fallback": self._has_openai_fallback
            }
        except Exception as e:
            logger.error(f"âŒ [Indexer] å–å¾—çµ±è¨ˆå¤±æ•—: {e}")
            return {"error": str(e)}


def get_indexer() -> Indexer:
    """å–å¾—å…¨åŸŸ Indexer å¯¦ä¾‹"""
    global _indexer_instance
    if _indexer_instance is None:
        _indexer_instance = Indexer()
    return _indexer_instance


def reset_indexer():
    """é‡ç½®å…¨åŸŸ Indexer å¯¦ä¾‹"""
    global _indexer_instance
    _indexer_instance = None
